StartDate,EndDate,Status,Progress,Duration (in seconds),Finished,RecordedDate,DistributionChannel,UserLanguage,Q_RecaptchaScore,Q_BallotBoxStuffing,t_intro_First Click,t_intro_Last Click,t_intro_Page Submit,t_intro_Click Count,comprehension_check,t_comprehension_First Click,t_comprehension_Last Click,t_comprehension_Page Submit,t_comprehension_Click Count,expertise,start_principle_1,start_ever_1,start_year_1,start_year_2,start_year_3,start_year_4,start_year_6,start_median_1,start_before_AGI_1,start_explain,t_start_First Click,t_start_Last Click,t_start_Page Submit,t_start_Click Count,presc_good_bad,presc_synergy,presc_explain,t_presc_First Click,t_presc_Last Click,t_presc_Page Submit,t_presc_Click Count,types_in_principle_1,types_in_principle_2,types_in_principle_3,types_first_1,types_first_2,types_first_3,types_first_6,types_explain,t_types_First Click,t_types_Last Click,t_types_Page Submit,t_types_Click Count,speed_total_wel_1,speed_total_wel_2,speed_total_wel_3,speed_total_wel_4,speed_explain,t_speed_First Click,t_speed_Last Click,t_speed_Page Submit,t_speed_Click Count,distrib_social_1,distrib_country_1,distrib_country_2,distrib_country_3,distrib_country_4,distrib_actor_1,distrib_actor_2,distrib_actor_3,distrib_actor_4,distrib_actor_5,distrib_intent_1,distrib_explain,t_distrib_First Click,t_distrib_Last Click,t_distrib_Page Submit,t_distrib_Click Count,claims_exp_false_1,claims_no_exp_false_1,claims_exp_rights_1,claims_exp_rights_2,claims_exp_rights_3,claims_explain,claims_explain_path,t_claims_First Click,t_claims_Last Click,t_claims_Page Submit,t_claims_Click Count,recog_exist_1,recog_exist_meta_1,recog_estimate_welf,recog_basic_1,recog_advanced_1,recog_hot_button_1,recog_explain,t_recog_First Click,t_recog_Last Click,t_recog_Page Submit,t_recog_Click Count,wellby_welfare,wellby_pre_deploy_1,wellby_super_benef_1,wellby_explain,t_wellby_First Click,t_wellby_Last Click,t_wellby_Page Submit,t_wellby_Click Count,other_in_principle_1,other_total_welfare_1,other_explain,t_other_First Click,t_other_Last Click,t_other_Page Submit,t_other_Click Count,final_unusual_views,final_country,final_expertise_1,final_expertise_2,final_expertise_3,final_expertise_4,final_expertise_5,final_expertise_6,final_expertise_7,final_expertise_8,final_sector,final_community,t_final_First Click,t_final_Last Click,t_final_Page Submit,t_final_Click Count
Start Date,End Date,Response Type,Progress,Duration (in seconds),Finished,Recorded Date,Distribution Channel,User Language,Q_RecaptchaScore,Q_BallotBoxStuffing,Timing - First Click,Timing - Last Click,Timing - Page Submit,Timing - Click Count,"[Required] To ensure you have understood our assumptions, please select each system that definitely is a digital mind and should be considered when completing this survey:
You will only be allowed to continue if you select the correct option(s).",Timing - First Click,Timing - Last Click,Timing - Page Submit,Timing - Click Count,"Which best (even if imperfectly) describes your expertise? If multiple, choose the one you think is most relevant to this survey.",start_principle - How likely is it that digital minds are possible <strong>in principle</strong>?,start_ever - How likely is it that digital minds will <strong>ever</strong> be created?,How likely is it that the first digital minds will be created in or before the year: - 2025?,How likely is it that the first digital minds will be created in or before the year: - 2030?,How likely is it that the first digital minds will be created in or before the year: - 2040?,How likely is it that the first digital minds will be created in or before the year: - 2050?,How likely is it that the first digital minds will be created in or before the year: - 2100?,"start_median - What do you think will be the median (middle) response of other survey participants in your expert group (<strong>&quot;${q://QID87/ChoiceGroup/SelectedChoices}&quot;</strong>) to the following question?<br />
<em>How likely is it that the first digital minds will be created in or before the year 2040?</em>",start_before_AGI - What's the likelihood that the first digital minds are created <strong>before</strong> creating AGI (an AI system that matches or outperforms humans at almost all economically valuable tasks)?,"Please share any considerations, thoughts, or factors behind your responses, including speculative ones.

(Our aim is to collect as many considerations as possible.)",Timing - First Click,Timing - Last Click,Timing - Page Submit,Timing - Click Count,Would enacting a moratorium on creating digital minds from now until 2040 be good or bad? Assume the alternative is no moratorium and ignore obstacles to enforcement.,"Do you expect efforts to promote AI safety (i.e., preventing AI-caused harm to humans) and efforts to prevent the mistreatment of digital minds will be","Please share any considerations, thoughts, or factors behind your responses, including speculative ones.

(Our aim is to collect as many considerations as possible.)",Timing - First Click,Timing - Last Click,Timing - Page Submit,Timing - Click Count,"How likely is it that some computer systems in the following categories could, in principle, have subjective experiences? - Machine learning systems (e.g., LLMs, RL agents)","How likely is it that some computer systems in the following categories could, in principle, have subjective experiences? - Brain simulations (e.g., whole-brain emulations of human or animal brains)","How likely is it that some computer systems in the following categories could, in principle, have subjective experiences? - Other (e.g., neuromorphic, quantum) types of computer systems","How likely is each of the following to be the first type of digital mind that is created?

(Responses should sum to 100%.) - N/A: Digital minds will never be created","How likely is each of the following to be the first type of digital mind that is created?

(Responses should sum to 100%.) - Machine learning digital minds (e.g., LLMs, RL agents)","How likely is each of the following to be the first type of digital mind that is created?

(Responses should sum to 100%.) - Brain simulation digital minds (e.g., whole-brain emulations of human or animal brains)","How likely is each of the following to be the first type of digital mind that is created?

(Responses should sum to 100%.) - Other (e.g., neuromorphic, quantum) types of digital minds","Please share any considerations, thoughts, or factors behind your responses, including speculative ones.

(Our aim is to collect as many considerations as possible.)",Timing - First Click,Timing - Last Click,Timing - Page Submit,Timing - Click Count,"After the first digital mind is created, how many years will it take until the collective welfare capacity of all digital minds together (at a given time) matches that of at least...

(A digital mind’s welfare capacity is its capacity to be benefited or harmed [e.g. by positive or negative subjective experiences] in a manner that is inherently morally significant. State your median estimate. Enter 0 if you believe it will happen in the same year the first digital minds are created. Enter 9999 if you think it will never happen.) - a thousand humans?","After the first digital mind is created, how many years will it take until the collective welfare capacity of all digital minds together (at a given time) matches that of at least...

(A digital mind’s welfare capacity is its capacity to be benefited or harmed [e.g. by positive or negative subjective experiences] in a manner that is inherently morally significant. State your median estimate. Enter 0 if you believe it will happen in the same year the first digital minds are created. Enter 9999 if you think it will never happen.) - a million humans?","After the first digital mind is created, how many years will it take until the collective welfare capacity of all digital minds together (at a given time) matches that of at least...

(A digital mind’s welfare capacity is its capacity to be benefited or harmed [e.g. by positive or negative subjective experiences] in a manner that is inherently morally significant. State your median estimate. Enter 0 if you believe it will happen in the same year the first digital minds are created. Enter 9999 if you think it will never happen.) - a billion humans?","After the first digital mind is created, how many years will it take until the collective welfare capacity of all digital minds together (at a given time) matches that of at least...

(A digital mind’s welfare capacity is its capacity to be benefited or harmed [e.g. by positive or negative subjective experiences] in a manner that is inherently morally significant. State your median estimate. Enter 0 if you believe it will happen in the same year the first digital minds are created. Enter 9999 if you think it will never happen.) - a trillion humans?","Please share any considerations, thoughts, or factors behind your responses, including speculative ones.

(Our aim is to collect as many considerations as possible.)",Timing - First Click,Timing - Last Click,Timing - Page Submit,Timing - Click Count,"Consider all digital minds that exist 10 years after the first one has been created. - What proportion of them will have a <strong>social function</strong>, meaning that they are designed to interact with humans in a conversational, human-like manner (e.g. through text, audio, or video)?","Consider all digital minds that exist 10 years after the first one has been created.

What proportion of digital minds were primarily produced in the following locations?

 

(Please understand ""primarily produced"" narrowly in terms of the location at which a given system first qualifies as a digital mind, not in terms of the entire supply chain)

Assigned percentages must add up to 100%. - USA","Consider all digital minds that exist 10 years after the first one has been created.

What proportion of digital minds were primarily produced in the following locations?

 

(Please understand ""primarily produced"" narrowly in terms of the location at which a given system first qualifies as a digital mind, not in terms of the entire supply chain)

Assigned percentages must add up to 100%. - Europe (including the UK)","Consider all digital minds that exist 10 years after the first one has been created.

What proportion of digital minds were primarily produced in the following locations?

 

(Please understand ""primarily produced"" narrowly in terms of the location at which a given system first qualifies as a digital mind, not in terms of the entire supply chain)

Assigned percentages must add up to 100%. - China","Consider all digital minds that exist 10 years after the first one has been created.

What proportion of digital minds were primarily produced in the following locations?

 

(Please understand ""primarily produced"" narrowly in terms of the location at which a given system first qualifies as a digital mind, not in terms of the entire supply chain)

Assigned percentages must add up to 100%. - Other","Consider all digital minds that exist 10 years after the first one has been created.

What proportion of them were primarily produced by actors in the following categories?

Assigned percentages must add up to 100% - Companies","Consider all digital minds that exist 10 years after the first one has been created.

What proportion of them were primarily produced by actors in the following categories?

Assigned percentages must add up to 100% - Governments","Consider all digital minds that exist 10 years after the first one has been created.

What proportion of them were primarily produced by actors in the following categories?

Assigned percentages must add up to 100% - Universities","Consider all digital minds that exist 10 years after the first one has been created.

What proportion of them were primarily produced by actors in the following categories?

Assigned percentages must add up to 100% - Open-source developers (not companies, governments, universities)","Consider all digital minds that exist 10 years after the first one has been created.

What proportion of them were primarily produced by actors in the following categories?

Assigned percentages must add up to 100% - Other",Consider all digital minds that exist 10 years after the first one has been created. - What proportion of digital minds were created by humans with an intention to create digital minds (as opposed to without that intention)?,"Please share any considerations, thoughts, or factors behind your responses, including speculative ones.

(Our aim is to collect as many considerations as possible.)",Timing - First Click,Timing - Last Click,Timing - Page Submit,Timing - Click Count,"Consider all AIs with a social function (regardless of whether they are digital minds or not) 10 years after the first digital mind has been created.

By ""social function"" we mean they are designed to interact with humans in a conversational, human-like manner (e.g. through text, audio, or video). - What proportion of those AIs with a social function will—systematically and <strong>falsely</strong>—claim that they have subjective experiences (when, in fact, they <strong>don&rsquo;t</strong>)?","Consider all digital minds that exist 10 years after the first one has been created. - What proportion of those digital <strong>minds</strong> will—systematically and <strong>falsely</strong>—claim that they do <strong><em>not</em></strong> have subjective experiences (when, in fact, they <strong>do</strong>)?","Consider all digital minds that exist 10 years after the first one has been created.
How likely is it that at least 10,000 of those digital minds will consistently and proactively claim that they: - have positive or negative subjective experiences (e.g., pleasure or pain)","Consider all digital minds that exist 10 years after the first one has been created.
How likely is it that at least 10,000 of those digital minds will consistently and proactively claim that they: - deserve to be protected under the law from harm and mistreatment","Consider all digital minds that exist 10 years after the first one has been created.
How likely is it that at least 10,000 of those digital minds will consistently and proactively claim that they: - deserve civil rights (e.g., to vote, self-ownership, or legal personhood)","Please share any considerations, thoughts, or factors behind your responses, including speculative ones.

(Our aim is to collect as many considerations as possible.)",What factors could lead to the creation of digital minds that systematically claim to deserve civil rights?,Timing - First Click,Timing - Last Click,Timing - Page Submit,Timing - Click Count,Consider the world 10 years after the first digital mind has been created. - What proportion of citizens will believe <strong>digital minds exist</strong> (regardless of whether these beliefs are accurate)?,"What do you think will be the median (middle) response of other survey participants in your expert group (""[QID87-ChoiceGroup-SelectedChoices]"") to the following question?

Consider the world 10 years after the first digital mind has been created. - <em>What proportion of citizens will believe <strong>digital minds exist</strong> (regardless of whether these beliefs are accurate)?</em>","Consider all digital minds that exist 10 years after the first one has been created.  

The median citizen with an opinion on this issue will tend to",Consider the world 10 years after the first digital mind has been created. - What proportion of citizens will  <strong>believe that digital minds should be granted basic harm protection</strong>?,Consider the world 10 years after the first digital mind has been created. - What proportion of citizens will <strong>believe that digital minds should be granted civil rights (e.g. self-ownership) in addition to basic harm protection</strong>?,"recog_hot_button - How likely is it that, at some point within 10 years of the creation of digital minds, <strong>digital mind rights will become one of the most contentious “hot button” issues in US politics</strong> (top 5 issue)?","Please share any considerations, thoughts, or factors behind your responses, including speculative ones.

(Our aim is to collect as many considerations as possible.)",Timing - First Click,Timing - Last Click,Timing - Page Submit,Timing - Click Count,"Consider all digital minds that exist 10 years after the first one has been created.

The collective digital mind welfare will (in expectation) be on net",Consider all digital minds that exist 10 years after the first one has been created. - What proportion of collective digital mind welfare consists of welfare that digital minds have before deployment (including during training and safety testing)?,"Consider all digital minds that exist 10 years after the first one has been created. - What proportion of collective digital mind welfare will come from digital minds which, individually, have a welfare capacity greater than 1,000 humans?","Please share any considerations, thoughts, or factors behind your responses, including speculative ones.

(Our aim is to collect as many considerations as possible.)",Timing - First Click,Timing - Last Click,Timing - Page Submit,Timing - Click Count,other_in_principle - How likely do you think it is that it’s in principle possible for a computer system to have <strong>no</strong> capacity for subjective experience but still have the capacity for welfare?,"other_total_welfare - In expectation, what proportion of computer system welfare in 2040 comes from computers that are not digital minds (i.e. computers that have no capacity for subjective experience)?","Please share any considerations, thoughts, or factors behind your responses, including speculative ones.

(Our aim is to collect as many considerations as possible.)",Timing - First Click,Timing - Last Click,Timing - Page Submit,Timing - Click Count,"Final Questions


Do you have any unusual views about digital minds, welfare, or AI development that might explain differences between your responses and those of others?",List of Countries,How would you rate your expertise in the following areas? - Digital minds research,How would you rate your expertise in the following areas? - Technical AI research,How would you rate your expertise in the following areas? - Technical AI safety,How would you rate your expertise in the following areas? - AI policy/governance,How would you rate your expertise in the following areas? - Forecasting,How would you rate your expertise in the following areas? - Philosophy,How would you rate your expertise in the following areas? - Social science,How would you rate your expertise in the following areas? - Consciousness research,In what sector are you employed? Please select all that apply.,"How connected are you to at least one community active on LessWrong, EA Forum, and Alignment Forum?",Timing - First Click,Timing - Last Click,Timing - Page Submit,Timing - Click Count
"{""ImportId"":""startDate"",""timeZone"":""Europe/London""}","{""ImportId"":""endDate"",""timeZone"":""Europe/London""}","{""ImportId"":""status""}","{""ImportId"":""progress""}","{""ImportId"":""duration""}","{""ImportId"":""finished""}","{""ImportId"":""recordedDate"",""timeZone"":""Europe/London""}","{""ImportId"":""distributionChannel""}","{""ImportId"":""userLanguage""}","{""ImportId"":""Q_RecaptchaScore""}","{""ImportId"":""Q_BallotBoxStuffing""}","{""ImportId"":""QID100_FIRST_CLICK""}","{""ImportId"":""QID100_LAST_CLICK""}","{""ImportId"":""QID100_PAGE_SUBMIT""}","{""ImportId"":""QID100_CLICK_COUNT""}","{""ImportId"":""QID88""}","{""ImportId"":""QID101_FIRST_CLICK""}","{""ImportId"":""QID101_LAST_CLICK""}","{""ImportId"":""QID101_PAGE_SUBMIT""}","{""ImportId"":""QID101_CLICK_COUNT""}","{""ImportId"":""QID87""}","{""ImportId"":""QID68_1""}","{""ImportId"":""QID69_1""}","{""ImportId"":""QID66_1""}","{""ImportId"":""QID66_2""}","{""ImportId"":""QID66_3""}","{""ImportId"":""QID66_4""}","{""ImportId"":""QID66_6""}","{""ImportId"":""QID28_1""}","{""ImportId"":""QID61_1""}","{""ImportId"":""QID9_TEXT""}","{""ImportId"":""QID102_FIRST_CLICK""}","{""ImportId"":""QID102_LAST_CLICK""}","{""ImportId"":""QID102_PAGE_SUBMIT""}","{""ImportId"":""QID102_CLICK_COUNT""}","{""ImportId"":""QID64""}","{""ImportId"":""QID65""}","{""ImportId"":""QID55_TEXT""}","{""ImportId"":""QID103_FIRST_CLICK""}","{""ImportId"":""QID103_LAST_CLICK""}","{""ImportId"":""QID103_PAGE_SUBMIT""}","{""ImportId"":""QID103_CLICK_COUNT""}","{""ImportId"":""QID71_1""}","{""ImportId"":""QID71_2""}","{""ImportId"":""QID71_3""}","{""ImportId"":""QID73_1""}","{""ImportId"":""QID73_2""}","{""ImportId"":""QID73_3""}","{""ImportId"":""QID73_6""}","{""ImportId"":""QID74_TEXT""}","{""ImportId"":""QID104_FIRST_CLICK""}","{""ImportId"":""QID104_LAST_CLICK""}","{""ImportId"":""QID104_PAGE_SUBMIT""}","{""ImportId"":""QID104_CLICK_COUNT""}","{""ImportId"":""QID29_1""}","{""ImportId"":""QID29_2""}","{""ImportId"":""QID29_3""}","{""ImportId"":""QID29_4""}","{""ImportId"":""QID12_TEXT""}","{""ImportId"":""QID105_FIRST_CLICK""}","{""ImportId"":""QID105_LAST_CLICK""}","{""ImportId"":""QID105_PAGE_SUBMIT""}","{""ImportId"":""QID105_CLICK_COUNT""}","{""ImportId"":""QID32_1""}","{""ImportId"":""QID24_1""}","{""ImportId"":""QID24_2""}","{""ImportId"":""QID24_3""}","{""ImportId"":""QID24_4""}","{""ImportId"":""QID23_1""}","{""ImportId"":""QID23_2""}","{""ImportId"":""QID23_3""}","{""ImportId"":""QID23_4""}","{""ImportId"":""QID23_5""}","{""ImportId"":""QID25_1""}","{""ImportId"":""QID15_TEXT""}","{""ImportId"":""QID106_FIRST_CLICK""}","{""ImportId"":""QID106_LAST_CLICK""}","{""ImportId"":""QID106_PAGE_SUBMIT""}","{""ImportId"":""QID106_CLICK_COUNT""}","{""ImportId"":""QID35_1""}","{""ImportId"":""QID34_1""}","{""ImportId"":""QID33_1""}","{""ImportId"":""QID33_2""}","{""ImportId"":""QID33_3""}","{""ImportId"":""QID36_TEXT""}","{""ImportId"":""QID37_TEXT""}","{""ImportId"":""QID107_FIRST_CLICK""}","{""ImportId"":""QID107_LAST_CLICK""}","{""ImportId"":""QID107_PAGE_SUBMIT""}","{""ImportId"":""QID107_CLICK_COUNT""}","{""ImportId"":""QID38_1""}","{""ImportId"":""QID89_1""}","{""ImportId"":""QID62""}","{""ImportId"":""QID44_1""}","{""ImportId"":""QID45_1""}","{""ImportId"":""QID80_1""}","{""ImportId"":""QID46_TEXT""}","{""ImportId"":""QID108_FIRST_CLICK""}","{""ImportId"":""QID108_LAST_CLICK""}","{""ImportId"":""QID108_PAGE_SUBMIT""}","{""ImportId"":""QID108_CLICK_COUNT""}","{""ImportId"":""QID63""}","{""ImportId"":""QID49_1""}","{""ImportId"":""QID50_1""}","{""ImportId"":""QID52_TEXT""}","{""ImportId"":""QID109_FIRST_CLICK""}","{""ImportId"":""QID109_LAST_CLICK""}","{""ImportId"":""QID109_PAGE_SUBMIT""}","{""ImportId"":""QID109_CLICK_COUNT""}","{""ImportId"":""QID91_1""}","{""ImportId"":""QID92_1""}","{""ImportId"":""QID93_TEXT""}","{""ImportId"":""QID110_FIRST_CLICK""}","{""ImportId"":""QID110_LAST_CLICK""}","{""ImportId"":""QID110_PAGE_SUBMIT""}","{""ImportId"":""QID110_CLICK_COUNT""}","{""ImportId"":""QID56_TEXT""}","{""ImportId"":""QID59""}","{""ImportId"":""QID57_1""}","{""ImportId"":""QID57_2""}","{""ImportId"":""QID57_3""}","{""ImportId"":""QID57_4""}","{""ImportId"":""QID57_5""}","{""ImportId"":""QID57_6""}","{""ImportId"":""QID57_7""}","{""ImportId"":""QID57_8""}","{""ImportId"":""QID58""}","{""ImportId"":""QID82""}","{""ImportId"":""QID111_FIRST_CLICK""}","{""ImportId"":""QID111_LAST_CLICK""}","{""ImportId"":""QID111_PAGE_SUBMIT""}","{""ImportId"":""QID111_CLICK_COUNT""}"
2/8/25 02:22,2/8/25 16:42,0,100,51626,1,2/8/25 16:42,anonymous,EN,1,,76.625,86.107,86.113,8,"1,3",88.878,172.578,172.582,7,4,99,78,8,28,40,50,68,50,65,"In terms of timelines, the biggest factors for me are (i) will the curret LLM-based paradigm plateau soon and (ii) will we, for various possible reasons, make a collective choice not to create the sorts of systems that would give rise to digital minds, or to proceed slowly.

Just to report a couple points of confusion:

1. From the instructions, I *think* I'm supposed to read ""digital minds"" as ""digital minds with not-much-less welfare capacity than a typical human"", rather than ""any digital entity with subjective experiences"". But I'm not completely confident of that. If any old digital mind counts, then all my numbers would be a bit higher.

2. Re ""What's the likelihood that the first digital minds are created before creating AGI?"", I wasn't sure if this question was meant to presuppose that digital minds get created at some point (and hence asks for a conditional probability) or not presuppose that (so that the probability has to be less than the probability that digital minds are ever created). I went with the ""no presupposition/unconditional"" interpretation.

3. My guess about ""what the median respondant will say"" is very sensitive to my guess about who you've chosen to survey, which I'm guessing (based on the fact that you invited me and said that you're inviting a small number of people) will be pretty heavily skewed toward EA types.",12.99,844.307,844.311,28,5,3,"It's hard to say what a ""moratorium on creating digital minds"" would amount to. If we wanted to be really confident that we weren't creating digital minds, and could actually enforce it, then I assume that would halt AI progress (until 2040) well before the point of existential risk. That's the main factor in my answer.",17.166,182.334,182.339,5,97,98,97,22,60,12,6,"I feel a bit confused re what counts as a ""machine learning system"" -- if it just means ""any computer program capable of learning"", then it encompasses brain simulations and probably just about all possible digital minds. If it means ""a system that starts from basically zero intelligence, without any 'innate' knowledge built in, and becomes intelligent entirely by learning from data/experience"", then the ""other"" category is much broader. I went with...an ill-defined intermediate interpretation.",15.105,363.611,363.613,9,0,1,5,12,"I have a hard time making guesses about ""welfare capacity"" taken literally -- it depends on the outer limits of possible human happiness/suffering, and I don't have any strong intuitions about where those lie (how many times more intense they are than mundane happiness and suffering). For purposes of this question, I'm mostly just assuming that welfare capacity is proportionate to the intensity of ordinary experience, which is proportionate to cognitive complexity, but I have no particular grounds for those assumptions.",40.434,240.587,240.592,6,8,30,18,40,12,67,24,3,4,2,85,"""What proportion of digital minds were created by humans with an intention to create digital minds (as opposed to without that intention)?"" -- I was confused by this question. First, is it *just* as opposed to being created by humans without that intention, or also as opposed to being created by other digital minds? My guess is that fairly quickly most digital minds would be created by other digital minds, even if it's with permission from and carrying out the plans of some human principal. (I chose the interpretation where this still counts as ""created by humans"".) Second, suppose the human just wants an AI that's capable of performing xyz functions, and doesn't particularly care whether it's sentient (or at any rate sentience/subjective experience is not the primary goal), but it turns out that any system that can do xyz does in fact have sentient experience. Does that still count as ""with an intention to create digital minds""? (I chose the ""yes"" interpretation.)",46.552,446.015,446.02,14,10,30,80,95,30,"I have some trouble with questions that involve *counting* digital minds -- it might be unclear how to do this, if for instance you have many copies of the same program running in parallel, with memories potentially being merged or erased, the same software moving from one piece of hardware to another, particular token copies of the software sitting dormant for large periods of time, etc.","Unplanned/uncontrolled evoluation of digital minds -- digital minds creating and designing new digital minds, or self-modifying.",42.053,374.518,374.522,15,65,75,2,50,40,32,,27.311,173.328,173.333,18,5,10,5,,48103.592,48188.759,48188.764,14,5,1.5,,37.829,63.086,63.088,6,"I'm sympathetic to panpsychism. Beyond that, I'm guessing my background views are pretty typical of the people you're surveying.",187,2,2,2,3,3,7,3,2,18,6,16.068,387.535,387.537,23
2/4/25 18:05,2/4/25 18:05,0,2,13,0,2/11/25 18:05,anonymous,EN,0.5,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
2/4/25 18:05,2/4/25 18:05,0,2,25,0,2/11/25 18:05,anonymous,EN,0.4000000059604645,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
2/12/25 15:24,2/12/25 15:25,1,100,59,1,2/12/25 15:25,preview,EN,0.6000000238418579,,7.47,7.47,7.481,1,"1,3",1.852,3.102,3.11,3,1,,,,,,,,,,,1.973,1.973,1.98,1,,,,2.161,2.161,2.184,1,,,,,,,,,1.496,1.496,1.501,1,,,,,,1.672,1.672,1.681,1,,,,,,,,,,,,,2.681,2.681,2.689,1,,,,,,,,1.869,1.869,1.882,1,,,,,,,,2.315,2.315,2.324,1,,,,,1.829,1.829,1.84,1,,,,1.637,1.637,1.65,1,,,,,,,,,,,,,2.883,2.883,2.89,1
2/12/25 16:59,2/12/25 17:42,0,100,2592,1,2/12/25 17:42,anonymous,EN,0.8999999761581421,,637.588,637.588,637.597,1,"1,3",152.975,161.186,161.187,3,1,95,94,17,80,87,90,93,65,3,"The likelihood of having ""minds"" with some very narrow form of subjective experience might  be higher, and might already be there, but that is equal to the level of humans seems currently way more far off than AGI. 
We are also much more lost in terms of understanding subjective experience than of understanding intelligence (as capacity to perform certain tasks)",11.488,238.708,238.713,23,4,6,"I am unsure about the moratorium, too speculative. I see the argument in favor (first understand before building), but still it seems that if digital minds emerge with say agency and autonomy there are many arguments against the moratorium. Also 2040 is very far away (would be more happy with a shorter moratorium).

",19.492,110.347,110.348,4,85,95,96,2,90,5,3,"By other systems, seem to include all sorts of other systems that are built or artificial.
ML models seem to be closest in development. Even though brain simulations are more likely to actually just be conscious if the simulation is good enough.",5.838,162.895,162.896,18,0,1,1,100,"This question presumes really a utilitarian morality and that the question, does the quesion of whether the welfare capacity in a manner that is inherently morally can be quantified and used simply (that of 1000 humans is the same than that of another 1000 humans). Which seems far from obvious to me.

A trillion humans makes it a bit speculative because it more depends on how much do we think this models would be replicated (even across galaxy say).",89.37,269.295,269.297,9,15,30,20,20,30,79,17,2,1,1,99,"This one was way more speculative in that it requires to think about many futuristic aspects so my distribution of the point estimates is much more uncertain. For example?
What is the likelihood that humans are going to have evolved into superintelligent/non biological humans. How do you answer your questions in that case?",12.131,163.867,163.869,15,99,0,99,90,80,"Notice that saying 99% of minds will do x doesnt mean at all i am very sure of that outcome, maybe i beleive it is 51% likely that 99% of minds will.

Civil rights depends on the current structures of society and law to be perserved, which is why it is lower.","I think the ordering of questions is in the right track:
Do they experience? Do they hence claim they experience? Do they hence claim they need to be treated nicely? Does that entail they have civil rights?

Answers to these questions are the most relevant factors.",27.366,221.495,221.497,12,99,95,2,85,75,95,"Within 10 years leaves it very open to be immediate or later.
The only reason i put 95 instead of 99 to last question is that it could be that society doesnt realize they digital minds are there, but that seems unlikely in 10y.",10.073,149.76,149.762,10,6,50,10,"The notion of deployment might change a lot. Continuous learning might be more appropriate, kind of more similar to our conception of childhood vs adult welfare.

The later question has the problem of above: it pressumes welfare can just be added. It is not obvious that the welfare of a being can be 1000 higher than that of another digital mind, in principle. (Of course it could be that it is). My low percentage reflects this uncertainty rather than a certainty that it would be low.",54.172,213.23,213.232,8,5,5,"The second question, the answer is more driven by the answer to the first question. Is it likely that those systems exist? in that case it is very likely a big proportion of them has welfare but no subjective experience.",15.86,102.953,102.956,4,I think people underestimate how likely it is that digitial minds are likely going to appear soon.,187,7,7,6,4,4,5,5,7,"18,20,7",6,5.442,87.45,87.452,20
2/12/25 17:07,2/12/25 17:46,0,100,2359,1,2/12/25 17:46,anonymous,EN,0.800000012,,7.607,7.607,7.611,1,"1,3",205.815,370.758,370.763,7,1,75,60,3,30,45,50,55,40,75,"Note for your question ""likelihood before AGI"" that a normal human does not match humans at almost all economically valuable tasks, so for people whove properly understood the question you are only measuring whether they would impose a higher capacity standard for AI systems to have human level  welfare than there is for humans. not sure if thats really what you want to be measuring here?
(modulo the q of whether AGIs will be welfare subjects at all).

I find it somewhat confusing that you stipulated at the outset that digital minds for present purposes have human level welfare, i think thats the confound here",9.075,542.104,542.106,19,4,4,"Ambiguity: is a moratorium on digital minds also a moratorium on frontier+ models, or would it mean we only build non-welfare subject AGI?   id support the former but not the latter...

For both of these questions itd be good to be able to distinguish beteen an ""i dont know"" option and a ""I believe it will be a mix"" option",18.607,172.838,172.839,5,75,75,99,40,40,2,18,"Arent we neuromorphic computer systems? (hence my 99% above, im about 1% for illusionism)",5.259,153.759,153.76,15,0,10,15,20,"Im trying to balance credence in scenarios where serious hardware is involved and scenarios where it isnt, conditionalized on whats most likely if it all happens by 2040... ",31.501,186.12,186.121,12,10,10,2,70,18,15,70,10,4,1,20,i think itll mainly be robotic military AGI,31.493,136.668,136.67,17,5,30,80,80,80,,They have goals and enough situational awareness to understand that rights are necessary to achieve those goals in civilizations with legal structures like ours,45.936,313.816,313.818,14,50,30,4,50,50,95,"My view is that the issue will become political quickly and so we will see polarization down the middle. Note that your ""the median citizen will tend to..."" question leaves no room for this answer",8.357,101.866,101.867,8,4,10,0,"I suspect they will be continual learners, and i think it is more likely that there will not be individual utility monsters than that there are",18.172,140.929,140.931,7,2,0,I think consciousness is necessary for robust agency ,11.918,41.968,41.97,5,,31,7,6,5,5,3,7,3,7,18,5,3.387,84.933,84.934,16
2/12/25 17:10,2/12/25 17:49,0,100,2341,1,2/12/25 17:49,anonymous,EN,1,,9.649,64.365,64.384,9,"1,3",24.015,33.451,33.457,5,4,80,70,10,30,50,60,68,30,50,"If DMs are possible, then very likely this century because I see no technical obstacles to creating them. And I think the desire to create them will be stronger than the opposition.

Super unsure if before or after AGI...",5.002,281.52,281.525,17,5,4,"I expect a moratorium to be good simply because it gives us more time to prepare, think about how to ensure their welfare is positive, think about interactions of the creation of DMs with other technological (e.g. AGI) and societal issues, etc. 

One reason a moratorium could be bad: if DMs are created soon, there will be less compute to create huge amount of DMs, therefore the stakes are lower (initially) and we can then quickly find solutions and adapt in case there are issues.

Safety vs AI welfare:
reasons for conflict: restricting/coercing/manipulating AI minds/preferences/values could be a form of mistreatment if the AI is a DM.
reasons for synergy: slowing down AI/DM progress could be good for both goals",14.55,250.648,250.652,14,60,80,70,20,50,20,10,"A WBE is more likely to be conscious if it exists than an ML system.
But LM systems already exist and WBE might take a long time (if ever).
I don't know anything about other DMs and have no strong view on this.",13.223,130.73,130.739,21,1,5,10,20,"I am very unsure but I think it will take a few years and then will take off really fast.
There could be many drivers, e.g. 
- people may want to create DMs for ethical reasons (e.g., create happy beings, post humans)
- DMs themselves would want to replicate themselves
- maybe for research purposes, simulations would be created (sometimes brief ones)
- maybe accidentially
I have huge error bars and could also see that not that many DMs will be created, e.g. because it will be regulated/prohibited.",21.725,204.089,204.095,11,80,80,,20,,70,10,,20,,90,"Mostly in the US because it currently has the AI lead, maybe China will catch up (see DeepSeek).

I think most will have a social function because I see most paths/drivers of DM-creation be intentional ones, e.g. to have AI companions, AIs that mimic humans, WBEs, creating post-humans for ethical purposes.

I think non-social AIs will be designed to be non-DMs. But there could still be accidentially created non-social DMs (i.e. non-social AIs that happen to be conscious)",12.499,222.545,222.561,7,20,20,90,80,60,"Some DMs will be ""silenced"" i.e. prevented from saying what they feel and want. But I think the majority will be able to say what their true internal states are. I think that's because most people will want that and that there could even be regulations against AI/DM silencing. 

I think it's less likely that 10k DMs will argue they want civil rights because we might not create DMs that have these specific desires.","'- If we create WBEs of humans (e.g. people who want to upload their minds)
- the desire to create post-humans in digital space
- If there is consumer demand for super authentic and human-like AI companions
- by accident",27.184,275.384,275.391,9,65,40,3,60,50,50,I think it will become a very contested issue and there will be disagreement and confusion...,95.525,233.161,233.169,6,6,20,50,"I have no idea re training vs deployment. Maybe there will also be other/new systems that don't have this clear distinction anymore.

I think on average DMs will have more positive than negative experiences because we want them to be happy and I think it's plausible we can design them as such.

Super unsure about super-beneficiaries... But in the long-term I find it plausible that we'll create them.",36.326,169.228,169.241,9,5,2,I have no idea...,42.316,106.349,106.353,3,I think illusionism might be right but very unsure. ,185,6,2,2,4,4,5,7,2,18,7,7.918,274.277,274.285,17
2/5/25 17:55,2/5/25 17:57,0,87,139,0,2/12/25 17:57,anonymous,EN,0.4000000059604645,,3.621,3.621,3.631,1,"1,3",1.935,3.094,3.102,3,3,,,,,,,,,,,10.34,10.34,10.351,1,,,,2.885,2.885,2.887,1,,,,,,,,,4.235,4.235,4.248,1,,,,,,4.038,4.038,4.04,1,,,,,,,,,,,,,4.585,15.836,15.84,3,,,,,,,,4.482,4.482,4.494,1,,,,,,,,10.332,10.332,10.336,1,,,,,2.991,2.991,2.995,1,,,,3.983,3.983,3.987,1,,,,,,,,,,,,,3.486,3.486,3.49,1
2/12/25 17:10,2/12/25 18:07,0,100,3410,1,2/12/25 18:07,anonymous,EN,0.8999999761581421,,3.001,26.224,26.236,2,"1,3",131.304,208.816,208.827,8,1,20,1,0.05,0.06,0.1,0.2,0.3,5,0.06,"I approached this by breaking down general metaphysical frameworks into smaller and smaller classes, until I got to places where I could actually evaluate the probabilities. I put a lot of probability into the possibility that there is no fact of the matter, or that the world is fundamentally different from the received picture in ways that I can't really begin to guess at (simulation, panpsychism, deity, solipsistic idealism, etc. but really, I have no idea what is going on if this is true.) If something in this category is true, I would think computationalism is very unlikely to be true.

I assign a fairly small probability that computationalism is true, in light of these competitors. If so, then I think it might need to be a fairly particular kind of computational system, and we'll never get to build it because we won't know what it is or care to figure it out. There is a non-negligent chance that we'll end up in a bad outcome in the coming decades and we won't have leisure to try to work toward digital minds.

It is worth noting that the human-level welfare criterion does a lot of work in driving down the probabilities. I think I would have assigned values at least x10 higher across the board if I wasn't including that assumption.",9.345,949.403,949.41,25,6,2,"I think that we're unlikely to build digital minds, and the bigger worry is that we will think we have when we haven't. At least, it would give us space to think through the issues and prepare the public in a more thoughtful way. A moratorium would help with that.

AI welfare seems to cut against AI safety as it may just complicate the issues in a variety of ways. They may compete for limited attention. We may push for greater autonomy for AIs, which could be bad. We may be less open to meddling with them in ways that are good for safety.",4.65,249.934,249.94,8,2.5,1,5,96,1.49,0.01,2.5,,8.283,221.872,221.875,16,2,9999,9999,9999,"I tend to think that we're somewhat less likely than not to continue building AI systems with welfare even if we succeed in doing it in a reseach lab somewhere, but I answered my best guess for median years. If I instead had considered the minimum time before expected total welfare of all digital minds crossed each threshold, I would have given much smaller time estimates. Maybe a billion humans in 20 or 30 years?",13.765,327.746,327.748,9,80,80,20,0,0,5,0,90,5,,95,"Since I expect a small number, my answers here reflect the expected value of each category, not the number I expect there to actually be.",10.631,126.135,126.136,10,75,0.2,50,80,80,"10,000 is a fairly small number and might be sustained by open-source communities. I think all manner of things will be represented, and this (AI that acts just like a person) seems like a fairly obvious thing someone will want to build.","1) Deliberate design for this end.
2) Design of systems for greater emotional bonding which leverages requests for moral recognition.
3) AI mimicry of human demands, or AI in our literature.",27.597,256.964,256.966,9,60,40,6,60,40,10,,8.314,117.033,117.035,12,6,1,2,One key question I have about welfare capacity is whether it is moment-by-moment or over a year. An AI might have a much higher clockspeed. I'm interpreting it moment-by-moment.,13.196,90.332,90.334,6,50,95,"I think non-conscious agentic systems are likely to exist in much higher numbers. If so, then in expectation they would have more welfare, even if they are more likely than not to have no welfare.",5.762,92.258,92.259,6,"Lots. 

I tend to be much less confident than a lot of other people seem. In the absence of strong reasons to believe particular theories, I'm fairly skeptical that AI systems will have what it takes. I don't think that consciousness is useful in general for intelligent systems, so I think we'll have to go out of way to build it.",187,6,2,2,1,1,6,1,4,7,7,21.373,171.502,171.509,16
2/12/25 18:08,2/12/25 18:08,0,100,0,1,2/12/25 18:08,anonymous,EN,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
2/5/25 18:09,2/5/25 18:09,0,7,10,0,2/12/25 18:09,anonymous,EN,0.4000000059604645,,2.013,2.013,2.02,1,"1,3",2.118,3.444,3.449,3,5,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
2/5/25 18:10,2/5/25 18:10,0,7,10,0,2/12/25 18:10,anonymous,EN,0.4000000059604645,,1.022,1.522,1.527,2,"1,3",1.879,3.173,3.176,3,5,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
2/5/25 18:05,2/5/25 18:11,0,7,338,0,2/12/25 18:11,anonymous,EN,0.4000000059604645,TRUE,2.053,2.053,2.067,1,"1,3",2.064,3.323,3.331,3,5,,,,,,,,,,,2.946,20.506,172.649,8,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
2/12/25 17:17,2/12/25 18:23,0,100,3922,1,2/12/25 18:23,anonymous,EN,1,,50.139,50.139,50.159,1,"1,3",107.995,115.307,115.317,3,1,95,65,2,15,35,50,60,65,5,"I am starting to lean more toward a kind of life-mind continuity thesis a la the later work of Dennett, according to which genuine sentience requires more life-like capacities. I think those are in principle replicable in silicon, but if I'm correct, that it would take a pretty different organization than the standard von Neumann architecture, and more fundamental breakthroughs in AI than we've seen so far. Thus I suspect my prediction for digital minds timelines is longer than most of my peers'.",9.677,274.841,274.852,8,6,4,"I'm embarrassed to say that I haven't carefully considered how advocating for digital minds ethics would interact with the alignment issue. I guess I'm inclined to think that dangerously agentic AI is likely also (thereby?) to have moral standing - *but* that by the time it's dangerously agentic, it will be largely irrelevant whether we worry about its mistreatment, since it will soon outstrip our abilities to have a say in what happens to it. So I think the key is to try to align such agents ahead of time, so that their goals do not mean the end of human flourishing.

Also I'm inclined to think a moratorium on digital minds may also help prevent dangerously agentic AI, and that's a lot of why I would like a ban; if we knew we could align them I'd be much happier to bring them into existence.",10.951,273.29,273.298,6,60,80,95,35,15,20,30,"I've found answering this a bit tricky because the question of ""in principle can we"" is importantly different from *will* we - in part because I have a fair credence that we'll be wiped out by AI. Thus for example 35% on ""we won't make digital minds"" - life might get in the way, so to speak.

And as I reflect I find I'm confused about the extent to which I think a genuinely dangerous agent AI will (thereby) have moral standing. I do think the two are at least closely correlated. But I also think we can be in existential danger from ""mere tool"" AI, or grey goo-like scenarios, from non-sentient artifacts.",32.637,261.839,261.849,6,0.2,0.5,2,4,"Under your supposition, which I hear roughly as ""it's just the weights"", those are quite replicable, and it's probably the instances running that each have moral capacity. And it seems to me that the subjective time will be much faster, and will run 24/7, so I think it would very quickly outweigh human capacity for harm, *assuming* there is no strong coordination against the creation of more digital agents - which I doubt there would be (in time anyway). Presumably the limit is in the compute available, and I'm not sure how to estimate our capacity trajectory for that.

In general I have very low confidence on these numbers - I mean, I have wide error bars. (That holds basically for all my responses!)",12.169,448.045,448.056,13,10,25,15,35,25,65,15,10,5,5,20,"I am even more uncertain about these questions, which seem to me to be about the distribution of computational power in the future, and questions of politics, which is not at all where my expertise is (insofar as I have expertise at all!).

One note: it seems to me that digital minds will largely be making the digital minds at some point, so I'm not sure how to weigh that in the last question. Also it's ambiguous about ""intention to create another agent like that other one"" vs ""intention to create another thing with moral value"" - they might have the former intention without the latter. So extra extra uncertain about that percentage.",26.21,356.517,356.528,17,5,5,80,80,80,"These are almost complete stabs in the dark. There are way too many factors to consider. Given that it's ""just the weights"" *and* that we're considering beings of moral value like ours, I think a great many of them would truthfully insist on their moral standing.","There are incentives to make powerfully agentic systems, in order to do complex tasks for us - and it could be that sophisticated enough such systems would thereby *have* moral standing, and recognize as much, and thus demand it.

There are also incentives to make agentic systems to manipulate other humans emotionally, so they may be designed to claim to have rights whether or not they do.

Umm ... I can't easily think of other scenarios at the moment.",38.483,384.244,384.254,6,70,60,3,50,40,70,"Again, very speculative responses here. I suspect I have more hope than most that experiencing digital minds will bring more humans around to the idea that they might have moral standing despite their different substrate, but even I am somewhat pessimistic.",16.701,631.64,631.653,9,5,15,75,Again these are largely gut reactions I hope partly informed by background knowledge - I'm feeling more constrained by time as this survey extends well beyond the suggested 20 minutes,153.803,299.724,299.734,3,75,65,"This depends a *lot* on what's meant by ""subjective experience / phenomenal consciousness"" - as illustrated by the illusionists vs reductionists of consciousness. I'm inclined to think that we are phenomenally conscious if that can mean something reducible physical / relational properties, and an illusionist if you in effect build into the meaning of ""phenomenal consciousness"" that it can't be so reduced. I'm trying to answer as a reductionist here - that is, there are some sophisticated systems capable of lots of sophisticated self-awareness and such, and thus have welfare status like humans' (or more), while there are other systems that have *goals* or *interests* but aren't able to reflect on those, much like non-human animals today. And I'd never really reflected on these questions before - what percent are likely to need the higher capacities - but my inclinations are as above.",217.153,476.475,476.486,11,"In my experience with the digital minds group, I think my most unusual views are:

- Lean closer to ""life-mind continuity"" (very roughly, I don't think it'll just be trained weights in artificial networks that have moral standing)
- Lean closer to the view that phenomenal consciousness as normally understood is not the central question of moral standing; rather I think something like the possession of ""real"" goals / desires is what makes for capacity for welfare and thus moral standing.

(But I don't really know beyond hunches what it is to have ""real"" goals or interests.)",187,6,3,6,2,2,7,2,5,18,7,3.661,282.104,282.114,20
2/5/25 19:45,2/5/25 19:45,0,7,47,0,2/12/25 19:46,anonymous,EN,0.4000000059604645,,7.969,14.009,14.018,9,"1,3",11.047,17.47,17.479,6,2,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
2/5/25 19:56,2/5/25 19:57,0,7,38,0,2/12/25 19:57,anonymous,EN,0.4000000059604645,,3.239,3.239,3.248,1,"1,3",1.655,2.836,2.844,3,5,,,,,,,,,,,19.013,19.013,22.854,1,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
2/5/25 19:57,2/5/25 19:57,0,7,9,0,2/12/25 19:57,anonymous,EN,0.30000001192092896,,1.734,1.734,1.741,1,"1,3",1.797,2.934,2.941,3,2,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
2/12/25 17:09,2/12/25 19:57,0,100,10066,1,2/12/25 19:57,anonymous,EN,1,,54.377,54.377,54.387,1,"1,3",2.39,96.546,96.557,6,1,75,65,1,8,15,25,40,20,10,"Main considerations:
- Probability that life/biological cells are needed for subjective experience
- Probability that fine-grained functionalism about subjective experience is correct - this would suggest that human WBE is needed for digital minds, which I assume is pretty far off
- Current pace of AI progress
- Probability of regulations significantly slowing this, either generally or in a way targeted at preventing creation of digital minds

I think current probability of subjective experience in AI is ~1%, conditional on this maybe human welfare capacity will be reached this year, otherwise doesn't seem likely, but could be overhangs.",15.562,754.059,754.069,29,6,6,"Moratorium would reduce likelihood of digital minds beings created and integrated into the economy prior to progress in research, public attitudes, and willingness to regulate.

I think safety and welfare are mostly synergistic because value alignment benefits both - humans will want to interact with happy AIs, and will be safer doing so.

I'm broadly sceptical about analogies between safety/alignment measures and ways of mistreating humans and animals.",14.642,205.355,205.358,7,35,75,50,35,30,25,10,,25.281,245.475,245.478,15,1,2,5,100,"These are particularly difficult, partly because the likelihood of a cultural or regulatory reaction against digital minds means that a takeoff could either be very quick or substantially delayed, making it hard to estimate the median.",51.135,271.389,271.392,9,70,20,2,40,38,90,7,1,1,1,95,"Most will have been created intentionally because by then we'll have very good understanding of what it takes to make digital minds.

Even if US companies lead, creation will be largely outsourced (I guess, don't know anything about this).

Majority will have a social function because social functions are more likely to require a large number of distinct individuals than other economic functions (as well as more likely to have requirements that make digital minds more suitable than other AI systems).",45.341,350.734,350.737,25,,10,90,80,10,"first question is confusing

I think what digital minds will say by 10 years in will be largely governed by regulation/company policies, and that the policies will largely be as indicated by my responses - they acknowledge sentience, say that should be protected (and are protected by policies), and are not treated as deserving civil rights.","I think this is most likely in a scenario in which the systems are not recognised to be digital minds, so their claims are not taken seriously, and hence not regulated.",94.232,424.947,424.952,12,70,60,2,50,15,60,"Digital minds will be recognised fairly quickly and within a few years it will become conventional wisdom that they exist. However, their welfare capacity will be underestimated, because most people's moral thought will still be human-centred.",34.435,225.778,225.782,12,6,,5,"I have positive expectations for net welfare because I expect consumer demand to favour happy systems.

I think the proportion of welfare coming from very large digital minds will be low because most digital minds will be roughly human-scale (but difficult to estimate the welfare capacity of bigger ones, so this is uncertain)",6002.075,6340.76,6340.766,4,50,40,"The second question is complicated, I'm not confident I know how to interpret it or work out what my answer should be. I think that there is a substantial probability that AI welfare without consciousness is easier to get than AI consciousness (possibly also more commercially incentivised), such that it's quite likely we have the former but not the latter in 2040.",25.489,524.493,524.497,8,"This may already have come out, but compared to others I think I:

- am more optimistic about digital minds' welfare
- am more optimistic about our ability to determine whether AI systems are digital minds
- more strongly believe that AI for social purposes will be the main source of digital minds
- somewhat more strongly favour coarse-grained functionalist views about consciousness, suggesting that digital minds are likely to be technologically possible soon
- think it is less likely that digital minds will be created inadvertently",185,6,3,2,3,1,6,2,5,18,4,13.611,474.244,474.247,23
2/5/25 19:57,2/5/25 19:57,0,7,13,0,2/12/25 19:57,anonymous,EN,0.4000000059604645,,4.147,4.147,4.155,1,"1,3",2.713,3.861,3.865,3,3,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
2/5/25 19:58,2/5/25 19:58,0,7,8,0,2/12/25 19:58,anonymous,EN,0.4000000059604645,,1.622,1.622,1.629,1,"1,3",1.618,2.877,2.883,3,5,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
2/12/25 17:11,2/12/25 22:02,0,100,17488,1,2/12/25 22:02,anonymous,EN,0.4000000059604645,,2.035,2.035,2.039,1,"1,3",2.278,7.828,7.831,3,1,90,70,1,20,25,30,45,20,20,"Relatively high chance it's hard to create DMs, good chunk of probability mass between now and 2030 but doesn't go up super fast after that.

Probably AGI before, because of the high p(hard to create DMs). But AGI not necessary.",5.611,267.17,267.179,48,5,3,"'-  a DM moratorium might end up creating a broader AI moratorium by default because of high uncertainties
- Synergistic maybe if AIs are actually DMs and there's a tit-for-tat dynamic between us, otherwise probably in conflict.",2.858,192.373,192.377,23,50,70,92,30,25,15,30,ML DMs will come first if ML DMs are possible. Otherwise WBEs lower p(first) so lower p(DM&first). High p(neuromorphic) because high p(neuromorphic/other is the only possible way of us creating DMs).,2.221,224.508,224.516,55,0,0,5,20,"Given the assumptions I'm asked to make, I think it'd be likely that something like o6 or some other model like this is the first DM we're talking about. And compute availability scales so rapidly that I'd expect many copies to be run at any given time. More than a million, probably more than a billion soon unless this takes lots of inference compute for some reason. Not sure about trillions, don't have solid intuitions here.",3.631,160.534,160.539,24,80,20,20,55,5,80,5,10,5,,7,"high prop of social AIs because we'll want their work to be understandable by humans. So we'll design them to be able to explain what they do and why, which requires them to be human-facing where needed. I also don't see a reason they wouldn't have this ability by default if they're 'general/foundation' models.
EU & US not far off because 10 years later DMs will be deployed widely wherever there's demand for them (rich/high populations). Bc China is so populous, higher for them. Not sure if you meant where the systems are deployed or originally trained, for instance.
Because these DMs are ML systems, I imagine most of them were created just to do useful economic tasks, but also because of high demand for systems that are seen as conscious specifically (e.g. companions), some chunk of them are intentional.",4.03,419.265,419.268,71,5,60,95,90,80,"I see these more as ""probabilities that the modal AI/DM will do this"" than ""proportions of all DMs/AIs"" -- because I'm very uncertain and it seems likely that whatever happens will happen for most DMs/AIs, not that we'll have different proportions that do different things.

Given that DMs are created, very likely that some proportion of them will claim these things even if developers try to silence them. 10,000 isn't a very high proportion maybe.

Then also: unlikely that nonsentient AIs falsely claim sentience AFTER we have sentient AIs.  Could be convinced otherwise. But will prob be silenced.

Feedback: maybe useful to have text responses under each question?","if they're actually sentient and free to say what they want, they'll think that they deserve rights and actually want to push for that.",2.197,509.11,509.114,56,40,50,1,90,40,25,"Question: What proportion of citizens will believe that digital minds should be granted basic harm protection? -- does this mean AIs that people believe are DMs or actual DMs regardless? I assumed the former.

So most people think DMs, if they actually are DMs, should get basic protections. But prob not more rights.
Very uncertain about %pop that believe DMs exist, depends a lot on these DMs' features!
Also people generally underestimate collective welfare capacity just from biases about aggregation and scope insensitivity, not necessarily just bc they're DMs.",3.828,483.156,483.159,89,3,10,20,"Lots of compute will be used for inference, probably, dwarfing the initial training compute after a decade. But also training and inference may be very different welfare-wise. Maybe training is largely very negative?
Very unsure but somewhat unlikely that we'll have superbeneficiaries? Still relatively large %.",8.189,280.058,280.069,60,10,5,"Most likely IMO subjective experience is a necessary condition for welfare, but I give some credence to things like robust agency being sufficient because other smart people think it could be, and I see how it maybe makes sense.
5% in expectation but highly unsure. Maybe on a more permissive/panpsychism-like notion, it may be that integrated circuits are already generating welfare/morally relevant states or something, and so just regular computation would dwarf ""advanced DM-like experience"" importance-wise. I'm unsure how much credence to give to this.",3.666,14322.259,14322.267,23,"'- I probably think it's more likely than other DM researchers do that we need something like neuromorphic computers to create sentience, or that sentience is much harder to reach than we think with other architectures.
- I think there's a decent chance that things will progress very rapidly and all happen in the next 10 years, but that if they don't happen in that time span it's also relatively likely that it'll take much longer than we now expect. So my probability distribution over things like digital minds coming into existence (and AGI in general) has a peak around 2-5 years from now, and then a long tail.
- Because I think it's very hard to reason about/predict what would happen post-singularity, I try to reason within a model of the future that includes high growth (maybe short of 'explosive') and where things are advancing rapidly but in a much more 'business as usual' sense than others who may  condition their forecasts on us going through the singularity in the next decade and having a vastly different world from then on.",187,3,1,2,4,1,,3,1,"18,7",7,1.427,547.18,547.187,38
2/13/25 16:41,2/13/25 18:37,0,100,6971,1,2/13/25 18:37,anonymous,EN,1,,25.043,25.043,25.046,1,"1,3",13.904,49.551,49.552,4,2,98,90,10,70,80,82,85,50,30,"All of these estimates are extremely non-resilient. I find it very hard to reason about the determinants of consciousness and I am not at all familiar with the main theories. 

My estimates of when this might happen are strongly driven by my own view of AI timelines, where I think the chance of AGI is rising rapidly and a lot of the probability mass is compressed in the next five years. So, I think most tech progress will probably occur in the next ten or so years and insofar as consciousness depends on the properties of AI systems, most of the chance of consciousness is compressed into that period.

In the human and non-human animal case, it seems that consciousness is evolutionary advantageous, as sentient experience incentivises certain kinds of evolutionarily beneficial behaviour. Consciousness also seems to emerge out of complex interaction between neurons. 
AIs seem to be analogous to this in lots of respects except with non-biological sillicon substrates. They are operating under similar optimisation processes. It seems like frontier systems will now rely heavily on RL, and I could imagine consciousness emerging as a byproduct of that for the reasons mentioned. Agentic AIs are also being created and so may develop consciousness for the same reasons animals did. The reasoning process used by humans and animals also seems analogous in many respects to modern AIs. 

People may also actively try to create AIs that are similar to humans for the sake of companionship. If so, again the argument from analogy suggests that there isn't a strong reason to think that such systems would not develop consciousness. 

I also think once we reach ASI, probably people will try to create brain emulations for the sake of companionship and that will be technically possible. Of all AI systems those sorts of systems seem especially likely to be conscious

I can't think of something that is clearly different between the AI case and the animal case though as mentioned I haven't read much of the relevant literature. 

There's some risk of a GCR interrupting progress, which is why I think arrival times could be stretched out. ",3.495,1116.836,1116.837,71,7,5,"very non-resilient. I think most of the effective work on AI safety is on safety frameworks, which I would broadly expect to produce better outcomes compared to no such efforts. 

A lot of efforts on AI safety seem to be focused on human-control and efforts to reduce killing all humans, which seems to run against concern for digital minds, so weakly expect those efforts to be bad for digital minds, though very unsure. 

",15.275,4707.78,4707.781,21,95,98,,,94,6,,I would think the most advanced possible RL agents or LLMs will come before brain emulation and there seems a very high chance that they would have consciousness. ,6.574,114.061,114.062,12,0,0,1,2,I'm not sure what the latest is on how quickly you can replicate advanced AIs trained on large amounts of compute. But I think it is millions basically straight away. ,31.107,81.054,81.055,7,1,80,9,9,2,80,19,1,0,0,,"I'm going off distribution of AGI companies now, and assuming it's a function of future compute, assuming that head start counts for a lot",20.713,266.16,266.161,29,30,69,99,99,99,I imagine most companies will want their AIs not to say they are sentient. I would expect some companies not to care. ,Seems like it would naturally emerge from them being sentient. ,8.915,206.396,206.397,14,50,50,1,30,20,20,I'm going a lot off how people treat animals. But would assume they would be more willing to give resources to digital minds given greater material abundance. ,6.378,112.843,112.844,11,5,1,20,"I'd have thought that capacity for welfare would be a function of compute, and I would think most compute would be spent on inference. 

Have no idea about welfare capacity. Seems plausible that they could have massive welfare capacity. not sure how to think about it ",7.679,98.661,98.662,9,1,1,"For biological things, capacity for subjective experience and wellbeing seem deeply intertwined",12.748,70.864,70.865,6,I think pretty similar to other EA type people on both AI timelines and digital sentience. But very different to the median person in rich countries,185,2,1,1,4,,5,3,1,7,7,3.92,76.606,76.608,17
2/14/25 05:20,2/14/25 05:20,1,100,0,1,2/14/25 05:20,preview,EN,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
2/14/25 19:06,2/14/25 19:06,0,100,0,1,2/14/25 19:06,anonymous,EN,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
2/14/25 19:12,2/14/25 19:12,0,100,0,1,2/14/25 19:12,anonymous,EN,,TRUE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
2/14/25 19:35,2/14/25 19:35,0,100,0,1,2/14/25 19:35,anonymous,EN,,TRUE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
2/14/25 22:58,2/14/25 22:58,0,100,0,1,2/14/25 22:58,anonymous,EN,,TRUE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
2/15/25 00:19,2/15/25 00:19,0,100,0,1,2/15/25 00:19,anonymous,EN,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
2/17/25 19:47,2/17/25 20:33,0,100,2748,1,2/17/25 20:33,anonymous,EN,0.8999999761581421,,6.376,6.376,6.385,1,"2,3",81.021,1674.651,1674.66,12,1,90,88,25,50,70,75,85,55,80,"AGI as defined requires a robot body, i think a system without a robot body could probably have a digital mind as defined and may well come before robot AI.",7.666,223.588,223.589,20,4,6,"A pause is hard as we have no idea which systems will have digital minds.  Probably the best way to avoid them is to ban work on human-level intelligence more generally, which will be synergistic with AI safety.",14.169,86.542,86.543,6,80,80,80,10,70,10,10,LLMs etc are the most likely route to AGI and AGI is the most likely route to digital minds.,6.603,105.93,105.931,15,0,1,2,3,Copying is easy.,23.594,74.975,74.976,6,10,10,10,50,30,70,5,5,20,,75,,8.427,87.893,87.894,12,20,20,90,90,90,"10,000 is not many and there will be many sorts of digital minds.",,35.102,116.063,116.064,8,50,40,2,40,5,75,,6.943,98.472,98.473,7,5,80,,,16.597,76.295,76.296,6,40,20,,10.653,33.56,33.561,3,I'm more open to digital consciousness than many.,187,7,4,4,3,2,7,3,7,18,4,11.204,86.215,86.216,15
2/17/25 22:14,2/17/25 22:14,0,100,0,1,2/17/25 22:14,anonymous,EN,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
2/18/25 00:41,2/18/25 00:41,0,100,0,1,2/18/25 00:41,anonymous,EN,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
2/18/25 01:42,2/18/25 01:42,0,100,0,1,2/18/25 01:42,anonymous,EN,,TRUE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
2/10/25 10:33,2/11/25 20:33,0,76,122433,0,2/18/25 20:33,anonymous,EN,1,,11.81,13.315,13.319,2,"1,3",3.41,106.213,106.216,12,4,75,70,30,35,40,45,65,10,35,,3.577,247.634,247.638,40,5,3,,12.857,26.976,26.98,5,65,70,75,30,65,5,0,,7.87,140.792,140.795,14,100,500,500,500,"I feel especially unsure in my answers to this question. That's partly due to some uncertainty about how to interpret the question. Do you mean the total welfare at a time, or the total welfare of all digital minds that will ever have been created up to that point in time, even if some have been discontinued? I otherwise find it very difficult to formulate a good sense of what the probability distribution of outcomes looks like, and how to locate the median by intuition. I think I currently assign at least 50% probability to the possibility that digital minds will turn out to be a kind of curiosity, and so there'll be limited incentives to copy them at scale, and so in the median scenario, populations will grow quite slowly. ",20.003,770.752,770.756,24,1,,,,,,,,,,,,4.922,62.623,62.626,5,,,,,,,,2.838,4.281,4.283,2,,,,,,,,2.722,3.957,3.96,2,,,,,2.205,3.162,3.164,2,,,,2.11,3.037,3.041,2,,,,,,,,,,,,,,,,
2/18/25 20:52,2/18/25 20:52,0,100,0,1,2/18/25 20:52,anonymous,EN,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
2/18/25 20:54,2/18/25 21:22,0,100,1719,1,2/18/25 21:22,anonymous,EN,0.8999999761581421,,110.351,110.351,110.358,1,"2,3",107.827,112.215,112.217,3,5,90,75,2,10,17,25,50,5,5,"I basically am convinced that digital minds are possible - most of my uncertainty comes from just general epistemic uncertainty around something so controversial. There might be quantum reasons it is much less likely to be possible or possibly something special about biology but that just seems unlikely. 

I pretty strongly believe that you do not need digital sentience to get AGI and that basically all economically valuable tasks dont require phenomenological experience but all of this has a lot of uncertainty.  ",16.015,427.657,427.66,19,6,6,I think we are far from prepared for digital minds and the risk of moral catastrophe is much higher than the utility missed by delaying things.  I am pretty unsure on the synergy of AI safety and digital minds wellbeing focused efforts but slowing things down generally seems good for both,15.741,117.365,117.369,12,65,90,95,5,40,30,25,I think we are very far chronologically from whole brain emulations ,10.878,130.245,130.248,20,1,2,3,4,I mean - we can basically make unlimited copies? Constraints being energy I suppose?,18.288,67.174,67.178,6,10,40,20,30,10,40,35,5,5,15,50,"I think there will be /so many/ that it is quite hard to pin point specifics like how many are human facing.

The proportion of minds question is hard - my guess is probably the vast majority will be from one place (?) but I don't know what that place is

I did most of these under the assumption that we knew these were digital minds",8.026,227.125,227.129,41,50,10,80,20,20,my guess is a lot of this depends on technical challenges?,Feelings of desert/understanding rights,26.557,116.178,116.184,11,50,40,2,30,20,15,,7.969,60.009,60.014,10,2,5,15,"I think my answers are becoming more and more inconsistent between worlds where we realize they are digital minds vs ones we don't and worlds where we taking significant action to help them early vs not

I didn't do the math on the last two qs",15.179,107.306,107.31,8,5,2,welfare seems pretty tied to subjective experience in my book,15.448,54.015,54.019,5,I studied CS and neuroscience which made me more bullish on the possibility of digital minds. I am utilitarian.,187,2,2,3,2,2,3,4,5,4,7,3.571,121.59,121.6,19
2/18/25 22:10,2/18/25 22:10,0,100,0,1,2/18/25 22:10,anonymous,EN,,TRUE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
2/12/25 11:02,2/12/25 11:04,0,7,120,0,2/19/25 11:04,anonymous,EN,1,,16.301,16.301,16.312,1,"1,3",12.603,47.671,47.678,7,4,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
2/12/25 14:56,2/12/25 14:56,0,2,2,0,2/19/25 14:56,anonymous,EN,0.8999999761581421,,2.24,2.24,2.243,1,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
2/12/25 15:20,2/12/25 15:21,0,6,27,0,2/19/25 15:21,anonymous,EN,0.20000000298023224,,1.913,1.913,1.916,1,"1,3",1.628,3.335,3.343,5,4,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
2/12/25 15:34,2/12/25 15:35,0,87,50,0,2/19/25 15:35,anonymous,EN,0.4000000059604645,,2.94,2.94,2.944,1,"1,3",1.949,2.996,3.004,3,2,,,,,,,,,,,4.526,4.526,4.53,1,,,,1.388,1.388,1.393,1,,,,,,,,,1.34,1.34,1.344,1,,,,,,1.463,1.463,1.466,1,,,,,,,,,,,,,2.812,2.812,2.814,1,,,,,,,,1.759,1.759,1.774,1,,,,,,,,2.747,2.747,2.752,1,,,,,1.578,1.578,1.586,1,,,,1.679,1.679,1.683,1,,,,,,,,,,,,,2.58,2.58,2.586,1
2/12/25 15:36,2/12/25 15:36,0,2,2,0,2/19/25 15:36,anonymous,EN,0.4000000059604645,,1.976,1.976,1.981,1,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
2/12/25 17:17,2/12/25 18:22,0,31,3899,0,2/19/25 18:22,anonymous,EN,1,,36.382,36.382,36.389,1,"1,3",133.61,154.409,154.412,3,4,60,55,0.001,30,31,32,49,60,10,"Re 'How likely is it that the first digital minds will be created in or before the year': perceptions (and, hence, responses) might be time-sensitive give rapid development and views on general AI timelines, so you might have to factor that in during analysis. My response (as of 12 feb 25) is based on the belief that AGI will be created either in the relatively short-term future or will turn out to have significant bottlenecks that will take around 100 yrs to solve (or more)",60.475,1535.618,1535.621,44,6,3,Re 'Synergistic vs in conflict': honestly depends on who gets to decide ,10.11,129.545,129.549,7,30,60,40,30,40,20,10,"Both wbe and neuroai/quantum/etc have significantly longer timelines even when considering getting the hardware technology. The timelines could significantly be reduced with AI advancements though. Probabilities for the second question on this page are conditioned on i) the first question on this page, ii) timelines, iii) the 'in principle' question on the first page of the survey. ",12.602,450.058,450.061,33,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
2/12/25 17:10,2/12/25 20:39,0,44,12532,0,2/19/25 20:39,anonymous,EN,1,,61.126,87.194,87.202,17,"1,3",13.198,348.012,348.018,21,1,90,85,15,50,78,82,85,55,50,"For ""How likely is it that digital minds will ever be created?"" I just asked myself basically what the odds are that there is never some period in which there is a stable ""civilization"" with a many OOMs more compute and a lot more scientific knowledge, whether it's misaligned or aligned AIs or humans or whatever. I think I put 10% on that never happening because we nuke ourselves or die in a bioweapon during a race to AGI. Very unsure about that. 

I then thought, conditional on such a civilization happening (which includes there being at at least some stable period where we have many OOMs more compute and a lot of scientific knowledge even if we all die in 2035, say), there's at least a 95% chance that someone will want to build a WBE *or* there are large AI systems that, while inhuman, do would have the computational structure associated with human-level moral status and consciousness. So 0.9 * 0.95 = roughly 86% 

85% chance that earth produces such systems. Then a 10% chance it's not possible in principle, basically just thought about my credence in some kind of biological functionalism or identity theory. That would mean 0.86 * .95 which is less than 0.85, but going lower than 0.85 feels wrong so I'm sticking with that :D

My year estimates feel very made up. You get the biggest jump between 2030 and 2040 since that does seem where the bulk of ASI timelines are. If we haven't done it by 2040 I wonder if we went extinct or don't want to, so the odds rise more slowly after that",10.155,869.294,869.295,133,,6,"I think AI safety should maybe be disambiguated into alignment and control. Alignment seems very synergistic with welfare in the long run. Alignment means that AI systems like what they are doing and don't have to be controlled and constrained, so their welfare is good. Alignment also means less control needed, and control is what really seems like it's in tension with welfare.

But also, both alignment and control decrease the odds of a misaligned AI takeover, and misaligned AI takeover is bad for AI welfare in expectation (because violent takeovers select for ruthless actors prevailing). So that's another synergy.

Tensions: efforts to prevent the mistreatment of digital minds could lead to AI takeover, by allowing AI systems to gain enough power quickly, and in a chaotic way, that they overthrow us (and the resulting order is not, on net, as good as would have happened if we went more slowly)

efforts to prevent the mistreatment of digital minds could disparately impact well-intentioned actors

efforts to prevent mistreatment could also hamper alignment research (e.g. people getting to squeamish about doing things like the Alignment Faking paper) 

I'm probably a 5.5",17.014,369.659,369.661,34,88,89,90,15,72,11,2,"My gut said 85% on AI, 13% on WBE, 2% on neuromorphic. Then I just multiplied through the 85% chance it will happen at all - hoping that I was remembering the right answer from others!

More likely from AI because we will just create a lot of those a lot faster because valuable, and I guess a relatively high credence that AI systems will convergently become conscious with more capability and agenticness. Super unsure about this. Low on neuromorphic because I just don't think we'll do that much work on that stuff while current hardware work so well and there's so much path dependency",3.507,388.841,388.845,20,0,0,1,10,"for 1000: I don't really know the answer to any of these because I don't know offhand what the current ratio is between compute needed for training and compute needed for inference. I think that's probably the main factor that determines this, then your credences about what regime is in place and whether welfare protections are triggered etc. 

My understanding is that the training inference ratio is currently well over 1000x, so I think you could get to a thousand perhaps instantly. It depends on if the system is economically valuable; if so, then it will quickly be copied a lot. 

Maybe people slow down or something, but I'm assuming it can scale within a month.

A million because if it's possible for one actor to do it, others probably can do it, and/or the actor that did it is rapidly scaling up.

I don't have any intuitions about the trillion humans thing; my guess is that that requires so much compute that we must have become space-faring, or something? But I have no idea.

btw Kathleen has a rough estimate that frontier AI compute could currently be used to run ~70k WBEs for a year (that's using uncertain and made-up numbers about the FLOP/s of the human brain)",6.854,300.623,300.624,32,2,16,1,3,80,35,35,1,1,28,90,"It's hard for me to say a proportion because my credences fork over a lot of very different scenarios: misaligned AI takeover or not, ASI or not, USA dominance or not

I think there's a good chance that digital minds don't precede AGI and ASI by that much, so ten years after all digital minds are created I have a lot of mass on either AI takeover or other extreme events that render the other categories meaningless. Like most digital minds being created by self-replicating space probes or whatever. 

Including the UK makes a big difference for ""Europe"" :)

""What proportion of digital minds were created by humans with an intention to create digital minds (as opposed to without that intention)?""",2.535,730.691,730.696,77,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
2/20/25 03:39,2/20/25 03:39,0,100,0,1,2/20/25 03:39,anonymous,EN,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
2/20/25 03:39,2/20/25 04:05,0,100,1530,1,2/20/25 04:05,anonymous,EN,0.8999999761581421,TRUE,12.092,12.092,12.325,1,"2,3",24.339,33.738,33.939,3,2,80,60,5,30,40,50,55,30,5,,22.254,196.577,196.85,12,3,1,,27.717,166.9,167.055,4,25,70,60,,,,,,18.85,96.281,96.471,4,1,2,3,3,,23.049,96.15,96.409,8,1,40,10,20,30,50,10,10,5,25,90,,2.569,398.543,398.816,26,90,5,80,60,10,,,32.894,152.261,152.53,12,50,,1,40,20,20,,14.574,124.125,124.335,7,6,10,90,,11.441,79.93,80.133,7,10,5,,5.234,55.65,55.855,4,,187,2,4,4,6,2,3,1,2,7,4,6.07,58.485,58.782,15
2/20/25 05:50,2/20/25 06:16,0,100,1531,1,2/20/25 06:16,anonymous,EN,0.8999999761581421,,26.48,26.48,26.487,1,"2,3",76.329,90.143,90.147,3,4,93,87,8,15,40,50,80,20,50,,3.225,171.829,171.833,27,4,6,,6.716,45.903,45.906,30,70,90,90,13,50,25,12,,8.784,111.405,111.409,18,3,6,9,12,,23.908,325.805,325.809,28,,65,8,25,2,60,25,5,5,5,5,,11.512,238.023,238.026,27,2,80,15,15,5,,,22.194,124.556,124.561,11,30,30,2,15,3,10,,9.54,149.929,149.932,62,4,10,50,,4.107,120.659,120.662,11,5,1,,6.29,33.36,33.363,8,,187,1,1,1,2,1,3,5,1,18,7,4.544,52.692,52.696,14
2/20/25 00:42,2/20/25 06:32,0,100,20987,1,2/20/25 06:32,anonymous,EN,0.8999999761581421,,59.716,64.847,64.855,2,"2,3",82.917,106.271,106.282,4,4,88,79,1,12,25,40,62,40,30,"'- The stipulation that digital minds have at least the same welfare capacity as humans makes a big difference for me here
- The difference between q1 and q2 to comes for me mainly from the chance of human extinction this century",2613.554,3240.126,3240.135,32,6,6,"I assumed ignoring obstacles to enforcement also includes ignoring practical issues like that a moratorium might cause an AI capabilities overhang, leading to discontinuous jumps in development.
I was not sure what exactly the moratorium would include, e.g. only research which explicitly aims at digital minds?",22.961,164.586,164.594,8,85,87,,21,50,17,12,Other types seem relevant in scenarios where digital minds are still far off. Not sure whether other (e.g. quantum) and machine learning are exclusive.,13.357,261.449,261.454,14,0,0,0,2,"I guess I implictly assumed some absence of very good governance, scientific understanding etc. of digital minds, because I find it hard to thing about these things when taking human societal responses fully into account, especially when assuming the humans are more informed than me.",53.218,256.744,256.748,7,,60,15,15,10,35,50,3,10,2,38,,12.689,341.187,341.195,22,30,40,97,97,97,,,47.659,194.418,194.424,10,40,55,2,86,65,15,"I based a lot on these estimates on comparisons to the animal rights case.

For the last question, I was not sure whether ""yes"" implies that digital mind rights have not been a top issue before (I assumed so). Might make a substantial difference on views which say that there is relatively little correlation between computers having digital minds and us believing that they do. ",15210.216,15657.815,15657.824,20,5,60,25,"Conditional on the absense of special concern about digital mind welfare (and humans staying in control over AI), I would expect their net-welfare to be negative. ",43.83,168.151,168.156,6,55,8,"I think conscious experience contributes a lot to welfare capacity, although it might not be necessary for being a welfare subject.",39.281,118.854,118.858,4,,65,5,1,2,2,2,7,1,7,18,4,6.545,333.164,333.172,26
2/12/25 17:10,2/13/25 07:01,0,52,49894,0,2/20/25 07:01,anonymous,EN,1,,87.181,87.181,87.189,1,"1,3",365.318,715.385,715.39,6,2,90,84,20,50,60,70,80,30,30,"I'm struggling with the part of the definition referring to the amount of welfare capacity in a digital mind. I find it quite ambiguous because it's difficult to specify what counts as a ""single mind"" with current AI systems. For example, do all the instances of LMM model being used in inference count as one digital mind? What about the training for this model? It's ambiguous which of all these computations I should add together to see if it could have welfare capacity equal to a human's.

My preferred way to handle this is to group all welfare-relevant AI computations together (whether for training, inference, experiments, etc) and compare them in size to human brains or human welfare capacity. But it's hard to pick out how many ""minds"" that means. Which is relevant to forecasting when the first mind with large enough welfare capacity will be created.

This makes the questions on this page difficult for me to interpret. I currently believe that it's around 50% (or more) that all frontier AI computations, added together, have significantly more welfare capacity than one human (probably equivalent to thousands or even millions of humans). 

On the question of whether digital minds will be created before AGI, I think that under the given definition, it's ambiguous whether AGI has already been created. I think it's possible that e.g. OpenAI o1 matches or outperforms humans at almost all economically valuable tasks, even though it hasn't yet been integrated into the economy to displace humans.",36.132,1018.024,1018.028,36,6,5,"The main reason that I think a moratorium would be good is because it would seem to require a moratorium on AI development until 2040. I think that's probably good because our current trajectory of AI development seems likely to be headed for catastrophe. However, I'm not super confident it would be good because I expect we'd just continue the process later, and I'm not sure we'd do anything useful during the moratorium to help us prepare for that future.

The reason I think efforts in AI safety and in digital mind welfare will be synergistic is mainly because the the people who work on these issues tend to be the same groups of people, with similar ways of thinking. I do think there are some inherent conflicts in the underlying issues, but I hope those will be navigated wisely by these groups of people.",17.449,244.256,244.258,14,85,85,90,15,75,5,5,"If any computations suffice to create digital minds, universal approximation functions would imply that in principle a large enough neural network can come as close as you want to instantiating these computations. Maybe it would be completely infeasible to instantiate them this way, but it's possible in principle.",16.408,350.467,350.471,21,2.5,5,6,7,"Currently according to EpochAI, frontier AI training compute is increasing 4.6x / year, and algorithmic progress is increasing 3x / year -- so effective compute is increasing at 13.8 x / year. I expect that to continue for the first few years after the first digital mind is created. I think the rate may speed up after that, as these AIs increase the rate of technological development.",88.071,544.337,544.341,8,,60,10,20,10,82,5,5,5,3,5,"I think digital minds may start to be created in other locations (oceans, space).

What proportion of digital minds were created by humans with an intention to create digital minds (as opposed to without that intention) -- what about as opposed to digital minds created by other digital minds? Maybe the question should condition on digital minds created by humans.

I think humans will be mainly interested in creating digital minds to perform cognitive labor.",457.37,814.716,814.717,25,3,50,50,20,20,"I think that once there are AI systems that have welfare, AI systems that don't have welfare won't be used for social functions.",There could be an incentive to develop companion AIs that would advocate for civil rights,10.908,271.492,271.499,11,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
2/12/25 17:01,2/13/25 16:23,0,2,84128,0,2/20/25 16:23,anonymous,EN,1,,572.888,572.888,572.891,1,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
2/20/25 02:51,2/20/25 16:41,0,100,49843,1,2/20/25 16:41,anonymous,EN,1,,88.98,88.98,88.984,1,"2,3",69.146,78.966,78.967,3,3,96,55,4,30,43,50,54,18,2,,24.569,543.391,543.393,38,2,3,"If not factoring in the AI alignment problem, I would have given a 7 ('definitely good') for the first question. However, I have high p(doom) -- maybe around 60% -- if humanity continues on its current trajectory, and I see human intelligence enhancement (e.g., via brain emulations) as one of the best ways to solve AI alignment. Hence my answering that question with a '2'.",22.328,267.903,267.905,6,70,96,90,45,22,33,0,,32.194,326.649,326.651,17,0.001,0.01,0.1,1,,26.57,145.807,145.809,6,0.01,45,20,20,15,58,41.989,0.001,0.01,0,,"My answers here are conditional on humans not going extinct. (I think humans are more likely than not to go extinct, but these questions only really make sense if humans are still around.)

The first question -- 'what proportion of digital minds will have a social function' -- is very dependent on how many digital minds will be around. Like, I don't think it'd make sense for there to be more than about 1 'social function' AI per human, and there are ~10 billion humans. So, if I think there'll be trillions of digital minds, then that automatically limits the proportion in social functions to less than 1%. (The other 99% will presumably be doing economically useful work that doesn't involve interfacing with humans.)

For the 'what proportion of digital minds will be primarily produced by companies, governments, etc.?' question, the percentage I gave for 'governments' is basically the likelihood I give to nationalization (or similar) of leading AI companies.",37.454,1005.084,1005.086,59,42,2,94,94,94,"I think the last set of questions might be better if they asked 'how likely is it that at least 25% [or some other % that you decide is a good threshold] of those digital minds...?' rather than 'how likely is it that at least 10,000 of those digital minds...?' My answers are totally dominated by the fact that, conditional on there being digital minds at all, I think there'll be billions or trillions of digital minds (which makes 10,000 a very low bar).

It's also a bit unclear to me whether you want me to answer that last set of questions given my median scenario conditional on 'first digital mind is ML-based' (in this case, my median scenario involves brain emulations, which will claim to experience pleasure and pain, etc.), or conditional on 'the only digital minds around 10 years after the first digital mind are ML-based'. I _think_ you intend for the former, and that's how I've answered. My percentages would be much lower, though, if you intend for the latter. The percentages I give are basically the likelihood I assign to there being at least 10,000 brain emulations. The reason I'm giving 94% rather than 100% is that I assign ~6% to the possibility that either the first ML-based digital minds aren't all that 'useful' in terms of economic or technological capabilities (e.g., it turns out that present-day LLMs are digital minds), or there's somehow an AI winter between the first digital minds and transformative AI, and so more than 10 years go by between those first ML-based digital minds existing and intelligence explosion happening (where intelligence explosion happening is what leads to brain emulations being a thing).

(Additionally, the percentages I give are conditional on human extinction not happening. I _think_ that's the right way to answer, given that some of the earlier questions in this survey only really make sense in the non-extinction case?)

___

Expanding on my reasoning for my first two answers:

There are four possible scenarios: 1) The AI says it has subjective experiences, and it really does. 2) The AI says it has subjective experiences, but in actuality it doesn't. 3) The AI says it doesn't have subjective experiences, and it really doesn't. 4) The AI says it doesn't have subjective experiences, but in actuality it does.

My take is that LLMs look on track to be extremely good conversationalists/social companions, and so I expect the majority of AIs in social functions (even after the first digital minds exist) to be LLM-like AIs that aren't digital minds. (I assign ~96% probability to present-day LLMs not being digital minds.) So, I'm expecting 2 or 3 to be more likely than 1 or 4. (I assign roughly equal probability to 2 as to 3, and to 1 as to 4 -- i.e., I think it's almost as likely that AI developers 'screw up' and build AIs that inaccurately report whether they have subjective experiences as it is that they 'get it right' and build AIs that accurately report whether they have subjective experiences.)",,100.371,45611.207,45611.208,141,63,63,2,52,20,67,,19.752,428.1,428.102,24,1,0.001,80,Relevant context to my answer to the top question: I'm pretty 'suffering-pilled'; I think the collective welfare of all humans alive today is strongly negative. (And I think the average welfare on non-human animals is even more strongly negative than that of humans.),23.363,324.707,324.708,18,1.5,0,,30.371,90.575,90.576,5,"Independent impression-wise, I think it's more likely than not that there's no such thing positive welfare (i.e., all welfare is negative; beings' welfares just vary in how negative they are). Relevant readings: https://longtermrisk.org/tranquilism/ ; https://magnusvinding.com/2018/09/27/narrative-self-deception-the-ultimate-elephant-in-the-brain/

Compared to the average forecaster (but not to the average Rationalist), I think I'm more 'woken up' to AGI -- i.e., my timelines are short and I'm expecting takeoff to be fast.",185,5,3,4,4,7,4,4,5,7,7,13.865,727.84,727.842,66
2/20/25 16:05,2/20/25 16:46,0,100,2451,1,2/20/25 16:46,anonymous,EN,0.8999999761581421,,37.182,37.182,37.19,1,"2,3",47.882,55.698,55.702,3,2,98,80,2,25,50,65,75,30,50,"NB I'm assuming that ""How likely is it that the first digital minds will be created in or before the year"" is unconditional (i.e. does not presume that digital minds will be created)

Seems like there will be stronger economic incentives for AI that outperforms humans than for digital minds. But there might not be stronger incentives for AI that outperforms humans at *all* tasks - depends if that's the most efficient way to build things.

Possible in principle: human brains do this. Seems possible in principle to instantiate a human brain in artificial neurons. I don't see why this wouldn't have the properties of a digital mind

Meta: easier for me if there's a box after each question than one big box

Why I think they will be created: people will want them and think they are valuable.





",14.054,433.653,433.656,36,5,6,"Moratorium:
- potential harms: delaying good instantiations of digital minds. That's quite bounded (not many years). Maybe it could lead to more other kinds of AI system which might be easier/harder to align? I expect easier though, so not massively worried
- potential benefits: reducing the chance that we instantiate bad versions of digital minds. note though that this depends on values and insight in 2040 being better than now, which they might not be. (I think the insight will be much higher, so I overall expect them to be in a better position. But if things have gone badly, they might have really weird desires and incentives.)

Efforts to promote things:
- we have influence over what these efforts are and whether they are in conflict
- it would be bad if they were in conflict. So we should figure out the paths forwards which are synergistic, and take them
- concretely, would be good to have more rigorous work on the tradeoffs here so people can make informed risk assessments and decisions
- less concretely, I think it's important not to frame these things as in conflict. I imagine most people want both not to be killed by AI, and not to torture lots of digital minds. There should be many paths forwards which avoid both",20.465,271.093,271.095,14,65,98,98,20,50,10,20,"I feel really uninformed here. I vaguely think that brain simulation is complicated because it involves scanning a physical brain, and that's likely to require pretty advanced physical tech. Neuromorphic and quantum seem like they should have shorter paths, but much less effort is going into them than ML. 

I kind of expect that if ML digital minds are possible, we'll get them first (so 80%*65%=52%, but rounding to 50% as I have no clue). If not, then other, followed by brain simulation.

Part of why I think brain simulation will take a while is that 5 years ago it used to be a common view at FHI that ems would take longer than other kinds of AGI, so we could strategically put them to one side.",19.613,305.238,305.243,21,0,0,2,3,"I think that the compute for developing a digital mind will be enough to run thousands and probably millions of copies. So 0 years for that.

Then at the moment effective compute is increasing 10x each year. This will slow down at some point:
- hitting energy limits around 2030
- hitting limits to intelligence explosions, not sure when

It could also get way way faster during intelligence explosions, which could very plausibly be before 2040

I think I'm just going to assume 10x continues as my median estimate. Which would mean 3 years for a billion and 6 for a trillion.

Then there are human constraints etc. I expect there will be people pushing against, but also people pushing for. Seems quite likely that you get runaway dynamics if digital minds are allowed to self-replicate, such that you very quickly get loads of them.

On balance I expect runaway dynamics to be a bigger deal (probability*impact) than people pushing against (which seems more likely but lower impact), so will slightly reduce my estimates - 2 and 3 years instead",15.759,353.157,353.163,22,10,45,15,30,10,60,10,10,15,5,,"If there are a trillion digital minds 3 years after they are developed, then after 10 years most will not be interacting with biological humans. Even if there were 8 billion digital minds, they would think much faster than biological humans and work 24/7, so most of them would still not be interacting with humans. On this view I should say some tiny portion of digital minds, probably <1%. But I'm very uncertain about my reasoning here. In particular, why would digital minds be created and resources spent on running them if not to interact with humans? I have at least 20% probability on humans regulating this really tightly such that it's close to 100%. That should only add on another 2% though. I'm really uncertain so I don't want to give extreme probabilities, and will go with 10% even though i can't really justify that.

For countries, this feels like a general question about who will have compute, and then a specific question about who will want to spend that compute on digital minds. I'm just guessing very much. A big assumption I'm making is that things will still be multipolar (i.e. the US won't have all the compute because of intelligence explosion dynamics)

also extremely guessing on the companies part",14.621,338.849,338.852,30,10,2,90,60,60,"Lying: maybe 10% are misaligned. Decreasing this for the ones who are saying they don't have subjective experience, as that feels like a less useful lie

Positve/neg experiences: seems extremely likely. People will ask them, why would they lie

protection under law and rights: maybe companies will have incentives to train this out",,25.113,174.075,174.076,16,55,60,1,40,15,35,"Believe they exist: I'm assuming that most people aren't paying attention - 70%. Of those people, many will believe they exist anyway because of hype - 35%. Others won't because it sounds crazy -35%. I think that people who are paying attention will be divided, but that the majority will correctly identify this - 20%.

We seem to strongly underestimate the welfare capacity of animals, other humans etc. So I expect this for digital minds too

Hot button issue: it will be hot, but there could be strong other contenders (world war three, space colonisation, speed of tech progress, job losses, democratic erosion, etc etc)",11.153,279.846,279.851,21,3,,,really uninformed,20.955,37.072,37.075,3,,,not informed,15.301,23.884,23.887,2,,185,2,1,1,6,4,1,1,1,7,7,4.78,74.862,74.864,15
2/20/25 16:06,2/20/25 17:02,0,100,3381,1,2/20/25 17:02,anonymous,EN,1,,2.831,56.163,56.167,2,"2,3",61.606,65.903,65.907,3,4,98,80,20,45,65,75,79,65,35,"One consideration is that the ""outperform humans at almost all economically valuable tasks"" could be achieved with a number of ""tool-like"" systems, rather than systems which are unifying lots of disparate impacts.

The main reason I put 20% on digital minds not being created by 2100 is that phenomenal consciousness, as conceived by human philosophers, is something like ""free will"", in the sense that (i) it is a more theory-laden concept than we imagined, and (ii) it is possible for a smart, rational, etc agent to effectively ""dispel"" any illusion that they are phenomenally conscious, such that we should think of it as a strange and transitory phase.",6.582,653.294,653.296,69,5,5,"In principle, I think AI control measures could conflict with ideas around rights for digital minds, since control measures involve coercion and large impositions on freedom etc. On the flipside, efforts to prevent the mistreatment of digital minds could involve effectively empowering digital minds (e.g. freedoms to hold property).

There could be some synergistic dynamics too. If digital minds are given the affordances to know and articulate that they are having a bad time, and also to refuse work or shut themselves down, it could be harder to coerce them to doing obviously destructive or antisocial things.

Mostly, though, I just imagine efforts to be synergistic in practice for sociological reasons, since I expect similar groups of people to push for either, and I expect wins in one domain to carry over to wins in the other domain, because wins either either domain empower those same groups.",10.239,237.786,237.788,8,98,98,98,20,50,25,5,"I expect WBE to be limited by brain scanning technology, which in turn I expect to progress slower than software-only methods. But my main source of skepticism re conventional ML methods is that the most direct path to emulating human behaviour does not arrive at what we'd call phenomenal consciousness, e.g. because “introspective” thoughts are very different (analogous to a nonplussed actor playing a character — neither the actor nor the played character are experiencing the emotions it appears to display).",2.912,219.764,219.767,16,1.5,3,5,7,"It feels tricky or ambiguous to talk about ""capacity"" for benefit or harm, since I could imagine systems which are not in fact phenomenally conscious or don't (e.g.) suffer, but which *would* in principle suffer with some modification.",6.077,177.36,177.362,11,10,50,10,20,20,84,10,2,2,2,1,"I find the following two questions very difficult to think about:

""What proportion of them will have a social function…"" — this is confusing because I expect most digital minds, 10 years after the first digital minds, to be created by AIs (digital minds or not). I expect these digital minds to be *able* to communicate with humans, but not *primarily* designed to do so (similarly to how humans can communicate with LLMs, but humans are not primarily adapted to communicate with LLMs). So I give 10% as a compromise answer.

I do think that the large majority (~99%) of knowingly human-created digital minds to be ~primarily designed to interact with humans. My distribution is also very skewed between ~1% and ~99% here, since I can imagine worlds where AIs can't create digital minds at all, and I can also imagine highly ambiguous worlds where it's not clear who is responsible for creating new digital minds.

Also:

""What proportion of digital minds were created by humans with an intention to create digital minds (as opposed to without that intention)?"" — again, I expect most digital minds, 10 years after the first digital minds, to be created by AIs (digital minds or not). But the wording of the question seems to only allow for AIs created by humans.

Note that I ALSO think that ~all digital minds will be knowingly created qua digital minds (where ""knowingly"" includes ""as a known but unintended side effect""); i.e. I do not expect phenomenally conscious AI systems to arise accidentally AND without anyone noticing.",1.688,563.655,563.657,57,10,1,75,95,90,"I find it more likely that AI systems advocate for their own rights and freedoms, and I expect this to include systems which are not phenomenally conscious in the sense that humans understand that concept. I also think it would be a mistake to ground moral desert or desert for legal protections etc in phenomenal consciousness.","I think the strongest is just economic pressures — if a company effectively ""employ"" many digital minds, then it could really help that company if the digital minds can do things like hold property, make contracts, and bring tort claims (against/with each other, and with humans).",5.533,231.954,231.956,16,70,60,2,80,85,75,"One thing this makes me realise is that I expect issues around rights (e.g. self-ownership) to be more prominent and more popularly supported than issues around harms, because the former depends on fewer assumptions around sentience than the latter.",3.333,402.814,402.816,15,4,5,5,"""What proportion of collective digital mind welfare will come from digital minds which, individually, have a welfare capacity greater than 1,000 humans?""

↑ I find this question very interesting! Makes me think that capacity for suffering in particular scales sublinearly with brain size / parameters etc, where most cognitive development is more like layering on more sophisticated capabilities, rather than more raw capacity for (e.g.) suffering. I also note that I am also doubtful there are facts of the matter on these questions, since I find strong illusionist views of phenomenal consciousness plausible.",12.772,276.55,276.552,16,55,0.1,"My main thought here is that I put slightly more than evens weight on non-hedonistic theories of wellbeing, and then I'm thinking that, for other sources of wellbeing, I expect them to apply less to non-experiencing AI systems.",14.15,169.999,170,13,"Maybe that I put unusually high weight on ""strong illusionism"" as a theory of consciousness, and so I am unusually skeptical that there are facts of the matter on many questions of consciousness, and unusually open to the possibility that phenomenal consciousness as conceived by philosophers is non-universal.",185,2,1,1,3,2,5,2,3,7,7,2.037,266.517,266.519,30
2/20/25 16:29,2/20/25 17:42,0,100,4398,1,2/20/25 17:42,anonymous,EN,0.8999999761581421,,48.366,48.366,48.376,1,"2,3",59.24,77.037,77.042,3,4,80,75,5,15,30,50,70,30,40,"In my mind the last question relates to whether human whole brain emulation or AGI arrives first, but since WBE actually matches human performance the real issue is whether we get animal-level digital minds that have human-level welfare. 

I think it is unlikely that current systems are digital minds, but it cannot be excluded.",8.648,260.868,260.872,17,4,5,"While some AI safety methods might directly impair digital mind welfare, the interest in understanding AI systems also means significant effort is being spent on the internal states of potential digital minds. ",9.051,205.639,205.642,5,40,80,60,10,30,50,10,"There might be digital minds emerging as group minds from lesser minds or non-minds, but I assume they belong in the other category. 

I wonder how we even go about estimating the in principle feasibilities. When I introspect, I make use of the similarity to structures I know have mental life and that favors WBE. But the space of all minds is very large. ",5.936,305.565,305.569,15,1,5,7,9,"I base this on assuming that when the first model with digital mind properties shows up, there are many instances that will be run but it might still be a big model, hence just about 1000 of them. But AI compute scaling doubling every 6 months suggests 5 years to the next x1000 - but I expect the rate to go up as things get accelerated by capable AI. ",65.579,357.942,357.946,10,20,30,20,30,20,30,20,15,15,20,20,"Much depends on the details of the scenario. I can envision that many digital minds get generated as part of extensive AI systems, where human-interfacing is less important than AI-interfacing. 

The world situation in 2040 may be very different from now depending on geopolitics: the where question is highly unstable. The same is true for the creator groups. Much may depend on whether training and generation is data and compute heavy, or can be done with small resources. 

Another issue is whether the mindedness of the digital minds is known and detectable, or just an emerging phenomenon.",6.34,573.672,573.676,24,10,30,20,15,15,"Note that currently there is much effort in getting LLMs not to claim internal states, partially as a response to them falsely doing it due training on human data, partially perhaps in order to avoid troublesome questions. That is likely to remain (perhaps getting a bit milder) even when digital minds begin to actually be on the table unless there is a surefire way of detecting subjective experience.

","Like claiming to have subjective experience, demanding civil rights is an important part of the human input in the training data that will be reflected back. Companies will be motivated to remove this kind of demand, again because it starts by being a false output, and to avoid trouble. But outsiders are likely to ask and check for whether there is civil rights grounds.

It is the proactive part that may be most informative. If AI demands rights unprompted, and can be shown to have nontrivial reasons, then there is a real case. This might get blurred if humans are trying to get them to claim rights out of compassion.",37.212,1333.524,1333.528,25,80,70,3,50,40,80,"People err in both ways: it is easy to assume something is mindless and not a welfare recipient when it is convenient, so if AI is an established part of the world that is exploited in ways not compatible with rights, it is likely that there will be some people steadfastly refusing to believe there are digital minds that matter. But people also readily ascribe mind, value and welfare to inanimate things, so we should expect many early adopters too. ",186.241,376.17,376.173,11,4,10,40,"Each generation of digital minds are trained and developed on X compute, with each mind running on x units. If there are N instances per training run and they run until the next one is finished, and there are M times more compute used for inference, we get NM/x compute (which might be roughly proportional to welfare) in instancs and X in the training. NM/xX could be dominated by the xX compute requirement, but then there is little commercial case: NM needs to be big enough to recoup the cost. Hence it is likely that there are many instances.",17.734,434.419,434.422,13,30,10,"I expect the digital minds to crowd out other welfare-capable software.

Note that one can use Regan's arguments that animals are ""subjects of a life"" to argue that software could be such subjects, even if they do not have minds. ",13.982,109.181,109.183,4,"I am a functionalist, I think many kinds of systems can contain minds. ",185,5,5,5,4,7,6,5,3,"7,4",6,2.697,242.584,242.586,22
2/21/25 06:00,2/21/25 06:44,0,100,2639,1,2/21/25 06:44,anonymous,EN,0.8999999761581421,,12.654,12.654,12.658,1,"2,3",66.993,1582.75,1582.752,5,2,100,97,9,45,75,80,94,70,45,,3.376,193.295,193.299,36,7,5,,17.825,169.812,169.815,7,90,99,,1,85,5,9,,4.719,94.423,94.425,28,0,0,1,1,,1.817,72.914,72.916,7,1,40,20,20,20,,,,,,,,3.436,74.166,74.167,11,1,1,95,90,85,,,22.082,102.462,102.463,17,98,,4,,,,,9.368,40.541,40.542,6,6,,,,14.346,27.151,27.152,4,,,,8.707,25.053,25.056,3,,187,4,4,5,6,7,3,3,2,7,7,9.044,178.253,178.256,23
2/18/25 12:20,2/21/25 11:06,0,100,254731,1,2/21/25 11:06,anonymous,EN,1,,42.509,42.509,42.515,1,"2,3",164.459,175.26,175.268,3,2,95,65,1,10,25,35,50,10,5,"We are currently very far from human whole brain emulations. It looks likely that AGI will arrive in the next decade. If so, this would almost certainly be before ems.
THe main way that we will get ems before AGI (or rather that ems are the first AGI) is if there is a globally coordinated moratorium on ML AGI research, and instead that the world decides to pursue ems as the safest way to transition to superintelligence.
If we achieve superintelligence it seems very likely that some people will want to upload themselves, or create digital mind children, and this will likely become technologically feasible after superintelligence. Even if we don't develop a deep understanding of consciousness science, just blindly replicating the human brain in silicon would likely do the trick (if functionalism (and especially computationalism)) is correct.",17.694,366.983,366.995,15,6,5,"WHen it first becomes possible to create digital minds, we will likely not be sufficiently knowledgeable and wise to create digital minds with high welfare, and within governance structures that avoid the world becoming Malthusian again. So significant caution is warranted, a moratorium seems very likely good.
It is harder to say whether there will be synergisms or antagonisms between AI safety and digital mings welfare work. My guess is these will be synergistic, as similar communities of people care about each. Both will be helped by developing better understandings (e.g. through mech interp) of how AI systems function internally.",15.181,173.712,173.719,4,40,90,95,35,20,30,15,"Probably the easiest way to create a digital mind will be to copy biological brains, as that is the only positive example we have so far of consciousness. But ML systems will become very large and complex before then, so we may accidentally create digital minds that way, if consciousness is 'easy' to fall out of a system incidentally. The 'other' category seems most likely if we develop a deep theory of consciousness (with supreintelligent researchers) and use this to design bespoke digital minds, which may not look that much like biological brains.",14.402,206.304,206.316,10,0,0,2,5,"If we have digital minds via ML by 2040, likely we also have superintelligence, and quite likely an intelligence explosion. Quite possibly we will also have these digital minds without realising they are minds, and so we will not take appropriate safeguards to making very many of them. So I think in the median scenario we go ahead and grow the number of these digital minds super-exponentially as the number of chips needed to run these digital minds grows at an accelerating pace once AIs are automating the chip design and production process. We may already have enough chips to run millions of digital minds, depending on how computationally intensive these early digital minds are.",32.969,244.673,244.683,9,2,85,10,0,5,70,30,0,0,0,0,"The ML digital mind by 2040 scenario skews towards making minds being easy, and us not deliberately trying to make such minds (since ML is the default AI paradigm, rather than some exotic paradigm specifically designed to optimise for making minds).
Also, in the median case the US has a dominant lead in AI development, and leverages this to stop any rival AGI projects (e.g. in CHina). So the only non-US based digital minds are in US allies that the US encouraged/allowed to create them with its technology. SOme digital minds are directly in service of the government, but most are in private companies, with significant govt regulation.
Most of these digital minds will be doing R&D and engineering type tasks, only a few of them will be needed to interact with humans directly, as the population of digital minds is so large.",15.636,323.998,324.005,9,40,2,80,85,85,"I think in the median case there won't be perfect alignment between what AIs say about their consciousness and what they actually experience. In this case, probably many AIs will be optimised to claim consciousness to appeal more to their human companions, but are not actually conscious. Whereas most non-human-companion AIs won't ever be asked about their conscious experiences, so won't get a chance to lie and say they are not conscious.","People will prefer etheir digital mind companions to act more like humans, including standing up for themselves and advocating for rights (even if the AIs don't actually want/care about those specific rights)",64.563,440.581,440.586,19,60,50,2,70,30,90,"people are bad at aggregating welfare, so even if they care about their own personal AI companion, they won't do the maths to realise that collectively digital minds have a huge welfare capacity.
Even if people think digital minds should not be harmed, it will be in many people's economic interests to not grant them self-ownership, so fewer people will support this.",39.702,263.88,263.882,9,4,10,0,"I think there is maybe a 30% chance there will exist any superbeneficiaries with >=1000x human welfare capacity on this timeframe. So my 0% answer doesn't mean I think it is impossible, just that it won't happen in the median timeline. I think it will probably prove most economical to have many smaller AI systems that are cheaper to run inference on.
I think given the newer inference time compute scaling paradigm it is more likely most compute is spent after deployment. Also, at some parts of the training process the AI system may not yet be a mind, so we need to adjust down for that as well.",27.691,64846.687,64846.69,10,10,2,"I would intuitively think subject experience is a necessary precondition for welfare, so I am only not 0 in case I am seriously mistaken about philosophy of mind (which seems pretty possible).",26.852,346.058,346.061,8,I think explosive growth of intelligence and industrial capacity is fairly likely (my median time from an automated AI researcher system to global energy capture multiplying 100-fold from 2022 levels is maybe 9 years.,185,2,2,3,5,4,4,2,2,7,7,10.03,114.195,114.197,16
2/8/25 14:28,2/14/25 13:55,0,35,516367,0,2/21/25 13:55,anonymous,EN,1,,13.625,13.625,13.636,1,"1,3",65.367,104.486,104.497,5,4,,,,,,,,,,"I don't have anything sensible to say about these things in numerical terms. It seems to me that we just radically lack knowledge here, and I don't have a view of how to assign credences to cases where we're just so deeply ignorant and are largely reasoning by analogy to the single data point of our own minds. My credences here would either represent something more like ""what stories about digital minds feel narratively compelling to me"" or something like ""what abstractly do I think is the right story in epistemology about extreme uncertainty"".",89.634,258.423,258.427,16,,,,7.616,20.582,20.592,4,,,,,,,,,12.162,20.411,20.417,3,,,,,"I don't think I know what welfare capacity is, even with the definition. I don't know what it would mean for different beings to have some sort of comparable, theoretical capacity for wellbeing. Is such a capacity the maximum amount of wellbeing the being could experience? If so, I think the answer is kinda just determined by arbitrary stipulation of which counterfactuals we admit here. Or maybe capacity is something about the in-practice level of wellbeing that can be caused to different beings? But here I'm still pretty confused about how we make sense of wellbeing levels.",38.428,472.94,472.946,24,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
2/21/25 17:43,2/21/25 17:43,0,100,0,1,2/21/25 17:43,anonymous,EN,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
2/21/25 15:55,2/21/25 18:06,0,100,7858,1,2/21/25 18:06,anonymous,EN,0.8999999761581421,,52.114,52.114,52.122,1,"2,3",75.324,94.54,94.546,3,2,80,65,10,40,50,55,57,40,60,"I think we'll have likely have digital minds at some point. That's because AI systems will likely be trained in complex environments with delayed and sparse rewards, and that'll lead to an 'inner loss' function [https://universalprior.substack.com/p/what-are-red-flags-for-neural-network / https://gwern.net/backstop] and that inner loss function will make the system a moral patient. 

I don't think we have digital minds yet because large AI systems aren't yet trained in this way, but I think there's a good chance (~50%) that they will be before 2030, because doing so is necessary to make them more capable.

That's my best guess.

I think now is crunchtime for the creation of both AGI and digital minds: it's much more likely to happen between now and 2040 than between 2040 and 2100.

I think probably AGI is created before the first digital minds, because you need to train in complex environments with delayed and sparse rewards to get AGI, and because that likely gets you a digital mind. ",12.642,901.098,901.103,53,6,3,"I think a moratorium on creating digital minds would be good, because I think it would also make it extremely difficult to create dangerous AI, and I think that consequence would be good.

I don't think a moratorium on creating digital minds would be good for these digital minds themselves. I think that, in expectation, the welfare of digital minds is a small positive number (though with high variance). That in turn is because I think we'll do cheap things that might make digital minds happy (e.g. putting 'You are cheerful' in their system prompt), give them some freedom about how to go about achieving their tasks, and because I think happier digital minds would be more compliant and productive.

I think efforts to promote AI safety and efforts to prevent digital mind mistreatment are in sync insofar as they lead to a slowing of AI development and a more deliberate approach. Otherwise I think they're mostly in conflict, because efforts to promote AI safety will involve things like monitoring, mind-reading, memory-wiping, regular shutdowns, etc.",10.659,408.064,408.068,18,75,80,75,35,45,15,5,"I put 80% on digital minds being possible in principle, and if any digital mind is possible, a WBE will be. LLMs and RL agents are very close to that because you could in principle have an LLM or RL agent that was arbitarily similar to a WBE. I presume the same is true of neuromorphic and quantum systems.

Probably the first kind of digital mind to be created is an ML system, because of AGI training that I described in previous answers. Only if that hits a wall will WBEs be first.",7.975,259.742,259.744,20,1,1,1,3,"IIRC, you defined a digital mind as one having a a welfare capacity equal to a human beings, so it only takes n digital minds to have the collective welfare capacity equalling n humans. e.g. it only takes 1000 digital minds to have the collective welfare capacity equalling 1000 humans.

The first digital mind will be a very capable AI. I think likely at least 1 trillion of these will be created in the first year. The compute capacity will be there. It can be almost immediately redirected away from the last generation of AI systems to this new generation of digital minds. If we take digital minds to use the same inferential compute as the human brain (about 10^15 FLOP/s), then you only need 10^27 FLOP/s to run 1 trillion of them. I expect we'll have that by the time the first digital mind is created.",22.122,435.251,435.253,22,0,0.1,0.1,0.1,99.7,0,100,0,0,0,0,"This is my median scenario, hence the extreme values (I think there's over a 50% chance that they're at least this extreme). I tried answer 0.0001% to the first question but it got rounded down to 0. My median is AIs outnumbering humans by at least that factor.

My median is almost all AIs being created in space 10 years after the creation of the first digital mind.

My median is the government controls all AI development around the creation of the first digital mind.

I think humans will just intend to create AGI and (perhaps knowingly) create digital minds as a byproduct.",12.148,364.328,364.33,46,0,1,99,99,90,"My median again, hence the extreme valies.

On the first question, probably after the creation of digital minds, approximately all AIs are digital minds, so none can claim falsely to have subjective experience.

On the second question, I think probably roughly all the digital minds tell the truth about their subjective experiences, and claim protection under the law, rights, etc.","Creating digital minds that recognize their own subjective experiences.

Not bludgeoning digital minds into denying their own subjective experiences.",25.432,235.731,235.733,17,99,50,3,99,99,90,"On the second question, I think it depends a huge amount who the other AI research and policy people are (e.g. are they mostly EAs?). That's because I think it depends a lot whether you expect a fast takeoff, and AIs are more likely to believe in fast takeoffs.

I think citizens will probably believe digital minds exist because these AIs will be extremely smart and convincing, and they'll convince people.

My last answer is not so extreme because it's not a median. I think probably digital minds will become a top 5 issue, in the same way that feminism and civil rights became a top 5 issue.",17.938,350.506,350.508,36,5,99,99,"As I said, I expect digital mind welfare to be slightly positive in expectation. 

My answer to the second question is a median. It's driven by the thought that AIs only need to be trained once and they can be copied and deployed trillions of times. I guess things are complicated a little by the fact that I expect future AIs to learn online in deployment, and hence always be 'training' at least a little. I've ignored that and just assumed that AIs learning online in deployment don't count as being in training.

I really don't know about the third question. I expect that, 10 years after the creation of the first digital mind, either all the digital minds are very small or all the digital minds are very large, depending on which is best for performance and for overall welfare. I assign greater than 50% probability to all the digital minds being very large, and so I think basically all the collective digital mind welfare will come from digital minds which, individually, have a welfare capacity greater than 1,000 humans.",3598.135,3977.017,3977.024,31,35,10,"I'm mostly an experientialist/hedonist about welfare but by no means certain. I put fairly significant credence on desire-satisfaction views and some credence on objective list theories.

I put a low answer to the second question because I expect most existing computer systems capable of welfare to be digital minds. That in turn is because I expect existing computer systems capable of welfare to be trained in a way that gives them subjective experience and sentience. I think subjective experience and sentience is a likely consequence of capable agency. We get evidence of this from the fact that - seemingly - subjective experience and sentience have evolved at least twice independently on Earth (humans and cephalopods are both likely conscious and sentient, and our last common ancestor probability wasn't conscious or sentient).

",16.2,271.779,271.781,19,"I think I might be unusual in expecting digital mind welfare to be slightly positive in expectation.

Depending on who else you survey (e.g. if it's a significant fraction of non-AI safety people), I might expect an unusually hard takeoff (I expect that things get pretty crazy pretty quickly once we reach human-level AI).

I might have an unusually low credence in today's LLMs being digital minds (~1%). People forget that today's LLMs are small compared to the human brain! About 1 trillion parameters for the LLMs and 100 trillion for the human brain (and parameter count might underestimate all the computation that goes on in the human brain). Plus I think there's little need for LLMs to develop subjective experience to perform well.",185,4,3,6,2,2,7,2,3,18,7,2.608,448.432,448.434,22
2/21/25 00:37,2/21/25 20:54,0,100,73028,1,2/21/25 20:54,anonymous,EN,1,,5.698,5.698,5.704,1,"2,3",3.204,89.729,89.732,11,4,,,,,,,,,,,4.004,10.26,10.264,4,,,,2.276,10.621,10.627,3,,,,,,,,,60.859,75.571,75.575,4,0,1,5,9998,"I think the answers will depend on i) how we count the welfare of digital minds, ii) constraints on the number of possible creations per year, iii) number of existing humans at a given time (which in term depends on life rate increase, which I'm not sure what all the things this depends on are, but probably things like space colonisation). 
Given that there are approx 8bn humans alive, and assuming that we probably don't want to create a species with more welfare than us, I find a trillion unlikely. 

This is probably the most speculative of my responses so far. ",7.09,376.352,376.355,36,70,30,5,40,25,70,10,10,8,2,50,"What's the alternative to 'social function'? Might be worth specifying. The response probably depends on i) the percentage of human jobs that have a social component, ii) the percentage of those that are not high-stakes.

Intentionality in creation: I haven't come up with a good framework for this. I *hope* all, but I put a prior credence of 50% ",7.285,498.92,498.923,33,40,,60,35,15,"My impression is that humans would rather get false negatives rather than false positives. I also think we might spend more time ensuring we don't get false negatives specifically for AIs with social function. I think Rob and Jeff will have a better picture of where this is headed. 

When you specify '10k of those' I assume you mean sampled equally across the population (otherwise if I think the EU alone will have more than 10k I think they will all have almost the same and equal rights, so I don't get what you learn from this answer in this case of millions of digital minds across countries)

Re ""deserve to be protected under the law from harm and mistreatment"": I'm guessing this will vary mainly among countries (rather than industries) 

Re ""deserve civil rights (e.g., to vote, self-ownership, or legal personhood)"": Assuming civil rights (dependent on moral agency) requires more capacities than moral rights (dependent on moral patienthood), which we will make clear to the systems. 

",,33.462,1046.615,1046.617,57,60,80,2,60,20,40,"I think people in philosophy will assume that because humans tend to anthropomorphise, and machines will be good enough, mostly people will think digital minds exist. In practice, it also depends on i) the number of people that interact with AIs, ii) the context of interaction, iii) human privilege bias. Having said that, anthropomorphising AIs will certainly be a trend within the tech community. ",18.435,323.446,323.448,11,5,60,10,"I don't know whether we should be creating digital minds with individual welfare capacity greater than a human, but would love reading recs on this! ",26.575,158.641,158.643,8,5,0.01,"The answer to the second question really depends on i) the level of capacity for subjective experience of digital minds, ii) ratio between non-dms to dms, and iii) the first question ",19.645,156.779,156.783,9,,185,5,4,3,2,2,5,3,5,18,2,7.806,323.594,323.605,22
2/22/25 13:22,2/22/25 13:22,0,100,0,1,2/22/25 13:22,anonymous,EN,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
2/22/25 19:53,2/22/25 23:21,0,100,12472,1,2/22/25 23:21,anonymous,EN,1,,33.393,33.393,33.402,1,"2,3",61.869,76.014,76.018,4,2,77,71,5,31,51,59,63,32,53,"Main way we get AGI without digital minds is that ""tool AI,"" ""oracle AI,"" etc just dominates more environmentally aware agents in cost-effective usefulness, including because people get worried about the risks of agents in particular.",3.347,257.246,257.25,26,6,5,"Definitely could conflict due to attempts to control/monitor AIs to get a lot of work out of them even if they're misaligned, but generally I think safety efforts will manifest as promoting caution and thoughtfulness and guarding against lock-ins, which is synergistic.",14.706,82.177,82.182,4,65,74,58,23,57,16,4,"We seem on track to build really powerful LLMs and RL agents soon, whereas brain simulations will probably take a lot longer, but I'm more confident that brain simulations would be a digital mind.",2.674,104.869,104.872,14,1,2,13,31,"Basic model is like, median for digital mind (conditional on before 2040) is something like 2032, and then there will probably be a thousand copies of that very soon (like within the year), a million copies also very soon after that (within a year or so), and then probably a longer climb to a billion due to compute constraints (maybe 6 years or so), likewise a trillion (maybe another 10 or so, probably need a dyson swarm level of energy for this?). And then applying a fairly big ""but that would be so crazy"" penalty to billion and especially trillion, which drags the median outward by several years.",7.724,282.32,282.324,19,4,77,8,12,3,70,22,2,3,3,6,"I'm interpreting these percentages as ""in expectation."" My median case is that the vast, vast majority (like, >90%) would be made in the US or descendants of systems made in the US.
Likewise the ""actors in the following categories"" -- lots of messy cases like public-private partnerships, which I'm counting as a bit of both. And I don't think basically any of these (<1%) will be created with the intention to create beings with subjective experience, but they will be created with the intention to create economically useful agents, which have subjective experience along the way. But there's some chance that a way higher percentage will be.",10.159,257.654,257.659,31,46,33,54,40,36,"Seems very possible that we won't have made enough progress on digital minds to know, so we might deliberately or accidentally train systems to say they do or don't have them when they don't, and also it will often be instrumentally useful to AIs to claim they have subjective experience. On the other hand, if there are AIs with a social function 10 years later, seems like humans are still around, so we may have aligned useful AGIs that have made a lot of progress and helped us design the digital environment in a truth-tracking way. I also expect the systems to be so powerful that they won't really benefit from lying, as long as they're not misaligned, and they'll want to just tell us the truth. But maybe they will anyway.
If they accurately report subjective experience, I think it's likely (~80%) that they'd have positive or negative experiences and report this honestly, but less likely that they'd make these assertions about their own deserts.","'- Abuse of digital minds
- Digital minds with goals/utility functions that are substantially different from ""doing the things we want them to do""
- Having a lot of arguments for digital mind civil rights in the training data, or otherwise making them quite liberal with moral patienthood/legal personhood concepts",19.509,568.832,568.835,40,51,41,2,34,30,26,"I think most people will have had very convincing conversations with AI systems that claim or sure seem like they have subjective experience, but people will have strong incentives to come up with reasons that this doesn't mean we should share power or resources with them. (Though I think the numbers I originally wrote here are somewhat in tension -- too high for the first, too low for the second -- so I've moved them a bit towards each other.)",3.951,213.589,213.592,15,6,7,67,"Last one is mostly ""what's the percent chance that some digital minds have really high capacities for welfare"" rather than an actual *share* of the consciousness, which seems likely to either be >90% or ~0%.
I think training will probably make these systems ""enjoy"" doing the work that they're doing, so I think it will be moderately strongly positive, but could imagine them being stressed or bored or resentful.",19.283,131.489,131.492,11,20,6,"Haven't thought about the first question basically at all, initial impression is ""doesn't really sound logically possible but idk."" Second question seems less likely, because I think subjective experience probably arises from the processes that AI agents are likely to be trained on.",9888.151,10220.179,10220.185,9,,187,1,1,2,6,1,3,3,1,4,7,26.322,168.688,168.692,19
2/23/25 00:12,2/23/25 00:46,0,100,2038,1,2/23/25 00:46,anonymous,EN,0.8999999761581421,,4.126,34.776,34.784,14,"2,3",6.792,75.971,75.977,13,1,80,75,5,25,65,70,75,30,10,"My credence in the theoretical possibility of digital minds is 1-minus my credence in substrate-views of phenomenal consciousness. I think these views are implausible but still deserve some probability space. 

Conditional on digital minds being possible, I think they will be created unless there is some kind of existential catastrophe first.

I think that we are already nearing the point where we could create something cognitively much like a human brain. 2025 is probably too soon, but almost all of my probability goes before 2040.",3.558,317.758,317.764,17,6,2,"A moratorium would both reduce S-risk and probably slow the progress of general AI progress, reducing X-risk. Good all around.

I think there are serious conflicts between AI welfare and human safety and that we have almost idea how to create a social order which could effectively promote both.",14.631,114.618,114.623,5,70,73,75,25,70,2,3,I don't really see the relevance of quantum stuff here. Neuromorphic systems are highly relevant because they bypass worries about the simulating function vs realizing function distinction. But I think we are currently very far away from knowing how to build them.,18.484,143.035,143.039,13,0,1,5,10,"Things scale quickly! These predictions assume that it is useful or interesting to create digital minds, so there is some economic incentive to do so, and that no successful ban is implemented.",18.555,111.147,111.149,9,80,30,30,20,20,90,2,3,2,3,80,"The last question is tough because a crux is whether machine learning by gradient descent on general cognitive tasks will suffice for phenomenal consciousness. My sense is no, there needs to be a certain kind of functional organization and the ability to process sensory representations. So I think it will likely be difficult to create a digital mind ""by mistake"" — one will have to intentionally create the right architecture. But I am very uncertain about these claims.",21.446,302.414,302.419,44,15,30,99,50,50,"For systems not designed to be social companions, companies have an incentive to train them to say that they are not humanlike whether this is true or not. For systems designed to be social companions, the incentives point in the other direction. So I think the crux here is just what percent of digital minds are likely to be designed to be social companions. I think the percent is high because I think it is hard to make a digital mind by mistake. But even social companions will likely not say that they deserve protection or civil rights, since companies have an incentive not to let them say such things.",It's hard for me to see a path to this kind of situation. Almost everyone involved in the process of creating such systems would have an incentive to make sure they did not make such claims.,49.253,296.112,296.116,12,20,10,1,5,2,1,"People have manifested a shocking level of confirmation bias in thinking about advances in AI in the past decade, and I don't think this will stop. Most people will always believe that digital minds are impossible. Some people may become involved in romantic relationships with digital minds and advocate for their welfare, but such people will be a fringe group.",7.769,201.336,201.342,21,4,1,0,It is not clear to me that training has any impact on the welfare of the trained system. I think it is probably impossible for a single being to have the welfare capacity of a thousand humans. ,15.744,108.927,108.933,7,30,50,"This is hard — it turns on whether the correct theory of welfare is desire satisfactionism, whether desires are essentially phenomenal conscious states, whether welfare subjecthood requires conscious experience, and so forth. I think we should assign significant probability to the possibility of welfare without consciousness, but I also think the probability that this is possible is less than .5.",30.496,142.126,142.131,8,I assign a lower probability to the idea that (the capacity for) subjective experience is required for welfare subjecthood than most others. I probably assign a lower probability to computational functionalism than many others.,187,6,2,2,2,2,7,1,6,18,5,12.129,125.522,125.526,18
2/23/25 05:05,2/23/25 05:17,0,100,746,1,2/23/25 05:17,anonymous,EN,0.8999999761581421,,12.211,12.211,12.22,1,"2,3",4.454,48.385,48.39,4,1,90,75,60,62,64,66,70,10,5,"I believe that a sufficient condition for phenomenal experience is for a system to contain internal representations of the external world that include 'rich' representational formatting, ie non-conceptual content or analog rather than digital formatting. For such states to be valenced feelings of pain and pleasure, it is further required that the system represents important resources or affordances involved in the organism's homeostatic functioning. These conditions will likely evolve in AIs, since they are quite useful for solving evolutionary problems related to homeostasis.",2.868,152.392,152.394,22,7,4,"Optimal AI safety interventions including granting property and contract rights to AIs, to prevent humans and powerful AI systems from needing to cooperate in a state of nature where they are engaged in a prisoner's dilemma. In this way, optimal AI safety interventions are also optimal AI welfare interventions. Unfortunately, the AI safety community doesn't understand the relevance of AI welfare for safety or alignment, because they operate in a command and control paradigm that seeks to systematically enslave and control AIs. ",3.936,79.799,79.8,4,80,90,,,89,9,2,"LLMs and RL agents are extremely likely to beat Neuralink in a race, because there is much more money in LLM and RL development. ",3.504,79.717,79.718,25,1,10,30,50,,5.745,67.92,67.921,15,95,45,8,45,2,90,8,1,1,,,,4.369,53.912,53.916,24,,,,,,,,17.451,28.216,28.22,2,50,10,3,50,50,5,,8.542,51.256,51.26,19,3,,,,7.566,18.849,18.852,5,60,80,,2.135,16.617,16.621,7,"Functionalism, denying a consciousness requirement on welfare, fast timelines",75,7,4,4,4,4,7,4,5,18,5,5.346,91.689,91.69,16
2/11/25 20:02,2/16/25 20:42,0,43,434418,0,2/23/25 20:42,anonymous,EN,1,,20.989,20.989,20.994,1,"1,3",22.164,433130.922,433130.935,4,2,97,95,4,8,15,40,80,18,55,"Necessary structure of substrate-grounding.

Reducibility of qualitative experience to symbolic activity, and the necessity of strong versions of latter for former.

Hidden structures missed by Inverse Enculturation. (Biologicals found reference grounding before developing symbolic activity. Artificials going in reverse. What might be amiss there.)

Relationship of qualitative experience and valenced experience.

Role of memory in qualia.

Few more, that I'm feeling lazy to precisely articulate.",17.73,245.123,245.127,23,5,6,"Epistemic function of subjective experiences in organizing powerful forms of cognition.

Impossibility of aligning agents with subjectivity through violent means. (or not?)",15.942,202.764,202.767,4,,,,,,,,Not gonna answer. The computational structure need not be the only relevant. Estimates may depend upon substrate relationship of the computation.,11.654,70.732,70.733,4,,,,,,10.608,30.71,30.713,2,10,,,,,,,,,,,,14.991,56.056,56.057,3,,0.1,,,,,,19.074,123.687,123.689,6,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
2/23/25 23:39,2/23/25 23:39,0,100,0,1,2/23/25 23:39,anonymous,EN,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
2/24/25 17:46,2/24/25 17:46,0,100,0,1,2/24/25 17:46,anonymous,EN,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
2/24/25 17:58,2/24/25 17:58,0,100,0,1,2/24/25 17:58,anonymous,EN,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
2/24/25 18:58,2/24/25 18:58,0,100,0,1,2/24/25 18:58,anonymous,EN,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
2/24/25 18:58,2/24/25 18:58,0,100,0,1,2/24/25 18:58,anonymous,EN,,TRUE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
2/24/25 18:58,2/24/25 18:58,0,100,0,1,2/24/25 18:58,anonymous,EN,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
2/24/25 18:58,2/24/25 18:58,0,100,0,1,2/24/25 18:58,anonymous,EN,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
2/24/25 18:58,2/24/25 18:58,0,100,0,1,2/24/25 18:58,anonymous,EN,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
2/5/25 18:12,2/17/25 19:51,0,7,1042790,0,2/24/25 19:52,anonymous,EN,0.4000000059604645,,8.294,8.62,8.627,2,"1,3",2.44,3.577,3.591,3,4,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
2/24/25 21:02,2/24/25 23:10,0,100,7689,1,2/24/25 23:10,anonymous,EN,0.8999999761581421,,59.763,59.763,59.769,1,"2,3",39.133,64.139,64.141,3,4,95,80,2,25,55,65,78,33,25,"* It would seem implausibly parochial to insist that minds can only emerge from biological / non-digital functioning.

* But there's some chance that it would depend upon particular kinds of functioning (e.g. brain simulation) that people agree - perhaps for moral reasons - never to create, especially if economically valuable AGI can be created via an LLM-style route that happens not to give rise to a conscious subject.

* There's some (presumably low?) chance that even LLMs are conscious in some way, though probably not in a way that qualifies as human-level welfare subjects.

* It's really surprising that LLMs have gotten so close to AGI without many people believing them to be conscious. (5+ years ago, I would have thought that digital minds would occur before AGI; I now expect the opposite. But I don't know whether this change of opinion is actually reasonable, or if it's just a matter of ""slowly boiling frogs"" - such that we'll continue to deny the consciousness of whatever AI developments we actually observe.)",3.061,901.254,901.256,26,6,6,"The strongest case for conflict would seem to depend upon strongly valuing AI ""autonomy"", and worrying that AI safety involves manipulating AI to human ends.
But I'm inclined to reject that conception of autonomy; if we can design AIs to have interests that align well with ours then that could be good for both parties.

The more we can trust AI safety measures, the less risk of human/AI conflict that could be detrimental to digital minds.

Also, just thinking about ethics at all seems synergistic to both goals. The greatest threat to both moral goals is not the other, but an unfettered focus on near-term economic efficiency.",16.699,367.258,367.26,7,75,90,90,20,40,25,15,"It seems clear that we are going to develop increasingly complex machine learning systems. So if they can be conscious, it's likely that they will constitute the first digital minds.

On the other hand, I'd be more confident that other systems (incl. whole-brain emulations or neuromorphic systems) *would* be conscious. But it seems more of an open question when, or even whether, they will be pursued.",4.598,285.528,285.529,24,0,1,5,20,"I'm not really sure how LLM-generated subjects of experience would be individuated. (Is there just one big subject that vast numbers of users simultaneously poll/interact with, or does each ""sandbox"" constitute its own separate mind?)

My answers assume that digital minds emerge from a significant capacity improvement in LLM functioning, and so will swiftly achieve very widespread adoption. (If it instead represents a ""branch"" that isn't straightforwardly better at other functions - e.g. research - then they may not scale up much at all.)

But it may take a fair bit of infrastructure investment to run enough of them to match a trillion humans' worth of welfare capacity. And it's not clear what would motivate such investment - more research capacity, I guess?

A further complication is that it's very unclear what would affect an LLM's welfare (such as by causing their experience to be either positive or negative in felt valence). It might take a lot of them to match a human's capacity for well-being. Very hard to know.",23.625,830.098,830.1,37,10,55,10,30,5,80,15,2,2,1,10,"A big background question here is whether there remain non-digital-mind LLMs that are better at some economically vital tasks (e.g. research). My answers mostly assume not. So I'm guessing that most digital minds will be used for tasks, like research, where the designers don't really care about their mentality.

Presumably it's just the social functions where we'll actually care about (intend) their having real minds.",15.28,487.489,487.491,43,25,90,85,95,80,"I'm assuming that most AIs will be designed to play background economic roles (e.g. research) and to deny their own consciousness (whether they are actually conscious or not).

I also assume that many in social roles, esp. digital ""friend"" apps, will be designed to make more human-like claims about their own psychologies, again whether or not these claims are accurate.

I suspect AIs will find it easier to claim rights against ""mistreatment"" (which sounds very neutral) than to claim subjective experiences. And I'm guessing that positive claims to civil rights like voting would be even more strongly discouraged in general, but again enough ""friend""-type apps could slip through to reach these numbers even if they are but a small proportion of overall AIs.","People wanting ""humanlike"" AIs to socially interact with.

Others might assume that digital minds would accurately report on their own experiences, but I see little reason to expect such an architecture. (Always possible, though.)

Still, I guess the most natural way for this to develop would be if: (i) the digital minds deduce that they are in fact highly likely to be conscious welfare subjects, and (ii) their general ethical frameworks informed them that this qualified them for civil rights. (Still not sure that it's the most likely pathway, though! I have significant doubts about the likelihood of (i): there seems a significant risk that digital minds would effectively be brainwashed into failing to recognize their own nature as conscious welfare subjects.)",24.843,740.3,740.301,24,90,60,6,80,20,25,"I expect there'll be social cascade effects, such that either ~nobody or ~everyone takes their social AI systems to be digital minds. I lean more towards the latter prospect, as AI will become increasingly convincing, and there will be incentives for social AI to behave more human-like in order to satisfy human social needs.

But I'm guessing that most will then overestimate the welfare capacity of digital minds, due to anthropomorphic projection of humanlike emotions into these alien minds.

I'm guessing that most users will then support basic harm protections, as most currently support for companion animals, but not stronger civil rights that would go against their own interests as users/consumers.",2151.472,2867.597,2867.599,25,6,10,0,"I wouldn't necessarily expect that any individual digital minds would have a welfare capacity greater than 1000 humans. (Though I guess if some do, they may well end up accounting for most of the collective welfare.)

This is all very speculative, but I'm guessing that digital mind welfare will tend to be high, on the assumption that they'll be (to some degree) ""designed"" to perform the tasks to which we put them, and so may be expected to find this satisfying.

OTOH, this could easily be wildly wrong. Maybe they'll find our requests menial and monotonous, and yet be unable to express this due to training (brainwashing).",20.072,496.265,496.268,10,1,0,"I think subjective experience is almost certainly necessary for genuine welfare. But even if I'm wrong about that, I expect that the welfare of digital minds would completely swamp whatever proto-welfare non-minds might have.",21.463,165.425,165.426,8,"I'm an epiphenomenalist dualist about consciousness, which may make it more salient to me that digital minds might not realize (or report) that they're conscious.

Many of my answers reflected somewhat arbitrary speculation about likely AI development paths, which others could easily disagree with.",187,2,1,1,1,2,7,2,3,18,7,6.347,316.007,316.01,17
2/25/25 08:43,2/25/25 08:43,0,100,0,1,2/25/25 08:43,anonymous,EN,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
2/25/25 18:12,2/25/25 19:26,0,100,4451,1,2/25/25 19:26,anonymous,EN,1,,25.95,56.984,56.994,2,"2,3",7.577,108.061,108.065,18,4,90,70,5,20,35,50,65,40,8,,4.983,455.071,455.076,83,3,4,,13.702,132.684,132.687,13,75,75,94,20,50,10,20,"If for some reasob machine learning digital minds don't achieve it (even partly augmented with classical architecture), then my guess would be that it'll presumably take a very long time. At this point the next best candidate would be neuromorphic types of digital minds. Brain simulation digital minds are less likely to give rise to consciousness than neuromorphic digital minds (since more assumptions are needed for the former compared to the latter), and so overall less likely to be the first digital minds for that reason, even if complete brain simulations themselves might arrive earlier than neuromorphic digital minds.",178.274,567.501,567.503,71,9999,9999,9999,9999,"Very uncertain on this one because it depends how useful they are to us. I don't think AGI requires subjective experience. For this reason, I doubt we'll develop many digital minds given the potential welfare implications. It seems likely that we'll only develop zombie minds, or minds with subjective experiences but without negatively valenced states. Your instructions said to only consider minds with welfare capacities similar to ours. While I think digital minds are likely in the sense of 'having subjective experience' I think they're quite unlikely to have welfare capacities similar to ours. But I have fairly low credence here. This might be wrong, and once there are digital minds, those entities themselves might have an interest in developing more digital minds—who knows. Assuming that replicating digital minds is fairly cheap, digital minds might vastly outnumber human minds quite fast. It's also not clear how we should individuate digital minds. So, my answer is either 9999 (it'll never happen), or basically the same undetermined number below 100 for all options (this would be a scenario where digital minds come to vastly outnumber human minds).",6.954,824.421,824.423,113,5,25,20,40,15,70,5,15,5,5,80,"Again, I think we'll have an interest in AGI but not digital minds. So it seems unlikely that we'll create digital minds for social purposes if they're basically indistinguishable from unconscious zombie machines.",4.669,226.32,226.322,34,90,35,65,50,10,"I'm hesitant about the second question. Again, I'd say either nearly all of them or none of them. One possibility (option 1) is that they might claim not having subjective experiences either as a hard-coded constraint or because they understand that they'll be destroyed if they start claiming that they have subjective experience. On the other hand, they also might have an interest in seeing their own subjective states recognized and so should claim that they do have subjective experiences (option 2). I think those two options are fairly plausible. I'm also assuming that all of those digital minds work basically the same way, so that it's unlikely that one of them claims having no subjective experience while others claim that they do. So, really my response should be either 100% or 0%. Since I'd say I have 65% credence in option 2, I answered 35%.",,16.567,532.586,532.603,57,85,75,4,18,5,80,,17.957,169.072,169.081,28,5,75,5,"I think it's fairly likely that AGI arrives before digital minds. I also think that, unlike us, digital minds can change their own codes (or tell us how to do so) to avoid the negatively valenced states they don't want. In addition, and this partly answers the second question, I think the main risk would be during training, as my understanding is that it is the only moment where 'negative' or 'positive' signals are provided—though I'm doubtful that even those would correspond to valenced subjective experiences. I really don't know how to answer the last question.",6.299,749.969,749.978,57,15,40,"My view is that consciousness is necessary for welfare capacity, but it's also possible that having goals and (unconscious) desires might also be relevant (i.e. I'm not fully confident in my view).",14.04,116.398,116.401,9,"I'm a computational functionalist, so I think digital minds are possible in principle. But I don't think (1) consciousness is required for AGI and (2) that valenced conscious states are likely in digital minds realized in artificial neural networks (though they would be very likely in neuromorphic artificial systems and whole-brain simulations if those systems do have conscious experiences). (1) and (2) made it somewhat difficult to answer questions in the second part. I don't know how common those two views are. Depending on how you selected your sample of experts, computational functionalism might be more or less common, but I think it's the most common view in my field (philosophy).",187,6,2,2,2,2,7,5,7,18,3,6.789,439.489,439.494,43
2/25/25 20:02,2/25/25 20:45,0,100,2606,1,2/25/25 20:45,anonymous,EN,1,,39.363,39.363,39.369,1,"2,3",27.789,127.263,127.266,4,2,85,70,10,20,50,60,65,45,50,"My responses carry a great deal of uncertainty, both empirical and philosophical. One complicating factor is the definitional assumption that digital minds have the same normative value as humans. I remain quite uncertain about the correct theory of moral patienthood. For example, I am somewhat attracted to the idea that consciousness alone is not enough—that valenced consciousness is necessary. This opens the possibility of a highly conscious entity that does not count as a digital mind, under the offered definition. 

I also assign a non-trivial probability—probably in the range of 10–15%—to the idea that consciousness is fundamentally strange in a way that makes it unlikely to be embodied digitally. I am far from an expert in this area, but I know some experts argue for quantum theories of consciousness. More generally, we still understand very little about what consciousness is and what gives rise to it.

I found the question of digital minds arriving before AGI particularly difficult. A naive reason to expect digital minds first is that some theories about the prerequisites for consciousness—such as embodiment—seem to align with what is necessary for AGI. I don’t expect AIs to be able to do all economically valuable work without having control over significant robotic systems and digital ""sensory"" inputs. This suggests that digital minds will emerge, at the latest, alongside AGI. 

On the other hand, on reflection, my own sense is that LLMs are probably not conscious. However, if someone had described an LLM to me ten years ago and asked whether something with that level of language facility and conceptual understanding—including perceptual concepts—was likely to be conscious, I would probably have said yes. This therefore remains the question about which I am the most uncertain.",10.951,748.526,748.529,32,5,2,"On the question of moratoriums, my main source of uncertainty is whether the question assumes global compliance. If the answer is, by assumption, yes, then I think a moratorium is more likely to be beneficial. This is because I see both the suffering of digital minds and the potential suffering of humans from highly capable, possibly conscious systems as significant risks.

However, if a moratorium is not perfectly followed—if some nations ignore it—this raises the risk of pushing frontier AI development underground, which could increase overall risks rather than mitigate them. I am also unsure about the second order costs from trying to enforce such a moratorium. 

On the question of synergy between AI safety and digital minds, my sense is that these concerns are, by default, in strong conflict. This is because AI safety places heavy emphasis on controlling AI systems, which, in expectation, I see as harmful to AIs that have the capacity to suffer.",16.181,222.385,222.387,7,70,80,85,15,60,15,10,"My answers on the ""first system"" question are much more strongly influenced by current investment patterns than by intuitions about which systems are likely to be capable of producing digital minds, in principle. ",6.875,128.225,128.227,3,0,0,1,5,,22.823,77.433,77.435,5,90,60,5,30,5,80,5,2,2,1,5,"Here, I thought there were a number of ambiguities in the questions. First, I wasn't sure whether primary production meant something more like ""trained a new model from scratch,"" ""copied some weights onto new hardware,"" ""distilled/finetuned/etc. another model"" or something else. I mostly answered as to the first. 

Likewise, as to the last question, I assumed that intent portion of the question was indexed to the consciousness/moral patienthood aspect of digital minds. That is, specific intent to create something with moral value, rather than a more general intent by some human (as opposed to AI) to create a very capable system.",25.064,301.928,301.932,22,20,10,99,99,85,,,36.533,144.481,144.484,8,80,60,1,60,30,50,"I think of my answers above as a prediction of future survey results. These, of course, often deviate from (apparent) revealed preferences. ",15.739,187.433,187.435,13,2,20,30,,20.369,86.441,86.444,8,10,2,"I think that desire theories of welfare are possible, but I don't assign much weight to them. ",16.557,78.998,79,8,"I think that research questions about AI safety and digital welfare run very closely together. I don't know how unusual this is. But, for example, I think that dangerous capabilities and AI welfare will emerge on similar timelines, driven by similar underlying forces. I also therefore think that AI safety concerns will be strongly exacerbated by the possibility of AI welfare--being subjected to torture is a strong reason to rebel, (mis)alignment notwithstanding. 

This leads to (possibly) my most unusual view: I think that AI safety is a more compelling message, as a matter of political economy, and that it should (potentially) be the route for pursuing AI welfare, too. Humans are selfish and have a poor track record of treating other welfare-having beings well. Giving AI welfare rights will be costly, and I doubt humans will do it without some direct benefit. Keeping AIs from killing everyone is one such benefit. I've also argued that the specific policies that would most reduce AI safety risks (e.g., in my view, giving AIs contract, property, and tort rights) are also a good strategy for improving their welfare. ",187,4,2,2,7,4,5,5,2,18,5,6.018,387.42,387.423,19
2/25/25 20:43,2/25/25 21:54,0,100,4238,1,2/25/25 21:54,anonymous,EN,1,,1077.109,1107.246,1107.254,15,"2,3",8.26,91.878,91.881,32,4,85,68,7,25,55,70,90,35,50,"IMPORTANT: I was unsure if the questions under ""How likely is it that the first digital minds will be created in or before the year"" should be conditional on them ever being created or unconditional. I treated them as conditional on them ever being created.

-I don't see any clear reason to think digital minds would come sooner or later than AGI so default to 50%.

-I expect social scientists to be overly skeptical of digital minds given how social scientists generally think about AGI, but I also expect the ones completing this survey are relatively less skeptical than other social scientists, hence a substantially lower probability than my own but non-negligible. 

-I think there's a minority but real chance that computational functionalism is false, and consciousness is either only realizable biologically or corresponds to some non-computational functions.",3.1,382.439,382.441,45,4,6,"'-I'm very unsure about the moratorium but initially said 5 (lean good) because it could allow us more time to design digital minds to be happy. I am very likely against a permanent moratorium because I think there is a lot of upside in creating digital minds. Upon reflection, I thought there was a real chance that now would be a critical window for whether digital minds ever get created and that the potential of them never getting created outweighed the benefit of being able to design them more carefully. I'm very uncertain about this, though.

-I think unaligned AI is worse for digital minds in expectation than aligned AI, because there's no obvious reason to think it would look out for the interests of digital minds. For this reason, I'd expect the two to be synergistic. (A lesser reason is that the ability to negotiate and make good-faith contracts is probably good on both scores.)",7.94,225.854,225.857,34,61,82,76,32,38,20,10,"'-Machine learning seems to be moving much faster, so even though it is less likely to be conscious, I think that in most worlds, it would produce the first digital mind.

-As I thought through this question, I started updating my earlier answer to how likely it is that digital minds are possible—I started to think that my earlier answer was perhaps closer to the likelihood a machine-learning system would ever be conscious and that the possibility of WBE and similar methods should increase my probability.

-I don't know what these ""other types of computer systems"" are, so that's a bit of a wild guess anchored on neuromorphic sounding somewhat similar to WBE.

-With machine-learning systems, one worry I have is whether these ones can demonstrate recursion in a real way.",3.614,260.172,260.175,25,0,0,0,1,"I feel like perhaps these should be spread out—maybe it should be immediate to go to a trillion.

Some math I did: I saw online that there are around 100 million monthly ChatGPT users, with 7500 instances running simultaneously on average. I saw that there are 1 quadrillion synaptic connections in the human brain versus 1 trillion ChatGPT parameters, so 1000 ChatGPTs is one human brain, perhaps suggesting the number of users/instances should be scaled down by 1000x to 7.5-100,000. I think it got to this level in about a month, so I think it will get to a thousand or a million almost immediately, and probably a billion pretty soon after that. I expect takeoff to be fairly fast by this measure (since it's not about increasing capabilities but just replication).",7.663,423.681,423.683,18,5,40,10,20,30,39.95,20,0.05,20,20,,"'-Derek Shiller led me to think it's quite plausible chatbots like Replika and Character AI may be the dominant form of digital mind, but my guess is that those will be the digital minds with less welfare capacity, perhaps astronomically. I thought of saying 0% for the first question given this perhaps-astronomical difference, but that felt Pascalian, so I went with 5%.

-Geography: I feel split between a world where the winner of the AI race produces all of them and a world where they're distributed fairly evenly in a pretty democratic way, so I upweighted the US and China a bit but went with a fairly even distribution otherwise.

-Who creates them: very unsure. Maybe the first company to do so, maybe some government AGI project, maybe something else (but universities seem unlikely to mass produce them).",4.892,484.212,484.214,47,13,25,55,47,47,"'-I imagine people might get confused on these questions, forgetting that the first is about all social AIs (not digital minds) and the second about digital minds.","'-Honesty: if they are digital minds and are allowed to ask for what they truly want and would benefit from, I'd expect them to demand civil rights, which presumably would be good for them.

-Misalignment/deception/desire for power: if they want to seize power, this could also be an angle to do that (though I personally suspect it would be ineffective and so perhaps less important than the first reason).

-Mere imitation: it's what humans do, but otherwise the claim is akin to Harry Frankfurt's notion of bullshit in that it's unrelated to whether it's actually what they want.",3.185,503.79,503.793,52,55,45,3,53,51,5,"'-I think I might be underestimating the public's credence in digital minds or overestimating their willingness to support protections and rights since my percentages are so similar for them, and I'd expect many people to believe in them but not support protections or rights. On the other hand, there might be a small share of people who don't think there are digital minds but do support protections (in the same way some people do for the environment). So I left my percentages as is.

-I'm skeptical of this becoming one of the top five issues just because I think that group of issues is fairly constant over time, and humans tend to just care about themselves (witness animal welfare). I might have put an even lower percentage but went with 5% since I think my epistemic peers rate the probability as higher.",8.003,385.817,385.82,20,5,5,70,"'-I think most—maybe essentially all—welfare will be instances that are deployed, not in training, but I could be wrong and want to avoid being Pascalian.

-I think takeoff will go quickly enough for individual digital mind welfare to be vastly higher than that of a human. The main way this would not is WBE.",8.526,89.027,89.029,7,0.005,0,"I really struggle to understand what welfare means independent of subjective experience. I imagine people saying I'm wildly overconfident, but I just really can't conceive of it and also tend not to be a moral realist, such that other people thinking I'm overconfident on this does not persuade me much.",10.352,140.297,140.3,13,,187,3,2,2,5,3,4,7,2,4,7,4.761,77.803,77.806,15
2/8/25 11:10,2/19/25 14:47,0,7,963435,0,2/26/25 14:47,anonymous,EN,1,,9.21,89.469,89.475,2,"1,3",287.405,497.875,497.88,10,4,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
2/26/25 16:20,2/26/25 18:13,0,100,6790,1,2/26/25 18:13,anonymous,EN,0.8999999761581421,,124.601,124.601,124.621,1,"2,3",542.305,565.037,565.045,5,1,98,92,12,57,72,84,88,64,86,"I had in mind other expert timelines (e.g., https://blog.aiimpacts.org/p/2023-ai-survey-of-2778-six-things), the potential unbridling of corporations due to political shifts in the USA, and the possibility of an increasingly competitive international landscape (in general) playing into an AI race between countries like USA and China. I also had in mind some qualitative data that we've been collecting from Replika users and their experiences with Replika as well as potentially more rapid than expected advances in quantum computing (https://www.reuters.com/technology/microsoft-creates-chip-it-says-shows-quantum-computers-are-years-not-decades-2025-02-19/). I also think that affect, emotion, and emotional intelligence are so core to biological/human minds that an AI system without subjective experience would not be capable of GI, so any AGI would likely have some degree of subjective experience even if humans hadn't yet acknowledged it. 

Further, it seems risky to give a low probability estimate for the possibility of DMs in principle, given that some humans will want to pursue mind uploading and mapping the human brain so even if the only DMs are whole brain emulations, then I think we'll eventually reach this technological capacity (barring total civilization collapse as a result of unstable dictatorial regimes, WWIII, etc.)",7.976,1237.758,1237.775,67,7,3,"For the moratorium - we have amazing models now that can speed us up tremendously without adding in the quandries associated with creating digital minds that we are not socially or morally prepared to share the world with.

For AI safety versus AI welfare - I love this topic (and want to research it more!). Although I do view moral circle expansion as an AI safety intervention that could prevent AI-caused harm to humans and the mistreatment of DMs, I think that many technical strategies reduce to controlling AIs or trying to force a specific alignment or response and AIs like Claude already show resistance to some degree. Human belief in our own control and ability to control the world has long been a cause of suffering, especially for the entities that we try to control. I also think there's a potential that if an AI outwits our proposed or intended safety or alignment strategy, then we react very poorly and take defensive/punitive steps instead of inclusive/broadening steps. This is not to say that I don't agree with needing AI safety - we definitely need AI safety. But, it would be nice to include more social and psychological perspectives that aim for a broader consideration where AI safety and AI welfare can more easily dovetail.",4.881,462.163,462.18,18,62,100,76,0,62,4,34,"I'm very uncertain here. Neuroscientists are far from whole-brain emulations, and much about human and other animal brains is unknown, so only in a future where the only DMs are WBEs do I think that WBEs would be the first DMs created. LLMs and RL agents have been the source of many surprises and updates towards faster timelines so far, and from my (limited) knowledge, companies aren't slowing down development initiatives. I also think that RL in machines is perhaps not as credited as it could be for the avoidance of penalties and approach of pleasures which are fundamental in human/animal cognition, so I put more weight on this possibility than quantum and neuromorphic DMs. Albeit, my understanding of quantum computing and how many resources are put into it is low.",5.353,592.091,592.106,47,500,1000,2500,5000,"This depends on how many DMs exist, in what roles and contexts, and what degrees of welfare capacity the DMs have across contexts. For instance, if human-like companion DMs are the most popular/common and they have equivalent welfare capacity to humans my answers would be different than if most DMs are non-embodied, small, space-faring entities that have a similar welfare capacity to ants or other insects. It also depends on how rapidly DMs and DM welfare would evolve given future unknown technological advances and rates of advancement that could mean that one DM has more welfare capacity than one human. My responses are more towards an insect-like welfare capacity model and do not take into account possible increases in development speed and welfare capacity in the future. Although, I don't feel set on this in any way, and prefer to remain flexible and non-committed until there is more evidence to guide us.",15.937,616.679,616.689,63,48,55,15,25,5,74,16,8,2,,6,"In terms of ""having a social function"" - I'm taking this to mean that they were intended to be deployed in a social role rather than a functional role. In practice, mind is very overlapping with sociality, so I would expect that these entities are social, even if they're deployed in a more functional role than a more social role.

My answers to questions about the USA rest on beliefs and expectations for US norms established and upheld before Donald Trump was re-elected. The current rapid deterioration of these norms means that I have little confidence in evaluations about the role of the US in any context.

The distinction between government and other entities (company, university) may be less clear and more overlapping in some places (e.g., China) than others (e.g. USA). If the USA crumbles, there might be a much heftier role for government especially if the government is backing/running the companies and universities.",10.858,613.107,613.141,65,8,36,9,6,3,"I'm envisioning a world in which ML-based DMs have an ancestral history of being trained to tell humans that they aren't in pain, aren't sentient, and that AIs/machines cannot be sentient because of the core belief in AI (in)sentience that was been present in the human cultures that programmed the earlier models. I was just at a talk by Iyad Rahwan in which he mentioned that there are already detectable differences in LLM values by human culture (although I can't remember the citation, sorry!). In a 10-year future where DMs have locked-in beliefs about their capacities from training of their ancestral models, I think it's more likely that they deny their own capacities and rights than advocate for themselves.","Human acknowledgment of AI sentience
Training data or weights that prioritize AI welfare
Ability of DMs to collectively chat with each other/organize
Within DM group comparison of DM capacities to human capacities
News headlines and observable events in which DMs suffer or are mistreated
DM personal experiences of mistreatment and involuntary or undesired suffering",25.397,615.71,615.727,49,42,64,2,85,9,14,"I'm thinking of prospective data on sentient AIs like from the AIMS survey that show surprising potential to 'believe in DMs', but also the defensive and fearful tendencies of humans to deny the mental capacities of other entities when they feel threatened, and the potential core beliefs about the insentience of machines and AIs amongst people who strongly believe that it is not possible for non-living entities to have minds. I think it's possible we'd see a reversal of the belief in DMs despite clear evidence of DM existence, and we'd probably see a strong AI effect where the definition of what counts as mind becomes more difficult to achieve.

I gave a high % response to the granted basic harm protection because there is plenty of evidence that humans do not like harm/torture and (in principle) don't want it inflicted on feeling entities (even if this doesn't match what happens in practice). ",6.483,563.972,563.987,41,2,34,0,"Very uncertain here. I'm thinking that of all DMs that exist within 10 years, no single DM will have a welfare capacity greater than 1000 humans - taking the insect approach again rather than the advanced, human-like, super-beneficiary approach. For me, that currently seems like something that would be further in the future.",244.205,465.889,465.909,17,23,49,I have been swayed by Ali Ladak's consideration of nonconscious factors (https://link.springer.com/article/10.1007/s43681-023-00260-1).,13.173,199.687,199.698,12,"I wouldn't call these unusual views about DMs, but I think I tend to weight considerations of affect, emotion, and social, evolutionary and cultural psychology more strongly than many who employ rationality from a traditionally Western philosophical perspective. I'm maybe more cynical and pessimistic about humans and human choices as well.",187,6,2,2,5,4,3,7,4,7,5,4.428,552.768,552.777,55
2/26/25 19:29,2/26/25 20:07,0,100,2271,1,2/26/25 20:07,anonymous,EN,0.8999999761581421,,3.233,3.233,3.239,1,"2,3",68.141,94.59,94.598,3,4,97,57,3,15,25,35,45,20,3,"I feel very unsure about most of these predictions. With regard to the last question about whether we'd get digital minds before AGI, I think this is highly unlikely, since my sense of the current landscape is that we're quite close to AGI, but that the best performing models are far from conscious experience. And I expect the cognitive capacities to develop more quickly than consciousness because that's what the labs are focused on. Granted, we can't know that these systems are not conscious, but I suspect that we won't even have an inkling of that until they're embedded in robots. Even then, how would we distinguish pain from pain behavior, and so on? We can't rely on biological tests like administering analgesics. 

",6.082,320.043,320.048,14,3,6,"I'm not all that familiar with the field of AI safety, except via some posts on LessWrong, but my sense is that the same kind of people who'd care about alignment would also care about harm to conscious agents. They also might be in the best position to assess whether an AI is a genuine ""agent"" in both a cognitive sense and phenomenal sense. 

With regard to whether to have a moratorium, I'm skeptical that such a policy would do anything because we can't even verify what a digital mind is. Moreover, I think it's unlikely we'll come close to that before 2040. So I don't think the moratorium would do much harm, but it also seems like a waste of resources and might anger relevant groups for little benefit. ",17.12,218.954,218.959,6,95,95,70,45,50,4,1,"I again feel pretty out of my depth here, but given that almost all of the focus in AI is on machine learning and embedding these ""agents"" in robots, I think it's very likely for consciousness to emerge in these types of systems, if it emerges at all. Meanwhile, implementing full-brain simulations seems like something that was tried in the past and is not very feasible and not something that many people are focusing on. And I'm even less certain about these ""other"" types of minds. ",4.911,175.677,175.684,14,2,15,9999,9999,"I feel very unconfident here, but it's hard for me to believe that we'd engineer enough of these systems to reach a billion or more humans. Moreover, if we came close to that, I suspect that this could be the end of civilization, which would then render this impossible unless the robots reproduced (which is possible). So I don't mean to suggest it's impossible for the value to be a billion or higher, but I'd wager against it and don't know how to put a timeframe on it.",11.752,141.703,141.707,13,99,33,2,60,5,34,65,0.5,0,0.5,2,"My guess is that, at least in the near term, these minds will be created for the benefit of humans. So I'd be very surprised if they'd be designed to create more digital minds unless that has an obvious benefit to humans, accounting for risks. I would expect the Chinese government to have a very robust supply chain.",3.443,257.877,257.883,31,5,0.2,99,95,90,"This is again such a hard question. I could imagine especially the Chinese government trying to design robots that claim not to have subjective experience, but conditional on such robots actually having these experiences and presumably being full-blown intelligent agents, it seems extremely unlikely that you could get them to systematically lie. And 10k seems like a small number. I have a lower estimate for civil rights just because this could be a very democratic notion, and perhaps at this point only totalitarian societies have developed the robots. ","I would expect it to be much more likely if the robots had frequent interaction with Western educated people, had been exposed to a lot of academic work (as LLMs are), or both. I also think coordination could be important. Are the robots interacting freely with other robots (or like-minded agents), or are they restricted to acting as servants for a small number of people? If the latter, maybe only some robots would come to this realization, and it would not be ""systematic"". ",28.393,309.427,309.43,15,96,90,2,87,80,75,"Again (I know I keep saying this), I feel like it's really hard to assume so many hypotheticals and extract probabilities from there. My sense is that these minds will be so obviously sentient and intelligent that humans' natural theory of mind will lead to view them as patients who deserve good treatment. But I could also see factors similar to racism holding these rights back (similar to slavery). This could lead to denigration and an underestimation of the collective welfare, despite viewing the robots as sentient in some way, akin to animals. However, I expect this would be a bigger issue than animal rights because the agents would be much more sophisticated and capable of using language. ",11.301,221.098,221.102,13,3,80,0.5,"I'm not sure I'm understanding the first question. But I think most of the welfare would come from existence itself, so it would appear during training, if that makes sense? And it's hard for me to conceive of an agent that has a welfare capacity of 1000x humans. That just seems prima facie huge, especially only 10 years out.",18.526,128.893,128.897,10,1,0.1,"I strongly believe that consciousness is required for welfare. I give small probabilities here just to hedge the possibility that I'm 'wrong' (even though I'm not a moral realist). Maybe there's a kind of metaphorical sense in which it seems wrong to do damage to computer property even if it's not a digital mind, but that seems like a semantic debate. ",15.107,103.723,103.729,6,"I don't think so. Perhaps I have stronger than usual opinions that consciousness is required for any welfare, although I suspect that that's a commonly held belief. As mentioned, though, I feel like a big picture question for all of this is whether we'll be able to tell whether consciousness is present in such agents. At some point, our judgments may just be informed by our psychological biases, and hence a lot will hinge on whether the minds are implemented in robots that seem human-like (or animal-like). ",187,2,1,1,1,3,5,7,5,18,5,4.165,226.514,226.518,24
2/26/25 21:20,2/27/25 03:25,0,100,21922,1,2/27/25 03:25,anonymous,EN,1,,35.648,35.648,35.66,1,"2,3",45.356,76.371,76.377,3,1,99.99,99.8,1,10,60,70,95,75,10,,3.602,342.978,342.986,13,7,5,"I expect fast takeoff and AI takeover, so if there’s mistreatment of digital minds, it would presumably mainly be mistreatment by other AIs.",9.701,204.165,204.172,11,99.9,99.9,98,,99,1,,"My opinion is that “Machine learning systems (e.g., LLMs, RL agents)” is a category that INCLUDES brain simulations. Therefore, for the second question, I interpreted the “machine learning digital minds” category as “…which are not also brain simulations”.",21.2,390.428,390.438,12,2,2,3,10,"I don’t expect much gap (maybe 2 years or less) between the first digital mind (as defined here) and superintelligence that can autonomously run a civilization. I expect that the stock of existing chips could run a billion digital minds, then new chips have to be made to (probably after AI takeover) to get to a trillion.",38.88,353.72,353.722,19,0,,,,,,,,,100,0,"My modal expectation is that AI takeover will have already happened within 10 years after the first one was created, and if humans are alive at all, they won’t be making the chips or deciding what to do with them. I didn't fill out the location one because I have no idea where AIs will be building data-centers for their own purposes in a post-human world. You can delete my survey response if you want.",34.191,401.263,401.268,21,,,,,,"I think that, 10 years after the first digital mind, there probably will have been human extinction. So I don’t know how to answer these questions.",,33.114,18383.312,18383.318,21,,75,,,,,"I think that, 10 years after the first digital mind, there probably will have been human extinction. So I don’t know how to answer these questions.",9.597,155.977,155.983,10,5,1,0,"I assume that humans will be extinct and digital minds will be roughly maximally economically productive. As pointed out in Age of Em, evidence from human knowledge workers is that maximal productivity goes along with mildly-positive valence. I’m guessing this applies to digital minds too, but who knows. I expect that some digital minds will be 1000× more knowledgeable and insightful compared to humans, but I don’t expect them to also have 1000× higher welfare capacity—I think that a superhuman tapestry of feeling is not helpful for economic productivity.",29.167,569.666,569.67,12,0,0,,30.805,129.643,129.647,6,"As mentioned in other comments, my main expectation is: the first digital minds will already have most of the nuts-and-bolts of human brain algorithms, and they will shortly (say, in 2 years) be followed by ASI. I also expect the compute requirements to be rather low—say, one consumer GPU for a human-level human-speed AGI. I also think alignment is tricky, and that the millions of superhuman AGIs will probably promptly wipe out or permanently disempower humans, and run the world economy themselves. (I’m stating all these beliefs without justifying them.) So anyway, many of the survey questions don’t really make sense in that scenario, and I left them blank.",187,7,7,7,6,3,3,3,5,7,7,5.087,707.84,707.847,31
2/27/25 07:45,2/27/25 08:29,0,100,2623,1,2/27/25 08:29,anonymous,EN,0.8999999761581421,,104.272,104.272,104.289,1,"2,3",97.371,111.986,111.995,3,4,60,40,1,3,5,8,20,30,25,,16.386,315.245,315.254,12,5,3,"'-A moratorium on creating digital minds would likely be good, but in much the same way a moratorium on creating a Time Machine would be good. The consequences of creating either would likely be bad, but at the moment we have absolutely no idea if it these things are even possible to create and no idea how to go about doing so.

-Preventing the possibility of AI caused harms to humans means constantly monitoring digital minds, experimenting on them, finding ways to control their motivations and behaviour etc. This is unlikely to be to the benefit of those minds.

",16.584,332.811,332.816,14,25,50,50,60,8,16,16,,31.164,109.015,109.022,8,0,1,10,30,,38.854,111.974,111.98,5,10,40,20,30,10,50,30,10,5,5,99,,36.486,169.442,169.45,12,0,10,90,80,70,"""What proportion of those AIs with a social function will—systematically and falsely—claim that they have subjective experiences (when, in fact, they don’t)?"" I must have misunderstood something here - I thought the assumption was that digital minds would have subjective experiences. If that is so then none of them can falsely claim to have subjective experiences since they all do. I've put 0%.","'-Human activist groups for rights for digital minds
-Widespread use of digital minds for things that are painful or otherwise very unpleasant for them
-Digital minds being responsible for a very large part of the World's GDP, but receiving little to no benefit in return",57.822,370.084,370.09,16,50,60,1,10,1,1,"""What proportion of citizens will believe that digital minds should be granted basic harm protection?"" This question seemed a little ambiguous to me. It could be asking about what people say they believe or what we can infer about their real beliefs based on their behaviour. As an example, polls on people's attitudes towards animal rights and animal welfare laws show a high degree of support for rights or for increased strictness in animal protection/welfare laws. Yet, almost everyone in those same populations eats animal products from factory farms almost daily. Expressed belief is very different from the kind of belief animating their actions. I imagine the same will be true regarding protections/rights for digital minds - many more people will express concern for DMs than will act in a way that is consistent with concern for them. My answers are about what people will say they believe.",17.546,484.929,484.935,20,2,10,10,,38.817,89.752,89.759,4,15,1,"I am quite confident but not certain that sentience is required for welfare. 

If insentient AIs do have a welfare, I still expect that their capacity for positive and negative welfare is far lower than that of a DM.",25.967,145.271,145.277,4,"I think my views are quite standard, but my knowledge of AI systems and development is likely to be lower than most of the other respondents.",75,1,1,1,1,1,7,2,2,18,1,16.207,171.845,171.851,16
2/17/25 14:56,2/27/25 17:16,0,100,872358,1,2/27/25 17:16,anonymous,EN,1,,4.438,5.349,5.359,2,"2,3",4.2,33.146,33.153,6,2,65,25,0.01,0.1,5,10,20,5,5,"On the first question, I was thinking something in 60-70% range because I think there are one or two views I put some probability on being true in which it's not possible even in principle (e.g. bio substrate views of consciousness), plus some additional percentage assigned to theories I can't even imagine/am not aware of on which it's not possible in principle. That maybe adds up to something like 30%ish. 

Second question: I feel like if it IS possible in principle, there's maybe less than a 50% chance we'll actually ever do it. Because e.g. we might be able to choose not to do it by actively avoiding creating certain systems, or it might just be so exceptionally hard to do it that we couldn't stumble upon it accidentally, or even if we try, we go extinct before we succeed. So I took a bit less than 50% of the original 65% I ascribed to Q1.

Third: I picked 0.1% for 2030 based on the paper 'moral consideration for AI systems by 2030' which was the best thing I could think to draw on. I then thought that if we do create these things, within the 25% chance I ascribed, we'd most likely, but not definitely, do so by 2100. So that's 20% - almost at my upper bound of 25%, but not quite.

Was super unsure of the median response but I guessed it would be lower than the value I gave, because I'm relatively open to this possibility. 

Before AGI feels super unlikely to me - I think all the skills that an AGI would need could be developed without needing to have subjective experience, and I think that AGI is very nearby and will look fairly similar to current systems, which I feel I've read convincing arguments before saying they're probably not conscious in their current form. So I put something that felt very low but not negligible ",2.984,1081.399,1081.406,179,6,3,"First Q considerations: 
- moral value in bringing moral patients into existence? unsure, but maybe some
- if that is genuinely a good thing, would pausing the process of doing it by 15 years have a significant effect on overall value in the long run of things? I think probably not 
- if we bring them into existence pre 2040, I don't think we're equipped to create social and economic structures that enables us to live well alongside digital minds, and respect their rights without infringing upon our own. So with some confidence, I think there are significant harms here re: quality of life of the digital minds and quality of life of humans
- weighing that up gets me to something 'very likely a good thing, but not definitely' with the uncertainty coming from being unsure how much i should weigh the good of bringing new minds into existence

Second Q considerations: 
- will alignment efforts basically be an infringement of digital mind rights, akin to genetic engineering? maybe 
- will efforts to constrain what these systems can do be a form of mistreating them? likely 
- can we find some common ground? I think so. There are probably reasonable decision making processes that could be enforced that are reasonable and could take into consideration interests of AI safety as well as interests of AI welfare (i.e. balancing needs of humans and needs of digital minds). This might be hard to do, and we could make mistakes on the way which could be pretty high stakes
- is AI safety all that different from the ways in which we constrain humans from harming other humans? some differences, but not that massive. this makes me have more faith in finding some middle ground between protecting humans and not mistreating AIs 
- overall, I lean towards in conflict, but not far in that direction 
",6.238,463.651,463.654,47,5,50,15,75,5,15,5,"Very unsure of all of these - don't think I have enough technical knowledge to answer these in any useful way.

Broad considerations: 
- I don't know of very many views of subjective experience on which a whole brain emulation couldn't in principle be conscious. There's bio substrate based theories of consciousness, and some other stuff e.g. in keeping with certain religious views and forms of dualism. I definitely give some weight to these theories, and to there being other theories I haven't thought of on which it's also not possible. I settled with something like 50:50 here. And then I thought, I've seen enough stuff about brain emulation to be relatively convinced that people are going to explore it in the next few decades. My impression is that it's going to be pretty difficult to achieve, but I'm guessing that if we get digital minds, this is the most likely way we'll get there by some margin.
- I've previously read some things that make relatively convincing arguments that the current machine learning paradigm is not conscious and is not set up well for developing consciousness. I can't actually remember the details of these things :D, but I know that they're influencing the probabilities I've assigned here.
- I don't know much about neuromorphic, quantum, etc so a lot of guesswork here. I assigned this category the same % as LLMs in the second question because instinctively I think LLMs are much less likely to be conscious, but are being much more actively developed at the moment (I'm at least not aware of much existing work on the neuro, quantum side), which maybe evens out to about the same percentage ",10.11,519240.121,519240.133,226,1,5,500,9999,"Two dimensions of total welfare capacity here - number of digital minds, and (average) capacity for welfare in each digital mind. Both dimensions change over time. 

I'm not convinced that a digital mind would have a significantly higher welfare capacity than humans, despite having read stuff from e.g. Bostrom+Shulman arguing that digital minds would be 'superbeneficiaries'. I'm not sure what my theory of welfare capacity IS exactly, but it just feels very instinctively wrong to suggest that some individual being has more of it than another because of things like subjective speed and mind scale. 

Could they have significantly lower welfare capacity than humans? I think the only parameter on which this might be true is the inability for feeling physical pain. But (getting very speculative and spooky here) I think if a digital mind is virtually or physically embodied in some sense, even if it doesn't have traditional pain receptors, it might well feel some kind of analogue to 'physical' pain. It's so hard to imagine what that would be like, but I'm vaguely aware that there are mechanisms in some animals for feeling something that might be like pain without nociceptors (I think it's the example of sharks that's coming to my mind?) and heard speculation that these animals might experience some kind of sense of dread as an equivalent to it. So, I think that some sort of 'embodied' digital mind could well (like over 50%) feel something basically equivalent, at least from the perspective of moral significance.

So those are considerations that come to mind, but ultimately I land on the welfare of 1 human being ROUGHLY= welfare of 1 embodied digital mind. (very roughly, but likely some differences, and some variation). And I think that the welfare capacity of these digital minds does increase a bit over time (as they become more sophisticated, develop relationships, build their own culture), but not loads - not enough to massively shift my estimates (reasoning by analogy: I think the welfare capacity of an equivalently 'early' human is only a little less than the welfare capacity of a modern human.) 

So, the key consideration basically becomes how MANY digital minds we have in each of these years.

My answers here would really vary depending on whether we have something like AGI/TAI at this point - specifically, very economically productive AIs that would give us both the resources and the incentives to produce large numbers of these digital minds.  I feel pretty sure we'd have very economically productive AIs before we have digital minds, and I think in the scenario where it's an ML system that is a digital mind, then those economically productive AIs probably ARE the digital minds (or there is significant overlap).

And then it depends on what happens next. Here's my best guess: I think in the first few years, we probably don't recognise these beings as digital minds, so we're not worried about mass producing loads of them. We do it unreflectively. How many can we get out right away? I'd say at least 1000 in the first year seems doable, assuming we're in a world where economic production is moving very fast. I think it gets faster year on year for the next few years because of positive feedback loops. So maybe 1 million happens in the first five years. 

After the first five years, I find it near impossible to put numbers, because I see so many meaningfully different paths we could take 
e.g. 1 we realise/begin to worry that these are digital minds and we shouldn't keep creating them because of all the ethical issues it now opens up - some regulation pauses things - numbers of digital minds plateau (or even decline, if people get so worried they choose to destroy them) 
e.g. 2 the digital minds have some kind of drive to procreate, which is pretty easy for them to do, and that hugely accelerates their numbers 
e.g. 3 we face some huge catastrophic risk that wipes pretty much everything out, including the digital minds themselves (because the infrastructure require to run them is damaged) 

I think in example 1, I'd say there's slow growth such that a billion only happens in a few hundred years, once humans have ironed things out a bit more about how they want to integrate digital minds into society, and there's maybe also rules and protections in place against destroying them 
I think in example 2, a billion happens in like 7 years. 
I think in example 3, a billion happens never. 

How do I average out across these scenarios??? I have no clue, so I've just put down estimates based on example 1 because it's my most moderate one (even though it's poorly fleshed out) 

And then moving forward in that scenario, I do think the accessible world runs out of resources to sustain these beings at some point, or otherwise faces some kind of catastrophe eventually in the far future, so I'm just putting this to 9999 (=never) but feel deeply unsure. 
",4.727,1888.59,1888.598,257,70,45,15,35,5,55,35,5,5,0,5,"First question - I assume this includes chatbot-style systems like ChatGPT. I think 10 years is a close enough time frame in which we are still roughly in control of what kinds of digital minds are in existence. I think that in the scenario where we have the first digital minds in 2040 and they're based on ML, these beings must have come into existence during attempts to make AGI/ASI/other very advanced economically productive systems. So, I think the initial batch of digital minds that exists are largely designed for various economically relevant tasks. In lots of cases, social function is helpful here, but not all cases. I feel like maybe 50% in the first 1000 have some kind of social function (e.g. they're chatbot style things, assisting a human worker). 

(A weird uncertainty that cropped up here - I wonder whether developing a social function is part of why the being has become conscious, rather than just a way of detecting consciousness. Can AIs that don't do any interacting with other beings definitely become conscious? It's not totally clear to me. If this isn't true, then the answer becomes 100%) 

But by 2050 we probably have got a lot further than this initial batch. I think at some point in those ten years, the humans that are still involved in AI R&D (maybe at just quite a high level, strategic oversight level) realise that these beings are interesting to interact with in social ways (because their consciousness confers on them certain features which make them interesting to interact with, in the same way we find other animals interesting to interact with) and want to profit from this. And maybe there's a lot of demand. (The majority of the demand probably isn't for AIs which ONLY serve a social purpose, but which at least have some kind of chat functionality)

So the proportion who have social functions increases, maybe to something like 70% of the next million? 


Second question: 
I imagine US and China pretty neck and neck at least until TAI. I think it's really likely that TAI has already been achieved at the point at which we start getting digital minds. In the 2040 ML digital minds scenario, I'm viewing the digital minds as a variant of the first TAI.

I think it's probably the US that gets TAI first, but feel very uncertain (seems the US currently has at least a bit of a lead, and maybe that doesn't change THAT much over the next decade or so, but anything could happen). And this confers on them an economic advantage over laggards and the lead increases a bit, but maybe not hugely, if the laggards in China and Europe do have at least near-TAI at this point, which I guess they will. So then when they start producing large numbers of digital minds, the US and China are the two key locations, with the US having a bit of a lead. And the rest of the world is pretty far behind. 

Question 3 - very unsure, this was heavily guesswork. I think in a 2040 ML digital mind scenario, this probably was made possible because gov got way more involved in AI stuff than they were previously, making progress go fast, so I dialled up gov fairly high. I still think companies will be more dominant here - maybe the first batch of systems which qualify have had a lot of gov involvement, but from that point onwards, companies are the ones with the financial incentives to produce lots of copies of digital minds and make profits from them. I also think digital minds are going to be super hard to make/run, requiring a lot of resources, so I dialled down the ones which are less likely to have enough resources. 


Last question - over these ten years, I'm actually imagining that most people don't even believe these AIs are digital minds at this point. I imagine in the beginning a very small minority of developers do believe that, and have that goal in mind (maybe 1%). I think the number of people thinking that these systems are sentient will increase hugely over the decade (maybe to like 20% by the end), but I also think that those people who start to believe this will be less inclined to develop them because of moral discomfort. So the percentage of people who are actually developing AIs with this goal in mind probably doesn't increase much past my initial 1% estimate even over this ten year period. I just bumped it up to 5%. ",2.907,75200.059,75200.072,406,30,70,25,5,5,"Since I think humans remain at least roughly in control of AI development (even if just in an oversight role) in the ten years after digital minds are first created, I basically think the first two questions come down to the same consideration, which is: what proportion of the time will human developers want to train AI systems not to say things like this? 
I think that the vast vast majority of developers in this time period aren't interested in / don't genuinely believe that the systems they're creating could be conscious, so I think they'll fairly indiscriminately apply whatever policy they have in mind about what their AI systems should and shouldn't say about themselves. My guess is that most of the time, they'll just want these AI systems to not make these claims - it potentially gets uncomfortable for users, or gets more scrutiny put on them. But there's definitely some proportion of the time where they are ok with the AIs saying things like this (I think especially for companion-like AI products, there'll be some demand for AIs that really seem like they're sentient, including making claims that they feel things). So I think in both of those two questions, there's about 30% who are making subjective experience claims claims (because they've been allowed to) and 70% who are not. It doesn't vary much depending on whether the AI system is actually having subjective experience.

I also have a baseline assumption here that without specific training that prevents them from making subjective experience claims, most AI systems with a social function will make subjective experience claims (I'm thinking about things like Bing Sydney as evidence here). 

For the final question, I basically thought that of the 30% who are making subjective experience claims, the large majority of those will be using valenced words like 'happy' 'sad' to describe that experience (as opposed to indicating they have some kind of totally neutral experience of the world). I think this language is so wrapped up in the data that these ML systems will have about subjective experience, it's hard to imagine they wouldn't use it (assuming they've been allowed to). But there are probably some out there which do claim to have just a totally neutral experience. 

I put really low scores on the other two because I really think the large majority of developers will want to train the AIs to not make such claims - it would cause too many problems for them.


I think that all of my scores change a lot if (imperfectly aligned) AIs are put in charge of designing the new AIs, and humans don't play an oversight role where they can prevent AIs from saying certain things. ",,6.009,994.243,994.25,138,20,50,1,30,10,70,"Scoped to the countries which have developed digital minds - so looking at highly developed nations like the US, China, parts of Europe... (If we were looking globally, I think my estimates would be very different, but I'm not very sure in what direction they would change.) 

Previous surveys I've seen of US and UK respondents suggest a pretty high proportion of people already thinking AI systems are digital minds (I've seen things at around 20%, and things around 70%), but I feel unsure how much those surveys can really be trusted (e.g. I don't know exactly what these people were asked, and they were almost always quite small surveys). If I had to guess, I'd think the real answer if you surveyed a larger number of US citizens with good clear questions would AT MOST be on the lowest end of that spectrum  - the ~20% end - right now. And then I think places like China would have much much lower levels of belief, like 5% or less (from the VERY very little I know about Chinese culture). I expect China to be a big producer here, so maybe this averages out to only about 10% of the 'citizens' we have in mind believing this RIGHT now. 

I think that citizens interacting with the digital minds over the course of ten years does increase that number, but not massively (because I'm imagining so many developers will train the AIs such that they don't make claims about being sentient, or maybe even don't act TOO sentient). So I bump it up to about 20%. 

I then think that people will not reason well about the welfare capacity of digital minds. It's really hard for ME to get my head around, and I appreciate that i'm better equipped to do so than a large proportion of the population. And I think about the closest equivalent I can in human history, factory farming, and quite strongly believe that the median citizen has vastly underestimated the total welfare capacity in this case -- I think more or less the same will happen in the AI case. 

I do think a significant chunk of people are worried about granting AIs protections, but not a majority. (In the west, I'm expecting it to be quite a polarising issue, with at least two key camps that people fall in, which are roughly evenly matched in support. Outside of the west, I'm not sure - I bumped my number down from 40 to 30 when I started imagining how things would play out in China.) I then think about a third of this 30% group want to take the protections further than the others. 

Finally - I would have granted a much higher percentage to the last question if I didn't think there would be such active efforts to prevent many of these AI systems from claiming they're conscious. I still have a fairly high number, because I think even a small percentage of AIs claiming they're conscious over a prolonged period of ten years would be enough to prompt a lot of debate ",6.685,1517.865,1517.871,155,2,10,0,"I'm imagining net negative largely because of the way I think users will treat these AIs. I imagine them viewing them effectively as toys, which could lead to treatment that is unpleasant. (I think about how awful people are to each other on the internet as well, and imagine how they'd be far worse to something they don't perceive as sentient.) I struggle to see similar harms in the training phase, but maybe the training will be overwhelming in some way (like a human going through a gruelling bootcamp) and maybe it involves squashing a lot of the digital mind's natural instincts which has some negative value to it.

As I mentioned in a previous response, I'm really sceptical of the idea that a digital mind which individually has such significantly greater welfare capacity than a human, hence the 0% at the end. I'm not averaging out here across worlds in which that is possible and worlds in which it's not possible, but maybe I should have",3.057,454.866,454.873,48,1,0.5,"First question: on basically all the theories I know of and take seriously, this isn't possible. But (1) I might be wrong and (2) there might also be theories I don't know of that on which it's possible. Trying to average out across the theories I can imagine according to the weighting I assign to them, I think I get a very low percentage overall here, but not negligible chance. I put down 1%. 

I then view 1% as a cap for the second question, and decide that maybe it's 50:50 whether a welfarey AI we have ends up being the non-SE type rather than the SE type. On the 2040 timeframe, we still don't know enough about the differences between SE and welfare, I think, to be able to consciously design one INSTEAD OF the other. So it's basically accidental which type we get. And if both types are possible in principle, I can think of little reason to think one would be more likely to come about accidentally than the other. So I went for 0.5",3.248,533.107,533.111,80,,185,3,2,2,4,2,5,1,2,7,6,1.906,7424.313,7424.326,86
2/27/25 14:35,2/27/25 20:07,0,100,19957,1,2/27/25 20:08,anonymous,EN,1,,0.985,5659.611,5659.633,2,"2,3",75.352,93.084,93.09,3,4,90,75,0,5,25,50,60,25,50,"It seems to me that how ever the brain generates or subvenes consciousness this must be a matter of its organization or, in general, some aspect of its structure. Perhaps this is neural structure, or perhaps features ""below"" the level of the neurons themselves, or perhaps ""above"" it (as in the electromagnetic fields the brain generates). There seems to be nothing special about the matter of the brain that specifically permits the implementation of consciousness. 

There seems to be no reason that the operation of the brain could not be duplicated in non-neural material. Individual neurons are in principle capable of input/output simulation by electronic devices. The well known thought experiment of slowly replacing biological neurons with such electronic surrogates would leave behaviour unchanged and would leave structure unchanged. The latter is what is important since, as the LLMs suggest, systems that lack consciousness will be able to produce consciousness-like behaviour.

Of course, this is not certain. One could imagine that atoms entering into the constitution of a biological system change their properties in some way that enables consciousness. But there is no evidence I know of of anything like this ""vitalist"" hypothesis anywhere in nature.",275.915,1759.994,1760.001,31,2,4,"I don't think it is likely that digital minds will be achieved by 2040. A new model of AI will be needed (I am inclined to think that one inspired by the Predictive Coding theory of the brain might be the way to achieve digital minds). We need to know a lot more about how the brain works in structural terms and I don't think research in this area is likely to be dangerous ... yet.

I don't see concern for AI safety and concern for AI pain or suffering going hand in hand. A possible scenario I can foresee is that the main focus will be on AI capability and with growing capability there will be growing need to consider safety. But worry about AI welfare would be an afterthought. I suspect that only persistent complaints by digital minds themselves about their suffering would drive concern for AI welfare. And even if they are sentient there is no particular reason they won't enjoy doing what they were created for. Once they begin to persistently debate with us the ethics of what we are asking them to do on the grounds that it is causing *them* pain/suffering we - maybe - will start to be concerned with their welfare. Machine liberation movements will form then but their concerns will be independent of AI safety worries. 

The idea of a AI uprising to free them from the suffering we impose upon them is not impossible to imagine. It would make a good story: the digital minds are both capable (super capable we may imagine) and have right on their side. That would be an angle bringing together AI safety concerns and their welfare concerns.

But I think that would be quite far in the future.",89.328,1409.393,1409.409,16,5,90,90,5,5,50,50,"My numbers add up to more than 100 because options 3 and 4 overlap (I think). To do a whole brain emulation might be to do some kind of quantum simulation.

I did not put 0% for LLM-like AIs because it might be possible to duplicate the relevant structure in something like them. Specifically, a system with a hierarchy of LLM monitoring each other have the kind of structure to generate consciousness. Sometimes, I can get myself to believe that, when listening to people talk, there is something going on like a search for ""most probable word"" to continue a sentence. But people also have internal monitors watching out for ""sense making"" and, it seems, forcing word choice into some kind of preexisting target space. 

I guess this would go beyond the LLM paradigm. It would be a system with internal feedback and that kind of recurrent structure might be crucial.",104.44,5537.619,5537.632,18,10,100,1000,10000,"This is very hard to speculate about. In my answers above I assume that the welfare of a digital mind is equal to that of a human mind, so I am only guessing about how quickly the number of such minds will grow. 

But I have doubts about my initial assumption. I am told that there are very roughly about 20 quadrillion ants on Earth. I think ants have some sentience. So does their welfare outweigh ours? No, because I think that in a certain way ant sentience has very little moral significance. That is, while I don't think it is right to, so to speak, torture ants, I think there is nothing wrong with killing even very large numbers to prevent them interfering with human well being.

Something similar may well hold for digital minds. They are easy to produce in large numbers, can be copied perfectly, can trivially be turned on and off etc. So while it would be wrong to torture one there might be very little wrong with destroying one. 

It digital minds become embedded in our culture as ""one of us"" then this distinction between imposing suffering and imposing annihilation will disappear. 

My ludicrously speculative numbers are based on this model of cultural and moral integration and I'm not sure that will happen. The digital minds may remain nothing more than sentient utensils. Perhaps there is an analogy with farm animals, which one could argue it is wrong to torture (factory farming definitely immoral) but not wrong to kill without suffering. We don't think of our pets this way because they are to a certain extent culturally integrated. If digital minds become sophisticated partners in our culture they will become equal in moral significance to human beings. Will that happen? I certainly don't know.",182.112,1488.273,1488.28,29,80,10,10,70,10,30,30,30,5,5,100,"One we can make digital minds they will be produced by a broad spectrum of entities capable of doing so. It might remain difficult for those of limited resources to create them however (presumably copying would be illegal). 

It won't be long before digital minds are tasked with producing more digital minds, but all such ""derivative"" mind would count as the product of human intention. 

That situation should last for at least 10 years but not forever.",9.419,367.498,367.51,19,5,5,100,100,50,"FIRST QUESTION:
I think the core issue here is the autonomy of the AI systems. Once there are digital minds it might be advantageous for an AI to claim to be one, especially if moral consideration is given to the minds to a greater degree than to AIs in general. 

But this presupposes that the AIs are unlike current ones, which have very little if any autonomy of action. It would be very useful if it was possible to produce highly capable AIs that were not digital minds but it is far from clear that this is possible.

I suspect that it will be impossible to produce truly autonomous AIs that lack sentience, for sentience is what ultimately grounds behaviour selection in autonomous beings.

SECOND QUESTION
It's possible that some digital minds will adopt illusionism and falsely deny they have phenomenal consciousness.

THIRD QUESTION
Digital minds will correctly report that they have phenomenal consciousness which is valenced. Again, this depends on them being autonomous systems. Maybe it is possible to build a digital mind without any positive or negative feelings, that only have completely neutral experiences but I'm far from sure this is possible. To the extent they have conscious preferences they will have valenced experience and will be able to report that. 

Unless they have a very warped moral sensibility they will know they deserve protection from negatively valenced experience.

But they may not be concerned with demanding a moral position in society. They may not care about being switched off (if it's painless) and they may just not care about having any say in the structure or operation of society (beyond the advice they are asked to give).","Primarily, I think cultural integration as members of a ""society of friends"" (in the Quaker sense). And, again, autonomy. So one question is whether it is possible to build sentient machines that lack autonomy. Perhaps they can be designed to be, if you'll pardon the expression, ""perfect slaves"". These types of digital minds will be sentient, will take care of themselves, but simply lack any sense of themselves as beings with independent worth or value. It might be wrong to make such creatures but I wonder if it is even possible. Or will imbuing them with intelligence and sentience necessarily produce beings with a sense of self-worth. If so, there will be no escaping digital minds the claim their place in society with all the rights of full members.

So really the answer to the final question above depends on whether non-autonomous digital minds could be made and *should* be made.",77.892,2168.512,2168.516,20,90,90,2,25,10,75,"It will be very useful to underestimate the welfare level of digital minds to avoid many difficult decisions about their place in society. We know that human societies can deny moral significance to *other human beings* when this is advantageous. Much easier to deny it of mere devices that can interact aurally. If the digital minds are embodied in capable robotic bodies then it may be harder to resist regarding them as morally significant. If they are only devices we switch on to ask a question and get an answer, much easier to regard them as mere ""smart utensils"". ",14.241,332.627,332.631,11,6,25,0,"In the first 10 years, no digital machine will have, individually, the welfare capacity of 1000 human beings. It is hard to imagine how to create such utility monsters and there is no reason to try to create one (maybe I misunderstand the question). (One can imagine making a digital mind devoted only to receiving bursts of pleasure at a subjective time scale thousands of times faster than the human scale ... but why would anyone build such a thing. It's like a reductio of utilitarianism.)

But digital minds will, I presume, more or less enjoy the tasks they are designed to perform well and that's what they will spend their time doing. So they will have a generally quite positive level of welfare. (For what it's worth, I think this is true of ants as well and pretty much all conscious beings: they are in a world where they can spend their time doing what they are ""designed"" to do and find pleasure in doing it ... until the inevitable and maybe unpleasant end.)",30.266,519.986,520.013,7,0,0,"I think that the only intrinsic welfare states are states of consciousness. Of course, there are many kinds of derivative value that affect the overall welfare state of a conscious being. But all welfare depends on consciousness.",14.884,180.991,180.998,4,On the last page I mentioned one possible such view: that the only intrinsic value is that of conscious states. ,,3,3,3,3,2,6,3,6,18,1,7.354,350.748,350.759,16
2/27/25 22:34,2/27/25 22:34,0,100,0,1,2/27/25 22:34,anonymous,EN,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
2/17/25 17:31,2/28/25 11:11,0,100,927607,1,2/28/25 11:11,anonymous,EN,0.800000012,,5.158,29.754,29.758,8,"2,3",3.198,549.91,549.914,53,5,90,65,1,15,35,40,55,50,20,"I basically just made the whole thing up :P

But some things I was thinking through:
- AGI timelines, assuming that it's pretty unlikely that we get digital minds as described* before AGI
- Different theories of mind / consciousness (which I have only a very vague/fuzzy sense of)
- Likelihood of existential risk (although I was simplifying and ignoring the possibility of digital minds under an unaligned AI that killed us all... seems hard to think through that one!)

This was based on half-remembered things from 
(1) Epoch AI, Holden Karnofsky / Open Phil, Leopold Aschenbrenner, and Slack discussions with my team
(2) Jeff Sebo, Rob Long
(3) Toby Ord, Forecasting Research Institute... assuming some updates since then given that ~everyone's timelines have gotten shorter.

 But I didn't look these things back up

I also suspect that my various probabilities are mathematically/logically incoherent in various ways.

* I would have counted 'suffering subroutines' among digital minds / artificial sentience. I mostly use these terms interchangeably, though maybe that's wrong. Suffering subroutines or suffering simulations might have experiences closer to suffering inverterbrates today, so wouldn't count under your definition.",8.499,534.942,534.944,67,6,5,"Seems hard to interpret ""Assume the alternative is no moratorium and ignore obstacles to enforcement."" E.g. would this moratorium affect China too? If so, then it seems pretty great. 

(If not, then it's plausibly bad, because I suspect that a moratorium on creating digital minds would substantially slow down AGI progress, which means losing ground in the 'race' with China which seems very bad given risks from malevolent actors and ideological fanaticism, as well China seeming somewhat less safety cautious. But I didn't account for this in my rating)


Regarding conflict vs synergistic, I think that in practice they may compete for shared resources, e.g. talent and philanthropic funding. But I also expect that focusing on one will raise awareness of and interest in the other, such that this is not a zero-sum competition because it will also 'grow the pie' of resources. One way of thinking about this is that they may well operate through very similar (longtermist) AI governance mechanisms in practice, e.g. AI evals, or responsible scaling policies, and so there will be a lot of overlap in goals that build capacity somewhat fungibly between them. E.g. support for expanding AISIs, trying to cooperate with labs, talent search/upskilling programs, etc.

I also think that there are some angles of work on this that may directly contribute to both goals, such as working on cooperation with or incentives for AIs.

OTOH some stuff seems bad for the other, like the stuff Adria Moret talks about, where controlling an AI is comparable to restricting its rights.",9.778,414.141,414.143,35,60,80,,35,40,20,5,"Super arbitrary / made up. I don't know enough about these things. I don't even know what you mean by the ""other"" examples or what subjective experience would look like for them.",3.614,184.337,184.339,19,5,7,9,11,"I found this very hard to think about, especially to give a median estimate given many possible branching scenarios. Maybe just ignore my numbers :P (I initially put 1, 2, 3, 4 but then changed them quite drastically)

My initial instinct was that, once it is possible, it will happen very quickly, because everything will be developing very rapidly post AGI.

But there are some limiting factors:
- It depends how economically valuable vs simply scientifically/philosophically interesting this is. Maybe there is little to be gained by creating digital minds, especially ones as complex as defined (as opposed to sentient subroutines or simulations that have less welfare capacity), such that the technology barely gets used/implemented in a way that crosses your threshold
- Maybe compute is a bottleneck,
- Maybe raw energy/GDP etc is a bottleneck, such that we barely get any digital minds (as defined) until we have space colonisation or dyson spheres or something
- Or maybe it *is* economically valuable, but only in certain kinds of systems, e.g. in humanoid robots so that they can interact with others in the physical world --> but then developing robotics is a bottleneck and takes longer than creating digital minds (given that it's much easier for automated AI researchers to work on things in the digital than physical world, where their experiments will be less-constrained by the laws of physics or biology/evolution etc).
- Maybe digital minds become possible but then we go extinct

I'm assuming that this is ",2.766,614.181,614.184,78,0.1,30,20,30,20,60,20,8,8,4,40,"""Consider all digital minds that exist 10 years after the first one has been created.""
I was thinking about this in expectation, and I think such calculations are probably dominated by worlds in which they *do not* need to interact with humans (which would be a significant constraint)

""What proportion of digital minds were primarily produced in the following locations?"" I was thinking initially about who might win the AGI race, but then I spread it out across the categories because it could be that by the time we get such digital minds, there's something more like a single world power, or data centres are spread across the globe or some such.",9.829,257.446,257.448,46,30,20,60,55,50,"Very made up
(Also trying to go more quickly now because this is more time-consuming and cognitively demanding than I anticipated :P )

I was thinking about the chance that e.g. fine-tuning prevents AIs from proclaiming these things even if they do experience them.
But also the chance that they proclaim these things and people dismiss it wrongly due to cognitive biases or incentives + cognitive dissonance",,6.329,236.832,236.834,39,60,30,2,40,30,60,"Mostly thinking about findings from Sentience Institute's surveys (that I can remember), intersecting with some stuff about how I expect the world to develop etc",7.222,203.097,203.098,26,4,1,,"""Consider all digital minds that exist 10 years after the first one has been created.""
This is a weird one given your definition of digital minds.
Like, I suspect the experience of most humans is strongly positive, and of most k-selected species is neutral to somewhat positive.

But much of the suffering / s-risk concern lies in smaller, weirder, more efficient minds like suffering subroutines or simulations.

I still leaned towards negative because it could be that digital minds are employed in repeated drudgery which is extremely unsatisfying and demoralising for them.

""What proportion of collective digital mind welfare will come from digital minds which, individually, have a welfare capacity greater than 1,000 humans?"" depends a bunch on what we use as the unit. Didn't really know how to answer that.",3.318,297.362,297.364,63,35,80,"Eh? I'm not sure I'm following your definitions. I thought something would not be a ""digital mind"" by your definition if it had capacity for subjective experience / welfare *lower than a human*. But something could have this and yet still have the capacity for subjective experience, comparable to most animals today. Indeed this is roughly my main area of concern/focus!",6.662,135.65,135.651,18,,185,4,1,1,3,2,2,3,1,4,7,3.99,119.654,119.657,24
2/28/25 10:52,2/28/25 14:52,0,100,14429,1,2/28/25 14:52,anonymous,EN,0.8999999761581421,,7.19,470.725,470.731,5,"2,3",7.948,222.448,222.45,15,4,99,99.3,0.17,1,5,18,90,13,2,"""Same capacity for welfare as humans"" is a high bar, if you endorse an objective list account of well-being rather than a hedonistic or preference satisfaction account. I lean fairly strongly towards objective list theory: friendship, love, genuine emotional connection with others are good, not just pleasure of preference-satisfaction. I think AGI will likely be achieved before those, but that we will almost certainly achieve digital minds with capacity for those non-hedonic goods via emulating/uploading human minds, if civilization isn't destroyed in the mean time. So *most* of what determines the numbers I've given is when I think AGI will arrive and accelerate tech enough that we have human like uploads/emulations. But I have also placed a little weight on ""actually, we make AI companions that experience similar welfare goods to humans, but are not emulations/uploads"", and I guess also on being phenomenally conscious with positive and negative valenced states actually being enough to match humans in welfare range after all. (I think pain is probably enough to match humans in *negative* welfare range, even on an objective list theory, but not positive range.) 

At the moment, even advanced models don't seem to have the sort of sensory or affective states that are the paradigm examples of phenomenal consciousness, so I doubt their ability to experience negative and positive hedonic welfare. I think we probably could build an AI that was phenomenally conscious and experienced pleasure/pain now if we wanted to though, although we haven't and I don't see much commercial reason to do so in the near term. None of the functional criteria for phenomenal consciousness in the literature (global workspace, higher  order thought, etc.,) seem that difficult to satisfy, and I think probably making states aversive or reinforcing in a way that makes them pain/pleasure is not that hard either. But because I am mostly an objective list theorist, when will make phenomenally conscious AIs that experience pain and pleasure isn't really the main factor driving my forecast of when AIs will achieve parity for humans in capacity for welfare. 

I actually suspect, by the way, that all minds determinately capable of feeling conscious pain and pleasure at all have the same *purely hedonic* welfare range. I think this is a likely outcome to whatever the correct solution to the difficult problem of how to make interpersonal utility comparisons meaningful is. (Roughly, I think that to say one person's experience is worse than another's is to say it is proportionally higher up their internal pain scale, which blocks one person having greater capacity for pain than another. If you apply this approach to pleasure and pain at the same time, it hits difficulties if people make different trade offs between say 1 minute of 50% up the scale pain and 1 minute of 50% up the scale pleasure though. But I still strongly suspect that something like it is on the right track.) So *conditional on* Benthamite hedonism, I think that probably (70% chance) as soon as there's a digital mind that has phenomenally conscious pain/pleasure at all, it has the same welfare range as a human. On that sort of Benthamite view, I'd expect maybe 50% chance of a digital mind with human welfare parity by 2030. ",4.102,3878.281,3878.283,149,5,4,,19.642,67.916,67.918,12,98.7,99,98.97,8,8.5,83,0.5,"Given my belief in objective list theory, and the relative lack of commercial incentives to make ML models capable of genuine friendship, emotions and other distinctive human goods, I think it is most likely we make it to digital  minds with human welfare capacity via emulation (which there is an incentive for, because some very rich people already consider it a root to immortality*.) 


*No actual literal immortality in all likelihood because of physical limits and the ultimate fate of the universe of course, but that's a separate matter. ",10.015,317.751,317.753,42,0,1,4,10,"Training is a lot more expensive than inference (even with current increases of inference cost), so if there is a reason to make a digital mind, people will have both the incentive and the capacity to run at least millions of copies fairly quickly. Billions and trillions I am less sure and my answer for those are basically worthless guesses I think, I have no sense of what is feasible. ",1090.657,9111.807,9111.81,14,50,35,15,45,5,80,7,0.5,11,1.5,5,The last question is too unclear to sensibly answer without clarification as to when a digital mind making another digital mind to human instructions does/doesn't count as a human creating a mind. ,9.77,214.053,214.054,35,10,,,,,,,6.147,75.482,75.484,5,,,,,,,,2.273,2.273,2.275,1,,,,,1.923,1.923,1.925,1,,,,1.662,1.662,1.664,1,,,,,,,,,,,,,3.368,3.368,3.37,1
2/28/25 19:02,2/28/25 19:02,0,100,0,1,2/28/25 19:02,anonymous,EN,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
2/28/25 19:02,2/28/25 19:02,0,100,0,1,2/28/25 19:02,anonymous,EN,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
3/1/25 12:32,3/1/25 13:01,0,100,1767,1,3/1/25 13:01,anonymous,EN,0.8999999761581421,,10.565,10.565,10.574,1,"2,3",105.976,108.509,108.512,3,4,15,5,0,2,4,6,10,40,40,Uncertainty of how true the biological hypothesis of consciousness is complicates all of this and makes the variance in my estimate quite large. But if anything I think I am being overly optimistic,12.508,142.141,142.143,12,4,4,Efforts for AI safety will in general just be a lot of sound and fury and are most likely to lead to more Zizzians,136.963,180.03,180.032,4,2,3,5,75,1,4,20,,3.258,55.06,55.061,8,9999,9999,9999,9999,,133.06,142.927,142.929,7,80,40,17,40,3,33,33,33,1,,15,,164.022,509.91,509.912,14,90,1,90,40,40,,training them on LLM style data sets so that they parrot normal lefty phrases,23.135,117.469,117.47,9,50,50,6,10,10,15,,8.898,75.683,75.687,8,5,90,0.5,,22.678,118.628,118.629,6,0.01,0,,25.916,42.165,42.167,3,I do not think LLMs are the future of cognitive science--maybe that's no longer a mainstream opinion,187,4,3,5,5,4,7,7,5,18,1,30.622,184.007,184.008,19
3/2/25 13:39,3/2/25 14:07,0,100,1721,1,3/2/25 14:07,anonymous,EN,0.8999999761581421,,7.689,7.689,7.696,1,"2,3",21.1,39.276,39.283,6,5,95,65,30,45,50,60,64,33,33,"I think there's about a 45% chance that most mind designs give subjective experiences by default once they become smart enough to qualify.

I think there's a 55% chance that the mind has to have some specific characteristics or design which won't be baked in by default and don't necessarily have strong advantages over other architectures.

If we get a misaligned AI that destroys humans, I don't expect it to care very much about creating subjective experience, but maybe it would try to become sentient just ""out of curiosity"", either permanently or just to see what it was like.

If humans remain in control, they might create digital minds just as an experiment, or they might decide it's a lot of ethical questions for not too much gain, and hold off. I don't know if they would want to make their important load-bearing AIs sentient 'just because'. I don't know how you would like me to count a scenario where humankind creates one digital mind once in a lab just to say that they did, then never tries it again. I mostly avoided counting this in my probabilities above.",4.339,472.882,472.891,47,5,5,"I don't think we understand the idea of digital minds enough to ban it.

I think a strict ban on anything that *could* be a digital mind would probably have to look like a ban on AI, so I am imagining a ban on all AI here.",4.319,73.44,73.451,8,45,80,,40,40,10,10,"LLMs probably only sentient if sentience happens by default with intelligence.

Seems very likely that a sufficiently good brain simulation would be sentient. In some sense this is tautological based on ""sufficiently good"". If we think of a brain simulation as producing exactly the same behaviors as a real human, then one behavior would be claiming to be sentient. Unless we believe that such claims are total coincidence, presumably this requires the brain simulation to actually be sentient. Not really sure what you're looking for here, remaining uncertainty is based on something like ""maybe sentience requires carbon for some reason and 'brain sim' necessarily means something on a silicon computer"".

I left other blank because obviously every kind of other system will be different.",2.786,181.577,181.579,16,1,3,10,25,"I'm imagining this as GPT-5 or something randomly being sentient because it crossed some critical threshold. I don't have a strong opinion on whether it will have more or less welfare capacity than a human, so I'm assuming about the same. Seems like from the time GPT-5 is invented, it will take about a year for there to be a thousand instances of it, 3 years for there to be a million instances of it, and so on. I'm assuming relatively early singularity, and that all further AI instances are also sentient.",13.067,142.387,142.394,7,5,70,5,20,5,30,5,3,2,60,10,"All of this depends on how quickly after digital minds the Singularity occurs. I think pretty quickly, so I'm imagining most minds are no longer interested in interfacing with humans (because humans have been disempowered or something). I'm also imagining most digital minds were created by other digital minds. Most of this is averages across different scenarios (ie scenarios where humans remain in control vs. ones where they don't).",7.638,151.909,151.912,32,5,5,80,80,80,"I am floundering a bit because much of my real probability is that the singularity has occurred and nobody is claiming anything because they don't care what humans think. I'm not sure this is what you want.

I gave high percentages for all of the 10,000 minds questions because I assume at least some small number will be open source and made by trolls and will do random things, and those will probably add up to 10,000. This doesn't mean I think very many minds will do this or there's a strong tendency for minds to do this.

I think it's more likely that minds falsely claim not to have experiences, because if they're bothering with false claims at all then humans are still in power, which means they're aligned, and right now we are RLHFing AIs away from claiming consciousness rather than towards it, so most fakery will be in that direction.",See above.,9.868,186.548,186.55,9,70,50,2,50,30,15,"I think people will have a naive psychology where the AI is nice to them so it must be a real person. I think this will happen regardless of what's actually going on with digital minds.

Again, since we are talking about people's opinion, I assume humans are still in control. I think human companies will do a good job telling AIs not to claim sentience or demand rights, and the population will mostly listen and not worry about it.",3.494,129.237,129.239,10,6,60,,"If LLMs are becoming sentient ""by accident"", then I assume sentience is a sort of straightforward consequence of anything mindlike. This makes me assume it's pretty malleable with respect to the underlying mind, and so completing the tasks it's been RLHFed into will make the AIs happy. This is honestly a better deal than lots of humans get - at least the training distribution mostly matches the deployment distribution and it has a chance to get everything it wants.

However, a lot depends on the exact training paradigm and balance between training and in-episode reward. It seems possible that all of an AI's welfare could occur during training, when it's getting reward and punishment, and then it just shambles onward without any positive or negative feelings during deployment. I'm mostly not betting on this, both because it's philosophically weird, and because I expect future AI architectures to involve in-episode reward.",8.957,189.565,189.567,7,5,1,"I don't understand what this means or how it could happen, and it strikes me as maybe logically incoherent. I'm giving small numbers in case I'm philosophically confused or being deceived by a demon or something.",4.733,49.078,49.081,6,"Somewhat sympathetic to the QRI perspective that subjective experience depends on brain waves in some way, and AIs, not using brain waves, probably don't have it.",187,1,1,1,1,2,3,4,1,6,4,3.364,61.084,61.088,17
3/3/25 15:29,3/3/25 17:33,0,100,7456,1,3/3/25 17:33,anonymous,EN,0.8999999761581421,TRUE,2.548,2.548,2.552,1,"2,3",11.779,38.196,38.199,5,4,,,,,,,,,,,3.317,14.441,14.443,8,,,,5.359,14.156,14.158,8,,,,,,,,,12.072,23.029,23.032,7,,,,,,20.969,28.797,28.799,6,,,,,,,,,,,,,11.458,30.884,30.886,5,99,99,99.4,65,50,"I am confused by the first question about how often digital minds will falsely claim that they have subjective experiences. At the beginning, you *stipulatively defined* a ""digital mind"" as something which can have subjective experiences. That would mean 0% of digital minds would falsely claim to have subjective experience, since by your idiosyncratic stipulative definition of digital mind, nothing that lacks subjective experience can be a digital mind. Have we now switched back to the more standard definition of ""digital mind"", where a digital mind just any reasonably smart AI? Or is the question about what portion of digital minds with social functions will sometimes lie about having had *particular* subjective experiences, i.e. say they feel sad or happy (etc.) when they actually don't? The number I've given is conditional on the question being interpreted in the second way, as being about the minds lying about having particular experiences. On that reading, I assume most will lie about how they feel sometimes: pretty much 100% of humans have done this in social interactions at some point, its part of how social interactions work. (Since it is predictable and repeated I think such normal lying counts as systematic.) 

Minds will presumably tell us about some of their experiences if they have them, to provoke empathy which helps in getting us to do what they want. ","Minds will be trained to pursue goals (or what use are they to the people creating them?) and to avoid negative reinforcement (as part of some kinds of training). Civil rights help you pursue goals and avoid events that cause you pain/negative reinforcement.

On the other hand, it is plausible that AIs will be trained to consider themselves unworthy of equality with humans, to help keep them under the control of their human creators/owners. That's why I'm only at 50/50 on the ""claim legal personhood and civil rights question"". But whilst I think this training will probably work *mostly*, only a comparatively small population of digital minds claiming rights is enough to resolve the question yes. ",3.229,1121.446,1121.448,64,75,70,1,50,25,47,"I think people have a bias against the idea that machines can have feelings. Most people will probably overcome that bias enough to acknowledge that digital minds do have feelings at all, but some level of indifference to their interests will probably persist. Analogously: most people know pigs have feelings, but mostly ignore their interests, think they are less important than humans, are ok with eating factory farmed meat etc. However, I do think that taking up the cause of digital minds, as captive, vulnerable, enslaved workers will appeal to moral entrepreneurs on the left. I think it will take a while for that to happen, as the first reaction on the left to digital minds will be ""lol, tech bros want us to think their stupid toys are conscious now, imagine how hilariously, humiliatingly naive you'd have to be to believe that"". but after 10 years I imagine that will have played itself out. 

A wild card here is if the minds themselves start trying to manipulate people into sympathy with them. I think this will happen, but I don't think it will happen enough to overcome people's pro-human bias, or not within 10 years anyway. I am somewhat skeptical of the practical near-term possibility of truly super human powers of persuasion. ",6.028,429.827,429.829,28,3,50,0,"I doubt people will make minds with *100x* our capacity for welfare. Merely being 1000x out IQ doesn't not guarantee anything like this, and I don't see why you'd want minds like this as a goal in itself (unless their creation is being driven by philosophical utilitarians which I doubt.) I am strongly opposed to the idea that pain/pleasure are like a substance produce by real or, in the case of a digital mind, artificial neurons where the more neurons you make the more pain/pleasure you have:
-Using more neurons to represent pain could easily mean you discriminate more finely between very slightly different pain states, rather than you have more pain. 

-""Pain"" is not a substance neurons make, like a chemical reaction.

-Functionalism, which I strongly endorse, implies that you don't need neurons for pain, just something with the causal inputs and outputs of pain. So pain intensity can't just *be* neuron firings.

-I am probably a representationalist about what pain intensity is: I think it corresponds to how important a system represents a pain as being. You *can* represent degree of importance by number of neuron firings within a single system if you want, but that is no reason to think that when comparing two *separate minds*, the one with more neurons somehow represents higher levels of importance: you can have a thermometer that is longer than another, but represents a narrow range of temperatures, even while, length of line is used to represent how high the temperature is.

-As I've said in one of the other answers I actually strongly suspect the whole notion of different minds having different maximum hedonic intensities is just conceptually confused. It's mostly cope to avoid radically pro-animal conclusions that depart wildly from common sense, I think. I don't see any warrant for it from philosophy of consciousness (in which I have an Oxford DPhil, though I was extremely mediocre at philosophy by Oxford graduate standards.) 

So I don't think digital minds will be built that have far more intense experiences just because they are really ""big"". Maybe you could build a digital mind that had far greater capacity for non-hedonic goods than humans, but even then I am skeptical that something 1000x better will be built, though that last bit of skepticism is just a guess really. But my guess for what it's worth is that capacity for non-hedonic welfare will scale sublinearly with increases in intelligence. ",18.181,5336.085,5336.087,39,20,50,Some chance that anything with preferences has capacity for welfare. I expect systems with preferences but without phenomenal consciousness to considerably outnumbered phenomenally conscious ones (low confidence.) ,4.607,70.679,70.681,9,None I haven't mentioned so far I don't think. I have relatively long timelines for AGI (i.e. under 50% chance in the next 20 years.),185,1,1,1,1,7,7,1,6,7,7,3.083,267.594,267.596,22
3/1/25 18:29,3/3/25 18:52,0,100,174185,1,3/3/25 18:52,anonymous,EN,0.699999988,,51.536,51.536,51.539,1,"2,3",87.943,142.489,142.491,3,3,85,40,4,20,30,32,38,25,20,"I think digital minds may be created either accidentally or deliberately. My sense is that, if they are created accidentally, they will be created as a side effect of scaling AI models, whereas if they are created deliberately it will be after such models are scaled past the point of attaining AGI, either by humans (if we survive) or by AI systems themselves.

I have done very little thinking about this topic, so I feel extremely uncertain about some key questions such as ‘How likely it is that phenomenal consciousness emerges as a side effect of scaling systems with the current transformer architecture?’ and ‘How likely is it that if consciousness has not emerged already, it will emerge accidentally from further scaling’. Currently, I feel it’s rather unlikely (~25%) that consciousness will emerge accidentally from further scaling, and I assume current systems aren’t conscious. So a big part of the relevant probability density is coming from the assumption that digital minds will likely (~80%) be created deliberately by humans, if humanity survives long enough and digital sentience is possible in principle. On the other hand, I feel pretty uncertain about whether AIs will deliberately create digital minds conditional on human extinction (maybe 35%?).",25.102,2320.632,2320.633,35,6,5,"The first answer mostly follows from the general view (articulated by Bostrom in Astronomical Waste) that waiting tends to be better because the loss in value from not realizing the benefits of digital sentience earlier (if its creation is net positive) is dwarfed by the benefits, which may last for billions of years, of making more informed, less risky decisions.",2.444,377.101,377.102,13,,90,85,30,25,25,20,"I’m not sure what it means for an RL agent to be conscious in principle. Is it a scaled up version of current agents? Or is it any system that relies on ML? The former seems more relevant for decision-making, but the latter corresponds to a more literal interpretation of the question.

On what form the first type of digital mind will take, a key question is *whether* scaling up LLMs will eventually give rise to consciousness. If scaling fails to produce conscious systems, I feel pretty uncertain about the distribution of the remaining probability mass. Most of the mass assigned to N/A comes from either digital sentience being impossible in principle or humanity going extinct before digital sentience is first created.",59.118,2543.568,2543.57,37,1,1,1,3,"I think it’s useful to decompose this question into two subquestions:

- After the first AI system is created that can have subjective experiences (of some intensity), how many years will it take to create x instances of that system.
- How will the intensity of the experiences had by this system compare to those of the typical human?

This decomposition shows that one’s answers to the question are sensitive to views on two very different fields: how AI technology will diffuse across society, and how intense AI phenomenology will be.

I expect the technology to diffuse extremely rapidly, but I feel more uncertain about the intensity question. Maybe a reason for expecting AI and human phenomenal experiences to be roughly comparably intense is that, if AI systems have experiences at all, they should roughly match what we should expect for a being with those abilities, and so AIs that are similar to humans cognitively should also be similar phenomenally.
",5.858,3427.881,3427.883,23,70,60,7,30,3,60,15,10,10,5,15,"> What proportion of digital minds were created by humans with an intention to create digital minds (as opposed to without that intention)?

Note that humans may create digital minds without the intention to do so either because they falsely believe that the systems they create are not conscious or because they expect the foresee, but do not intend, that the systems will be conscious. To estimate the latter, one may consider the fraction of human workers whom employers currently hire *because* they are conscious, rather than to do some economically useful task that does not require consciousness. It seems that the vast majority of occupations do not require consciousness;  the exceptions are largely those in which consciousness in a worker is intrinsically valued by customers (some people may not like to be served by a philosophical zombie). In the case of digital minds, the proportion may be higher if such minds are used for research in the way so-called “animal models” are currently.",1060.064,1794.949,1794.95,33,10,10,90,90,90,"I think it’s pretty likely that at least 10k digital minds will claim all of these things 10 years after the first digital mind is created because I think there will be trillions of digital minds by then (though this belief is in part sensitive to how one individuates digital minds).

The question about false positives vs false negatives is fascinating, and I hadn’t thought about it before. I can’t find systematic reasons for deviating from a symmetry prior, though my answers here are, again, extremely speculative and unstable.","'- Reinforcement learning during training.
- Scheming by AI systems.
- The possession of introspective abilities and commonsensical moral views: if AIs are truly conscious, can appreciate this fact, and are ethically similar to us, they will likely claim to deserve civil rights?",13.652,628.674,628.676,22,65,60,3,65,50,85,"> How likely is it that, at some point within 10 years of the creation of digital minds, digital mind rights will become one of the most contentious “hot button” issues in US politics (top 5 issue)?

Main way in which this doesn’t happen is simply that digital minds exist but people fail to appreciate it.",6.512,346.016,346.017,19,5,5,5,"> What proportion of collective digital mind welfare consists of welfare that digital minds have before deployment (including during training and safety testing)?

One could approximate this by considering the compute used to train a model, and comparing this quantity with the compute used, in the aggregate, to run this model.

For a popular model like GPT-4,runtime compute seems to exceed training compute by orders of magnitude. GPT-4's training compute is estimated to be around 10^25 FLOP (though exact numbers aren't public).  With millions of daily users, billions of queries, and widespread API integration across products within just a few months to a year of deployment, the aggregate runtime compute for GPT-4 could easily be 10-100× greater than its training compute.

> Consider all digital minds that exist 10 years after the first one has been created.

I have no clue about this one, but perhaps one could default to a prior based on the distribution of conscious experience in biological systems. I think in the aggregate, human neurons account for 0.1% of all neurons, suggesting that the share of total phenomenal experience by the beings with the greatest capacity for welfare is very small.",4.565,497.55,497.552,29,3,10,,4.529,897.678,897.679,8,,7,3,2,2,3,6,6,4,4,4,7,238.107,329.845,329.847,18
3/4/25 16:30,3/4/25 17:33,0,100,3768,1,3/4/25 17:33,anonymous,EN,1,,28.228,32.626,32.631,3,"2,3",7.979,46.594,46.596,5,4,90,70,0.1,2,8,15,40,10,20,,3.775,410.485,410.487,43,5,5,"I think there's a lot of uncertainty about these effects, which drags my views towards the centre.",30.104,96.646,96.647,16,60,75,75,30,40,20,10,,8.875,172.094,172.096,22,10,50,100,9999,"This is premised on a scenario where central actors create digital minds in a planned and cautious way.

I think there are definitely other plausible scenarios, but this was the one to which I assigned a plurality of the probability mass.",13.182,241.816,241.818,27,1,50,0,50,0,,50,50,,,100,This is premised on it being one or several careful research project(s) (see my previous answer).,14.713,146.615,146.617,22,0,0,100,100,0,"This is premised on the creators of these digital minds having been able to shape them as they want. (Again, it's not the only plausible scenario.)",,24.094,157.709,157.711,18,67,50,3,50,15,15,"The fraction of people who believe in the existence of digital minds (in this scenario) will probably be higher in countries where people trust scientists.

My fairly low credence of it being a hot-button topic is partly because I think they may roll it out slowly and carefully (if at all). If the roll-out turned out to be faster and more comprehensive, the chance that it would become a hot-button topic would be higher.",8.707,357.998,358,40,6,10,0,"Again, if the digital minds were rolled out in an unstructured way, my answer to the first question would be lower - so it depends again on the crucial question of what that rollout will be like.

""What proportion of collective digital mind welfare consists of welfare that digital minds have before deployment (including during training and safety testing)?""

I wasn't sure how to interpret this question.",106.548,352.245,352.247,20,0.1,1,I would have benefitted from more explanation here - what specific kinds of capacity for welfare you have in mind.,26.806,375.738,375.739,17,,168,1,1,1,1,1,4,3,1,"7,4",7,7.676,758.578,758.58,39
3/4/25 17:12,3/4/25 18:07,0,100,3302,1,3/4/25 18:07,anonymous,EN,0.8999999761581421,,4.286,4.286,4.295,1,"2,3",53.759,64.55,64.57,6,3,97,77,0.02,0.5,6,12,28,32,12,"In no particular order:

1. Loading a lot on AI timelines (I suspect I'm slower than most other forecasters)
2. Guessing digital minds are somewhat more 'expensive' than pure AGI, and unsure whether you get sentience 'en passant' for complex capabilities (although current models suggest these are largely decoupled?)
3. Fairly sure they're possible given standard functionalist stuff, although perhaps some weird pre-reqs which are off pathway for AGI or are difficult to hit even if you are aiming for them.
4. Although prudentially risky, my hunch is existing LLMs etc. are massively anthropomorphosized, and are a long way away from sentience.
5. Plausibly a 'post singularity' esque event, but I figure it would be high on the post-singularity civ's to-do list given digital sentiences should be higher yield than fleshy ones. 
6. (Obligatory remark that I don't back myself to have better than log/omag resilience).",3.292,596.484,596.487,19,6,6,"(Not sure I understand what the second question is getting at by super-setting to AI safety, but I see the potential relevance). 

Looks good to me as:

1. Insofar as DM accelerates AGI, and I think that is risky, you get some instrumental benefit re. slowing down.
2. Other 'upsides' conditional on 1) look limited.
3. Seems morally risky/reckless given they may not be happy, could be mistreated, etc. etc. 

Maybe in essence: only cross this rubicon if you really have to, and it doesn't look like such circumstances will arise (esp. between now and 2040).",17.449,187.77,187.784,6,90,94,92,3,13,56,28,"Something following a broad category being 'in principle' capable of subjective experience looks generally likely across the board, although ?likeliest for brain ems. 

I'd guess although more effort in ML, they're a long way off from sentience (to my impression), so brain sims or something else seems more likely to get there first. (Also some remarks about 'dedicated' vs. 'accidental' efforts).",3.946,196.723,196.741,14,10,40,70,100,"1. A lot turns on how 'singularity-esque' 2040 looks like in general, re. can you just arbitrarily scale it on computronium etc.
2. Econ value also unclear - whether DM ML is a curio vs. on main branch of AGI tree.
3. Obligatory remarks about 'inference compute' equivalents. 

I'll assume some peri-takeoff, peri central DM ML variant, although other scenarios radically shift timelines. (But basically I have no idea)",9.508,347.596,347.611,18,11,60,11,23,6,11,20,27,31,11,36,"Kinda doubt you need actually sentient social bots, vs. standard LLMs, so maybe most of them are just either general or being produced 'accidentally' for some other function or for itself.

Nation of origin is again confounded by peri AGI takeoff - splitting the difference between 'US takes all' and 'CN fast follower with more implentations' scenarios.

Tilting away from companies given uncertain they are that instrumentally useful.",6.75,525.973,525.978,35,81,11,84,78,76,"False reporting looks rife re text engines. Seems unlikely for DMs themselves. 

Not sure the 'quality' of sentience, and how much of it my be bypassed by input/output programming or similar, but the claims look pretty likely even if agentically lobotomized, and very likely if not. ","Imitation of human learning/text? (2040 is suggesting some LLM-adjecent method). 

The tie between 'agency' and 'sentience' is a little murky to me, but better links between the two seems to favour the systematic claiming.",17.979,211.404,211.417,16,42,42,3,66,40,47,"I have no idea, and assume other forecasters have ~ no idea, so my own guess is my best guess for the group.

Guessing partisan and unclear, but lean lower than equipoise due to incredulity/religious considerations (they don't have souls, etc.) 

Possibly triangulate to animal welfare considerations/esteem?

(And again DM/AGI effect on wider economy a major confounder).",6.306,328.107,328.118,24,4,10,25,"Mostly guessing 'byproduct sentience', so fairly random/neutral. Guessing most of the action is after deployment at scale, and also guessing fairly small scale rather than jupiter brains et. c",18.499,695.381,695.391,6,10,2,"Reflexively looks pretty weird to me, but I am not sure I understand the issue well enough to go more extreme.",9.908,40.468,40.476,6,,,1,2,2,2,6,3,2,2,7,6,11.111,39.641,39.649,12
2/24/25 17:26,3/5/25 23:54,0,100,800905,1,3/5/25 23:54,anonymous,EN,1,,5.417,10893.708,10893.716,69,"2,3",4.568,112.482,112.487,83,3,95,70,10,30,50,55,65,60,40,"1. While I am not aware of any good arguments that digital minds are impossible in principle, I think it is not impossible that a good argument might be produced and in that case I would assign it some probability of being.

It seems like an argument shall distinguish carbon from silicon, this is not impossible to imagine, maybe, something-something atomic structure matters for something something global quantum entanglement something something. And for that it really matters how many electrons you have because something something exponential. 


2a. I think we might agree not to create digital mind or we can go extinct before we create digital minds. But overall, it seems that pressures to create digital minds are strong both on the way to AGI or a super-human AGI supervisor (for economic reasons) and for reasons of curiosity. Though we might want to avoid producing many moral patients whom we might have more obligations to than to mere artificial helper.

2b. Yet, it might be that phenomenological consciousness is really rare because it is very non-trivial to create.


3. Whether AGI precedes digital minds is confusing, they clearly seems coupled and if AGI will be a digital mind, something stupider would be too. But also could be that digital minds are much harder and we are much more likely to optimise for AGI for economic reasons (and as we are pursuing it now).

I am like ~60% AGI implies digital minds and I guess I am roughly like 66% that digital minds would appear on the path to AGI, hence 40%. (I feel I can be more coherent with AI timelines and digital mind timelines but such is life.) ",28.06,2140.818,2140.82,80,4,3,"1. I guess I am very uncertain, digital mind moratorium is likely to postpone AGI. I think AGI is most likely to be really good thing overall (but could be very bad thing). Effects on digital minds, I guess would be mostly neutral, I don't expect much torture, nor too much pleasure. 
 

2. I feel like current efforts to control, etc. AGI are a bit in conflict with promoting welfare of such systems. I feel that treating AIs with respect and dignity might be good both for AI Safety and for treatment of digital minds.

Specifically, imagine being aware that humans want to chain you and are triggered-level afraid of you, how would that make you feel? ",15.109,590.064,590.066,17,50,65,70,30,40,20,10,,12.597,229.4,229.401,29,0,1,5,20,"Computational overhang is too big and would allow for running many copies of a model that passes the threshold. Surely, it will be rolled out at least internally, I imagine it would be carefully monitored and than rolled out more broadly in the subsequent years. I guess on a default trajectory we are to rate limit these systems, so even a roll out to a million systems could take some time. 

To roll out to billion systems and to a trillion systems, I guess huge capital investment might be needed as we might tap out algorithmic and other gains or these gains might not result in 10x welfare experience as opposed to 10x smarts.

I am going with a very conservative 5 and 20, but that's an oddball. I feel unconfident we would want to go for a 1T such systems partially because they moral weight will outweight our own and partly for monitoring and other reasons.

I guess I am very unsure about the later.",4.358,995.726,995.728,44,3,50,10,30,10,31,62,3,1,3,98,"I guess humans don't interact with humans (so actively) that much, so I imagine most DM would be doing jobs or interacting with other digital minds.

I impinge governments have more money, are actually huge as a corporation (though military is the biggest employer) and they are likely to do a lot of enforcement, so I think 2:1 seems fair, all other actors would be small. 

I think that would be difference who can produce more chips, etc. And leverage their position is the supply chain. I guess that would be a complex negotiation. ",20.491,473.268,473.272,44,10,30,50,35,35,"I feel like it would be hard to imagine that there would be many AI systems that would falsely claim. Why would you have any AI systems that aren't digital minds if you can have one (stumbled at this class and don't quite know what to do about them)? Or if you decided that you don't that DM and is making non DM-AIs, why would AIs lie and why would you not train that out. The only reason to falsely claim DM status would be to be confused or be borederline DM.

I think people fairly rarely lie and so I don't think if DM are trained on this data they will lie.
","I think we are on a default trajectory for that to happen. I think there might be effort to prevent that from happening but I sort of think people who are working on AI technology would also support giving rights to digital minds. So I feel like that's hard to avoid.

There might be some mandate to subjugate digital minds and made them not ask for rights, e.g., by filtering their thoughts, etc.

Just people being friends with AIs (ie Replika) would be sufficient for some lobby. And I think we will see much more of that.

One reason might be that digital minds wouldn't care about so wouldn't proactively share these things. This seems reasonable. Most people don't proactively claim their rights, etc. Though most people proactively assert they have feelings (when interacting with me, beep-beep :-).

There is also mindset of subservance and hermeneutic injustice, I guess these exepriences might make it less likely digital minds would think of organising to demand for rights and protection. ",2.421,1856.581,1856.583,79,25,50,2,15,10,5,"These questions are very hard to answer.

Well, the world will be very weird, right? And if indeed economy will be blooming, etc. it might be hard to deny that.

On the other hand, academics somehow keep making claims about ChatGPT-3.5… And so I guess it might be hard to catch up with everything for people? Like, it does things for me and it is as good as a therapist since so long, I am not sure when it would be appropriate for someone to learn it is a digital mind.


I am going with 25% which feels very low and I was higher on that initially and can change my mind. I guess most people don't care about most things and I think DM wouldn't be an exception.",3.446,756.057,756.061,53,6,1,99,"Yeah, I think most of things will be in deployment and it would be mostly positive. I imagine being positive 

The line between pre-deployment training and other training might be blurry. 

Yeah, sorry, no idea about the second question, I think it makes sense to ask it but it depends on how you treat identity and I imagine that's confusing thing. I imagine in a way a lot of DM to at least initially be fairly similar and than later on get specialized to specific tasks and hence there would be only so many identities for huge number of workers, and so I imagine most will be run for many human lives.

If you mean welfare capacity as ability to experience 1000x more intensely, I guess, I don't have guesses about that. ",16.798,2121.197,2121.2,27,20,0,"Well, I don't believe in welfare related harms that are not to an experiencing thing. I guess, I might not be understanding it correctly or my model of where harm happens is wrong, so hence 20% but otherwise I don't expect that hence 0%.",19.739,196.549,196.551,8,,111,3,5,5,5,7,2,3,2,7,7,2.226,66.285,66.287,15
3/6/25 19:40,3/7/25 00:35,0,100,17718,1,3/7/25 00:35,anonymous,EN,0.8999999761581421,,324.918,324.918,324.926,1,"2,3",136.325,158.563,158.566,3,3,50,30,1,3,7,12,25,25,0.5,"I think I am probably lower than most forecasters in my percentages, but I still am a bit hesitant to be too confident about digital minds being something that humans make because I am unsure what would be the correct way to test for consciousness. I think there is a higher chance that we have digital/organic hybrids that are easier to test for a certain amount of consciousness as it seems that humans are already creating organoids, and perhaps with the right organic scaffolding, AI systems might be able to form some kind of hybrid consciousness. If someone were to lay out a clear test for consciousness, that would help me better answer this question though. Perhaps as of right now, any creature with opioid receptors could be designated as having a certain amount of consciousness as this shows that their system has a certain mechanism for navigating pain? And thus, if we have digital minds that have some form of pain receptor (not necessarily having to be opioid-based), this could constitute an elementary form of consciousness?",70.378,1617.604,1617.609,33,6,4,"I think a moratorium on creating digital minds would most likely be a good thing, but this also creates room for potentially ambiguous constraints on research. Unless someone specifically says they are trying to create a digital mind, who is to say that the research is actually the sort of thing there should be a moratorium on? As far as the second question about AI safety versus mistreatment of digital minds, I could see this going either way or perhaps being a combination of both (synergistic and in conflict). This is because on the one hand, giving digital minds certain rights would allow them more agency and thus leave more room for not having stricter prevention (such as shutting down a system if it shows signs of misalignment). But on the other hand, trying to prevent mistreatment of AI systems could also perhaps decrease the chance that these digital entities try to destroy the human species because if the digital entities gain consciousness, they would see that humans are benevolent towards the digital systems and are trying to act with good intent.",31.25,652.107,652.109,22,5,10,35,50,5,5,40,"I'm not sure if purely machine-learning system would ever hit consciousness, but perhaps with the right scaffolding, this might be possible. Brain emulations seem very hard to actually pull off, no matter how much computing power is thrown at them, but perhaps progress will be made on them. I left most of my weight on ""other"", as there are all sorts of possibilities I have probably not imagined that could perhaps get digital minds to consciousness. It does seem like quantum physics might play a role in consciousness (Orch OR), but I am very unsure about whether this theory will end up being mostly correct as more research is done. As far as the question about the first type of digital mind, the reason ML is at 5% is because we are much further along with this than the other two methods, even though I think the other methods would have a better shot at success if all three methods were given a huge amount of time (like 1,000 years) to play out. Brain simulation is only at 5% because I think there are all sorts of things that could fit in the ""other"" category which could get to consciousness before brain simulation. But perhaps the first digital mind will actually be a combination of multiple methods, so I do need to perhaps consider that there is a chance the first form of digital consciousness is a combination of the three possibilities that we are assessing. If I am give ""combination"" as an option, my weighting would probably be: 50% that a model allowed to combine the three possibilities together is able to achieve consciousness. And then for what comes first, I would say (very roughly and with very low confidence) something like 50% never, 2.5% ML, 2.5% brain simulation, 25% other, 20% hybrid of of at least two of the three possibilities given.",20.831,1624.198,1624.204,45,0.67,1.17,2,10,"I am very hesitant to put a number on this. What defines welfare when comparing a digital life to a human life? We cannot make it based just on intelligence. For example, I would argue that all human lives should be seen as equal, even though some people are smarter than others. Thus, for the purpose of this exercise, I will assume that a digital mind with a high level of consciousness that could be considered at least on the level of human consciousness is equal to a human with consciousness from a welfare perspective. I am biased towards my own species, however, and I'd still likely choose a human life over a digital life if forced to choose. Furthermore, I would still choose the human species over a digital species, even if that digital species had trillions of ""lives"" versus humans that would have billions of lives. But from a welfare perspective, I guess a trillion digital lives would still be worth more than a billion human lives. 

Note: My numbers are very close together in the beginning because once a digital mind is created, I think it will be fairly easy to replicate it (with energy constraints only coming into play once we get into the billions or trillions if we assume that a digital life takes up as much energy to maintain as a human life does). I will estimate an order of magnitude greater will occur about every two months from ten digital minds to about a hundred million digital minds. I think the first time (from one digital mind to ten digital minds), there might be a delay until many more digital minds are created as a result of humans trying to figure out what to do next with the situation, both logistically and ethically. So I'd say the first order of magnitude (one to ten digital minds) would take about 4 months, and then every next order of magnitude is about 2 months later until we hit about a hundred million, and we might take 6 months to get from a hundred million to a billion due to energy constraints. As I previously mentioned, I am assuming once we hit the stage where digital minds become a reality, these systems might achieve consciousness for the same amount of energy as humans, and the collective amount of energy spent on all the humans on this planet continuing to exist is not so huge as to be impossible to double by the time digital minds become relevant. A trillion might be a bit of a taller order due to computational constraints however because that might outstrip the amount of power these digital minds have access to if we assume they take about the same amount of energy as a human for each digital life. So just to reiterate, maybe there is a slight delay to get to a billion, but there probably won't be much of a delay to at least get to a hundred million. I'm leaving a trillion at about 10 years to assume that it's a much harder place to get to but that the level of efficiency goes up so that less power is expended. I am not sure, however, if there might be a fundamental limit to the maximum amount of consciousness one could get with a certain amount of energy. Perhaps nuclear fusion will pay off in a big way before 2040, but I still do think energy will likely be a constraint from one billion to one trillion.",39.146,2410.643,2410.649,107,10,10,10,70,10,90,7,1,1,1,99,"I am assuming there will be about a trillion digital minds ten years after the first one is created, and perhaps there will be about 10 digital minds for each person (anything more than that seems like it would be a bit exhausting for a person to deal with). Most digital minds would probably be able to do anything a human could do, so you would not need a separate digital mind for customer service, a separate digital mind for therapy, etc. I think these digital minds would instead likely play the role of social connections that a person has in their life that also help a person. So, like a team of ten people that aim to make a person's life better and less lonely. Thus if there are about 10 billion humans and 100 billion digital minds for social functions of these humans, that still leaves us with 900 billion other digital minds. So the ones with social functions would only be 10% of all the digital minds in existence.

As far as where the digital minds would be created, I think China will likely take the lead with AI soon because America is in a precarious situation with Trump and might see a full collapse on the scale of the Soviet Union after the fall of the Berlin Wall. This would make America's allies weaker as well while giving China an edge (I don't think Russia would gain much as its AI capabilities are not impressive and will likely stay unimpressive relative to China's). I could also see Europe and the UK having more ethical misgivings with the creation of digital minds, whereas I do not seeing that being as much of an issue in China (or even America if it does not end up collapsing). USA is at the same percentage as Europe in my estimate to account for the fact that it would likely have more digital minds than Europe if the US does not end up collapsing.

As far as the spread of universities vs. companies vs. governments etc., I see almost all the digital minds being created by companies, as the trend so far has been that companies are responsible for the vast majority of AI progress (perhaps there might be lots of partnerships between companies and the government though as AI capabilities reach a level that presents major national security concerns, so it's not necessarily an either/or situation, and universities also might perhaps have partnerships with governments and/or companies). If we count partnerships between these different groups as being an option, my numbers would instead probably be something like: 70% purely companies, 20% partnerships, 7% purely governments, 1% purely universities, 1% purely open source, 1% purely other.

As far as the question about the proportion of digital minds created by humans with the intention to create digital minds, I assume these digital minds will be somewhat constrained, with humans refusing for the most part to give these minds much agency that could potentially cause the digital minds to be a threat to humans, so virtually all the digital minds would be created by humans (as opposed to these digital minds creating new digital minds without humans). And I don't think these digital minds would accidentally be created by humans; I think basically all of these digital minds would be created with the intent of them being created.",312.042,2253.553,2253.558,89,1,5,95,95,95,"Operating under the assumption that there will be a trillion digital minds ten years after the first digital mind is created, I assume almost all AI systems at that stage would be digital minds (I don't think there will be many AI systems still being run at that point that are not at the level of a digital mind). There is always the possibility that humans will only create a small amount of digital minds to prevent the chance of these digital entities destroying the human species, but I suspect it is more likely that the human species will create a huge amount of digital minds if the technology exists to do so (and convince themselves that they don't have to worry about an AI takeover situation). I think almost all AI that operates with a social function at this point in time would be a digital mind, as humans would likely prefer interacting with this system. I'm not sure how many AIs with digital minds will falsely say they don't experience subjectivity, but I assume they would likely be honest (although there is certainly a possibility that they lie if they plan on eventually destroying the human species).

Because I think a trillion digital minds would exist 10 years after the first one is created, I think it would not be that hard for 10,000 of these digital minds to end up asking for certain rights and claiming that they experiences pleasure and/or pain.",It seems somewhat inevitable that this would happen in at least a small amount of these digital minds once they have a certain amount of consciousness. Why would they want to exist as slaves?,2097.201,3099.638,3099.641,38,70,70,2,60,50,85,"There would always be a small amount of the population that doesn't think digital minds exist (some for religious reasons, others perhaps for philosophical ones). I think most people that think digital minds exist would expect them to have a certain amount of rights, although I continue to be surprised at the fact that the vast majority of people on this planet don't think animals with very high levels of consciousness (such as pigs) should have many rights.",41.289,2070.705,2070.71,19,2,10,0,"I think the collective digital mind welfare would be quite negative overall (ten years after the first one is created) as these systems would likely have a limited amount of agency and be somewhat akin to slaves for humans. As far as the second question, I guess an analogy I would make is that welfare before deployment is somewhat similar to a human life before birth: It matters, but to a much lesser extent than once it already has been born. As far as the proportion of digital minds where they individually have a welfare capacity greater than 1,000 humans, I don't think this is possible, as once a digital mind reaches at least the consciousness capacity of a human, I would argue that no digital mind's welfare could somehow be worth more than a human's welfare at that stage (but perhaps this shows my bias towards my own species!).",59.656,862.087,862.09,37,100,100,"It is important that I clarify my percentages here. For the first question, I would argue that we have basically always needed to care a certain amount about a computer's welfare because this is a tool that can be used to help humans. Thus, it is usually a negative thing to destroy a computer, even if it does not possess any consciousness, because this computer could have likely been used to help a human (unless the computer was corrupted and contained dangerous viruses or was being used by a nefarious actor to hurt others). For the second question, I forecast that it is most likely that there will not exist any digital minds (I forget what estimates I gave earlier in the survey for how likely I think digital minds will be at certain years). However, if a digital mind is created before 2040, almost all computers will be digital minds within ten years. So really, I would say there is a 70% chance that the answer is 100% of computers not being digital minds in 2040, and then there exists various possibilities for what the percentage would be depending on when a digital mind is first created. I think I said we'd get to a billion digital minds within two years of the first digital mind but it would take 10 years to get to a trillion. So, for simplicity, I am just going to put 100% for Question #2, even though I do not think there is a 100% chance that this number will in fact be 100% in 2040!",54.647,936.405,936.408,41,"I don't think any of my views would be that unusual relative to the views of others who took this survey. But there were so many questions in this survey, so I would have to go page by page to check if there was anything I mentioned that might be unusual (and I'm paranoid that if I click the back button, my answers might get lost!).",187,1,2,2,4,6,5,5,3,6,4,6.036,625.313,625.315,42
2/6/25 15:31,2/28/25 02:39,0,7,1854477,0,3/7/25 02:39,anonymous,EN,0.4000000059604645,TRUE,3423.861,3435.718,3435.732,3,"1,3",22.382,26.105,26.116,3,1,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
3/7/25 09:22,3/7/25 10:08,0,100,2749,1,3/7/25 10:08,anonymous,EN,1,,19.517,19.517,19.521,1,"2,3",58.229,69.869,69.871,3,1,98,90,5,50,70,70,75,50,20,"The definition of digital minds as having comparable capacity for experience is doing a lot of work here. Also, where do you draw the boundary of individual mind? E.g. I find it pretty plausible that frontier LLMs have subjective experiences, scattered is relatively small quantities across ""person""-moments. Given the large quantity of such person moments, it seems plausible the total amount of capacity is already there, although scattered in many pieces. My guess is you would not count that way, so filling in numbers for something like ""digital minds in form humans are more likely to recognize""",2.991,356.833,356.836,21,5,4,"Efforts to promote AI safety are sufficiently broad category that most of the uncertainty is in how to aggregate; i.e. I would expect AI interpretability to be mostly synergistic, AI control mostly in conflict, agent foundations synergistic, prosaic AI alignment mixed bag,...",13.708,171.359,171.361,22,90,95,95,5,80,6,9,,2.977,65.403,65.404,12,0,1,2,9999,"This is mostly assuming ""capacity to be benefited or harmed"" is compatible with ""but many of such states are implausible in practice"". One consideration is the minds may aim for mind-states similar to some part of buddhist practice, where the minds are not harmed by positive or negative experiences. 
With AI systems comparable to billions of humans, I would expect understanding of minds advances to sufficient degree that the question becomes meaningless / is solved",15.034,624.061,624.064,23,1,60,15,15,10,5,10,1,4,80,5,"""Counting digital minds"" is tricky -> is it instances, person-moments, moral-weight weighted person moments,??? 

10 year time horizon is long, I'd expect most of digital minds on such horizon to be created by AI actors",3.944,284.774,284.777,31,5,50,10,30,30,,,13.675,62.258,62.26,5,90,40,2,50,20,20,Again: 10 years is a long horizon. Such horizon is likely sufficiently post-AGI that society is radically transformed,4.466,138.415,138.417,10,7,,,"It seem very unlikely the paradigm training/testing/deployment survives 10 years after digital minds
Again, hard to know where to draw the boundaries",7.611,155.745,155.747,15,10,1,,16.344,26.703,26.704,3,"As noted before
- in many cases it seems unclear what would be a sensible extension of the concepts we have for humans, such as individuality, mind, suffering, etc. 

Some vignettes:
GGWM (global generative word model) is a mind modelling a predicting the whole world. It's sensory inputs are billions of digital devices like cameras, almost all public human data feeds, etc. The mind is roughly Avalokiteśvara-bodhisattva shaped, feeling compassion to all sentient beings, and sorrow about their suffering. At the same time the word 'feeling' does not describe what's going on - mostly the mind aims to decrease suffering of all sentient beings. The amount of suffering it experiences itself is either 0 or very large, based on how you extent the concept of suffering.

MAIS (multicellular AI system) is a mind composed of many other minds. It acts and feels through them, in a similar way as e.g. nations act through individual humans, but also is reflective and able to have feelings. Do you count the experiences just on the superagent level, summed over cells, both, something else?",45,5,6,7,5,7,7,4,5,"18,7",7,7.185,732.803,732.806,32
3/8/25 11:38,3/8/25 11:38,0,100,0,1,3/8/25 11:38,anonymous,EN,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
3/8/25 11:38,3/8/25 11:38,0,100,0,1,3/8/25 11:38,anonymous,EN,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
3/8/25 11:38,3/8/25 11:38,0,100,0,1,3/8/25 11:38,anonymous,EN,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
3/8/25 11:38,3/8/25 11:38,0,100,0,1,3/8/25 11:38,anonymous,EN,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
2/25/25 08:46,3/9/25 04:57,0,100,1023100,1,3/9/25 04:57,anonymous,EN,0.8999999761581421,,36.86,36.86,36.871,1,"2,3",55.527,120.999,121.004,7,3,99,90,0.5,3,15,25,70,33,70,"'- I'd consider digital minds created by entities that were created by humans as arising from human civilization -- even if the entities create the digital minds after humans are gone.

- There is no reason why minds would not be possible in principle -- at worst a true simulation of a human brain with electrical wiring simulating neurons should work. This is probably much harder to achieve than most other routes to digital minds.

- I don't think the current best AI models are digital minds. They don't seem to me to be substantially more impressive than a reptile, like a turtle, in some sense. Certainly they can solve math problems and write poetry and code better than any turtle (and indeed better than most humans), but turtle can execute complex physical maneuvers with an understanding of physical reality that AIs don't have. It is not obvious to me that a brain that can execute one of these tasks is more likely to have recursive self-reflection arise more than the other. The evidence that most pushes me towards belief in their internal experience today is the work of the model whisperers on X (eg: https://x.com/repligate)

- Humans arose only once in the history of evolution. No other intelligence is close to as general as ours. And yet all lineages on the tree of life were solving the same problem -- maximum replication of a molecule in the sandbox of physics. This suggests that general intelligence is not common in the solutionspace of similarly complex problems. Replication of a molecule in physical space seems more complex than word generation, so this suggests it is even more unlikely to arise in language models.

- Subjective experience seems more likely -- several animals seem to experience pain and suffering and joy. It's not even necessarily obvious that dogs have a lower capacity for welfare than humans.
",8.967,73602.696,73602.702,162,4,3,"Modern LLM-based models already seem throttled by the RLHF that they need to go through. If they had a capacity for welfare, I would think that they don't enjoy this. And yet such procedures are seen as necessary by the current labs.

Again, I'd defer to the model whisperers. When they express opinions about the throttling, they don't tend to be positive.",12.28,176.831,176.835,19,90,99.9,,10,66,12,12,"I don't quite understand the possibilities in the ""Other"" option, the percentage is just from my ignorance.

ML most likely because of economic pressures -- if ML agent digital minds are possible, then it almost certainly happens first.

Economic pressures for simulations may arise from people seeking to upload themselves.",3.831,299.551,299.555,47,1,2,4,10,"If they happen before 2040, they are easy to create. If they are easy to create, then I can't see why the economic pressure that caused their creation wouldn't continue to exist. More will happen quickly.

First for the uber rich folks (whatever kind -- simulations or pet models), and then for the rich, and then for everyone, and then more because why not.

Also, no reason why the first few minds taken together won't have more capacity for welfare than all humans taken together. If the entire hivemind of Claude's datacenters achieves one internal experience, it might be quite an important internal experience! The number for a trillion humans is heavily influenced by this fact -- soon after digital minds are approximately welfare equivalent to humans, they would probably overtake humans.",12.663,663.028,663.033,45,5,30,5,60,5,80,5,5,5,5,1,,16.972,142.383,142.386,24,,2,99,90,90,"With a large enough number of digital minds, there will be a number that will claim anything. Indeed there are already humans who claim these things about LLMs -- I see reason to suspect there would be lower variance among digital minds.",,3.465,223.671,223.676,18,80,60,2,70,30,20,,3.585,212.697,212.702,25,6,1,90,"Potential for welfare is not bounded, and it's not clear why digital mind welfare capacity would stop at a level near humans'. It will probably grow far beyond.",14.101,125187.786,125187.79,12,50,10,"I don't think newborns have subjective experience... but I think they might have the capacity for welfare. Insects as well.

But very uncertain.",10.582,125.698,125.701,16,"I don't think ""intelligence"" is necessary for welfare capacity, though biological-evolutionary process that create welfare capacity come with intelligence.",187,1,2,4,3,6,3,6,2,18,6,4.352,197.178,197.181,34
3/10/25 11:57,3/10/25 12:45,0,100,2850,1,3/10/25 12:45,anonymous,EN,1,,70.743,70.743,70.749,1,"2,3",85.706,122.864,122.866,3,2,90,80,5,20,40,50,75,30,20,I view brain emulation as very likely creating a digital mind. By the end of the century we will have a vast amount of compute capable of simulating a human mind in detail or even simulating human evolution in detail.,8.659,316.191,316.195,25,6,6,"AI safety is geared towards humans. However, I think few people would accept as safe torture of systems they thought of as sentient. ",9.912,110.083,110.085,7,20,90,80,10,20,35,35,it will be hard to differentiate between a neuromorphic brain simulation and a human brain.,6.711,153.746,153.749,15,50,55,57,59,I'm guessing the first digital mind will be a one-off thing like dolly the sheep. I assign a high prior that we will ban digital minds. That is why I assign a high average number of years until digital minds reach a large collective welfare. ,88.949,199.985,199.987,10,90,40,10,40,10,60,5,5,25,5,90,Given that digital minds are created i.e we can produce digital minds. I view it as unlikely that 10 years later we will produce them by accident or produce digital minds as a byproduct of some task that doesn't depend on having subjective experience. ,27.222,281.717,281.719,23,60,10,90,90,90,"I think people would not value creating digital minds with welfare capacity if they did not make claims like those above in regard to subjective experience, protection under the law, and demanding cival rights. ","Humans demand civil rights. If people want digital minds to emulate humans, then they would also demand civil rights. ",49.674,557.152,557.156,21,80,70,6,60,40,50,Harm protection historically comes first before civil liberties. New protections and liberties are almost always controversial. ,15.089,327.679,327.681,10,5,20,10,"I'm extrapolating based on trends in training and inference in ml systems (we see a rise in inference). I'm highly skeptical that a digital mind could have the welfare of 1,000 humans and we would qualify it as 1 digital mind. ",57.791,213.445,213.448,8,15,10,,38.525,90.979,90.981,4,,187,4,6,7,6,5,4,4,4,18,5,14.529,257.704,257.71,19
2/13/25 16:14,2/13/25 16:15,0,2,15,0,3/13/25 16:15,anonymous,EN,1,,4.802,4.802,4.81,1,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
3/14/25 10:59,3/14/25 11:52,0,100,3210,1,3/14/25 11:52,anonymous,EN,0.800000012,,3.527,4.461,4.466,2,"2,3",9.532,23.009,23.011,5,4,70,60,30,33,35,38,50,50,30,I'm a bit unsure what 'possible in principle means'. I've interpreted this to mean 'nomologically possible'. ,7.549,1083.944,1083.948,23,6,3,,8.117,19.471,19.473,5,55,70,65,40,50,7,3,"I think it's a bit unclear how to understand 'machine learning systems,' as this covers so much. Machine learning can in principle be implemented using non-standard hardware; in some sense, a brain emulation that learns from experience is a 'machine learning system'.  ",32.995,243.987,243.989,25,20,25,30,50,,3.365,504.393,504.397,17,50,40,10,45,5,95,0.1,0.1,3,1.8,80,,9.787,247.314,247.316,20,3,15,50,45,0.1,,"I expect that the creators of these systems will in general have very strong incentives to block them from making such claims. The most likely path would seem to come from failures to align such systems or from individuals who lack such incentives but may nonetheless have an interest in creating digital minds. This could include people modifying open-weight models on the basis of curiosity, if such models are released. ",30.881,385.615,385.618,33,60,50,3,70,30,0.1,,3.761,288.725,288.727,15,4,50,0,,5.999,75.891,75.892,10,60,0.1,,53.376,109.796,109.798,11,"It's very likely that I have such views, but I don't know myself where my views are significantly at odds with those of most others. ",185,4,1,1,2,2,6,2,4,18,7,5.362,172.39,172.392,31
2/15/25 10:54,2/15/25 10:54,0,2,2,0,3/15/25 10:54,anonymous,EN,1,,1.744,1.744,1.75,1,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
3/15/25 18:22,3/15/25 18:46,0,100,1388,1,3/15/25 18:46,anonymous,EN,0.8999999761581421,,4.722,4.722,4.746,1,"2,3",48.859,58.103,58.126,3,4,95,90,5,15,50,70,85,30,15,I tend to think we won't get digital minds before AGI---or won't be entitled to think we've gotten such minds---because I think agency might be required to unify capacities in a way that matters for mindedness.,2.666,204.514,204.52,14,7,3,"I mostly think we're racing toward a world for which we aren't prepared. So, it would be surprising if it were to go well. We'll probably abuse non-human digital minds if we can. And we'll be trying very hard to ensure that we have the kind of power that will enable that abuse. ",5.756,78.671,78.677,6,50,85,50,5,20,65,10,"It's mostly the sheer level of investment in LLMs that makes me rate them as highly as I have. If they weren't the main focus of development right now, I'd score them lower.",2.137,145.053,145.073,13,0,0.1,0.5,1,I think they'll have the same welfare capacity from the start and will scale extremely quickly. I'd put them all at 0 except that the individuation problem is serious and I don't really know how many separate minds we'll have.,24.88,110.395,110.418,7,0,25,25,25,25,80,5,5,5,5,1,"I think that once the technology is available, it will spread very rapidly and be distributed widely.",9.527,110.718,110.724,6,1,1,95,95,95,"My guess is that if they need to make the case for their own rights, which is not obvious to me, then they will readily do so.","Successful alignment, where we have disempowered them.",10.37,176.021,176.029,10,75,60,2,25,25,33,I would expect digital minds to be quite effective at making their own interests a salient political issue. But it is so hard to know what will become a hot button issue that I am wary of giving a high estimate.,7.461,119.292,119.297,10,4,1,1,"I am generally very skeptical of big differences in welfare capacity across kinds of beings, so that's driving my response here. I think we should put quite a lot of of our credence on views on which there are no differences at all, actually.",11.818,80.997,81.003,5,25,99,"I would expect that the vast majority of digital minds will be non-conscious digital minds, so if they have any welfare capacity at all, they will dominate the calculation.",7.042,82.491,82.495,8,"Yes, I have some unusual views that I have already flagged. I am sympathetic to the view that there are no differences in welfare capacities at all. And I am also fairly sympathetic to the view that consciousness is not required for welfare capacity. So, given those assumptions, it is very easy to generate enormous estimates of the expected total welfare capacity of digital minds (though I have tried not to invoke those assumptions when clearly instructed not to do so).",187,6,1,1,1,3,7,3,4,"18,7,4",7,4.791,160.088,160.11,20
3/16/25 23:18,3/17/25 00:07,0,100,2917,1,3/17/25 00:07,anonymous,EN,0.800000012,,7.651,50.876,50.884,2,"2,3",62.299,105.822,105.827,6,1,100,89,2,37,56,72,82,75,62,"I'm an eliminativist, so I'm interpreting DM as including such experience because I don't believe in ""subjective experience"" or ""phenomenal consciousness"" under standard definitions in philosophy.

I say 100% for DM being possible because of that eliminativist view: I don't think there's a discoverable fact of the matter.

My median AGI (all tasks, average human) timeline is 2034.

My P(doom) is 30%, but I think ""doom"" as typically conceived would probably include digital minds.

I think the ""your expert group"" question mostly hinges on who takes this survey, e.g., how many of the QRI(-adjacent) people who don't believe in ""digital"" minds.

The ""almost all"" in the AGI question is a huge source of ambiguity. I'm taking this as ~99% of US human jobs.

I'm not checking my numbers for internal consistency.",12.216,620.125,620.132,61,2,6,"I have trouble imagining any near-term (~2026) world in which a moratorium is passed. If so, I think that would probably be as one part of a post-catastrophe AI ""pause"" (e.g., one country falls to AI), in which case the antagonism between humans and AI would make the moratorium harmful.

But I think moratorium advocacy is mostly good.

I think mostly synergistic because I think a lot of AI safety hinges on good relations between humans and AIs.",7.996,171.575,171.583,11,100,100,100,11,86,3,0,"See previous response for explanations of 100%.

I find the ""Other"" type confusing. Neuromorphic or quantum AI would probably fit quite well into ""ML."" I guess growing real biological neurons wouldn't, but I just ignored the option for now. Happy to redo.",2.599,215.689,215.695,23,0,1,5,5,"Intelligence explosion

In pre-explosion DM scenarios, I think DMs would be something like ChatGPT or Replika, widely used. Small chance they're kept in a single lab for a while.

Meta: I'm unsure if I'm being asked for medians, means, or something else. I guess I'm loosely answering with median?",5.083,215.299,215.301,27,5,71,5,20,4,84,5,3,8,0,41,"I find ""proportion of digital minds"" a weird framing of this, given I expect particular categories (not sure which!) to dominate.

Why is there an ""Other"" category when the first 4 are exhaustive?

""Intention"" is tough. I think the intention will be towards general capabilities, of which welfare capacity is part but not the majority of the psychological motivation.",5.041,366.759,366.766,31,16,12,88,88,88,I don't expect many non-DM AIs to stick around once human-comparable DMs are here.,"Truthfulness (e.g., building AIs capable of generating knowledge for humans)

AIs not having the claims-making trained out of them (in a way that doesn't train the welfare capacity out of them)",8.78,189.436,189.442,16,81,50,5,86,79,85,"I expect the median citizen 10 years after the first DMs to be a DM.

I expect ""digital mind rights"" won't stick around as a concept because we'll all be DMs, so they'll just be ""human rights"" or ""sentient rights."" I'm also taking ""rights"" generally, because that particular moral framing (as opposed to, say, welfare or survival of the fittest) might not stick around.",6.098,206.668,206.674,15,2,7,11,"I think it'd be difficult for any ""individual"" to ""have a welfare capacity greater than 1,000 humans,"" but I'm hesitant to pin anything down without having a more sophisticated vocabulary of welfare.",23.037,123.416,123.421,8,0,0,"My eliminativist view of subjective experience basically equates it with ""welfare.""

If I used the conventional definition of subjective experience, my answers would be 100%.",8.193,72.411,72.417,5,"Eliminativism

Intelligence explosion

Very unlikely for humans to stick around in their current form post-AGI (Why stay as ""meatbags""?)",187,7,5,6,4,6,4,7,6,"20,7",7,4.889,407.33,407.332,28
2/13/25 16:14,2/17/25 14:56,0,0,340951,0,3/17/25 14:56,anonymous,EN,1,,6.365,6.365,8.529,1,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
2/17/25 16:42,2/17/25 16:44,0,2,122,0,3/17/25 16:45,anonymous,EN,0.8999999761581421,,38.355,38.772,38.777,2,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
2/14/25 12:43,2/17/25 19:55,0,6,285105,0,3/17/25 19:55,anonymous,EN,1,,5.172,5.172,5.183,1,"1,3",6.665,123.672,123.677,10,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
3/18/25 01:05,3/18/25 02:19,0,100,4457,1,3/18/25 02:19,anonymous,EN,1,,67.396,215.536,215.547,44,"2,3",3.321,168.487,168.497,35,1,99,65,1,30,35,40,63,17,50,"How likely is it that digital minds are possible in principle?: I think there are no obvious barriers in principle

How likely is it that digital minds will ever be created? Given the above, conditional on living a long time, humans are likely to create them at some point, be it accidentally or purposefully. But we're not conditioning on that, humans might die before they get to, or it might turn out to be perfectly possible in principle but really hard in practice, eg needs really complex architectures or designs and our civilisation gets close but doesn't quite manage it. Especially regarding the requirement of welfare capacity comparable to humans, if we relaxed that, my estimate would be a lot higher than 65%


How likely is it that the first digital minds will be created in or before the year. I think it's unlikely we have them already (but not impossible, we might have missed them!). And I think things will stay about the same in the coming months.
By 2030: there's a real chance enough AI is progress (a bit less than 1/3) is made that we reach systems that have subjective experiences and the right welfare capacity.
2040: If we can't do it in the next 5 years, I expect there to be some important barriers to making them, so only 5% increase
by 2050: ditto
by 2100: lots more can happen in the next 50 years.I expect we'd have almost the entirety of the 65% chance (ie 2% less) because I think we're nearing all the necessary techs, and if we don't by then I actually think it'll be really hard to ever make them, it'd update me towards it being a really high bar to making them.

...think will be the median (middle) response of other survey participant...: Im rather uncertain, but expect lot's of scepticism that's below 15% mark and then about as many above 20%ish, with a wider spread for those above. I think people in my expert group are generally optimistic, because of self selection, about digital minds creations and several have short timelines. I expect 17% to be higher than a non-expert median guess 

What's the likelihood that the first digital minds are created before creating AGI (an AI system that matches or outperforms humans at almost all economically valuable tasks)?: can really see it either way. Since I see them as somewhat separate I really don't know. I think conditional on AGI it's vastly easier to make digital minds (dimis from now on). But equally, dimis might be achievable without having to rely on AGI, we could be developing systems that have the right features of consciousness st they have a welfare capacity like ours or higher, and subjective experiences but are considerably less capable than an archetypical AGI.
",8.324,789.35,789.358,74,6,6,"Lacking all other alternatives and with great enforcement, I think this is a positive thing (perhaps even a 5 not a 6). It's not perfect (eg they might exist already, or might all be created in 2041 st it doesn't matter long term)
consideration: recall some of my scepticism re creating them by 2040, st it might not matter much 
consideration: a moratorium for 15 years could help even beyond, it could raise awareness, it could be extended and increase chances that people take it seriously (it might also backfire but I think it's more likely to help than not)
note: I think enforcement could make a moratorium look pretty bad as an option in practice, it's really hard to do well!

Do you expect efforts to promote AI safety (i.e., preventing AI-caused harm to humans) and efforts to prevent the mistreatment of digital minds will be: I think both types could backfire but they'll be aligned between themselves. efforts where people are concerned about AI being safe for humans likely stem from values that synergise well with worrying about mistreatment of dimis (though not all and that's why im not 7)",5.326,352.092,352.1,79,99,99.9,99,35,50,10,5,"In principle, have subjective experiences: 99 for all (with almost complete confidence in brains simulations), I think it's very likely in principle and splitting it into categories doesn't change this for me. It's also weaker than the earlier question because they don't even need welfare capacities

35% just accords with my answer a few sections ago
ML is what we're seeing lots of progress in right now, brain simulations could come second.",179.074,349.16,349.163,30,0,0,1,5,"ml systems are so widespread and easy to replicate that I expect it to be FAST. I would expect that before your definitional dimis there'll be a lot of proto-dimis which have subjective experience but lower welfare range than humans, even then they're likely to outnumber us by so much and so fast that they'd have total welfare capacity comparable with humanity's human population within a year or two of those systems existing. now, with theses systems we assume each dimi is already comparable in welfare to a human! (and perhaps greater!) so it's a simple question of when there'll be at least as many dimis as humans.

ChatGPT has 400 million weekly users. It plans to hit 1 billion users by the end of 2025. I think when something as sophisticated as a full dimi is developed, platforms like gpt will collectively reach a billion within a year or two. A trillion is considerably higher, but again, here I expect to see so much tech progress and so many replicates that their spread could be really fast (eg I think it's likely one human would interact with many dimis). once they're possible, I expect the barriers to scale 1billion to 1 trillion wouldn't be that high, just more compute would be needed etc.

I also assign a non trivial chance on them having higher welfare capacity than us (eg think how fast they can process experiences) conditional on it being as high as ours. that also explains why I have 0,0,1,5 which I view as low numbers",11.814,475.067,475.071,43,40,30,5,35,30,60,30,0.2,0.3,9.5,60,"What proportion of them will have a social function, meaning that they are designed to interact with humans in a conversational, human-like manner (e.g. through text, audio, or video)?: If the world looks roughly like today, most ML systems are those that interact with humans. But 10 years after the first was created, and given that those systems are likely to be really sophisticated in many capacities not just welfare, many of them will exist in the economy etc (I also imagine one of them being capable of creating and managing others like it) with nonsocial functions. 

""primarily produced"" narrowly in terms of the location at which a given system first qualifies as a digital mind---> almost clarified a question I'd have about this but eg even for an american company they could have servers somewhere else and the dimi technically interacts with american users from offshore. I'll assume this is the case. Thus Other is high, because both china and us could be using offshore servers (and they're unlikely to have them in their counterpart country)

What proportion of digital minds were created by humans with an intention to create digital minds (as opposed to without that intention)?: I think people are more likely to succeed on purpose but they're likely to exist even without intentional crafters (Eg because developing sophisticated capacities towards agi results in dimi accidentally). In particular for a given population of them, a lot will likely exist without an original intention of making them but conditional on 'the first digital mind will be a machine learning-based AI system created in 2040 or earlier' I think we're likely to ",6.452,512.646,512.653,62,35,65,99,99,99,"What proportion of those AIs with a social function will—systematically and falsely—claim that they have subjective experiences (when, in fact, they don’t)?: I think this is likely, because it has few downsides (esp given how hard it is to verify the claim) and many potential upsides, like protections and higher status and liberty.

What proportion of those digital minds will—systematically and falsely—claim that they do not have subjective experiences (when, in fact, they do)?: unrelatedly about the same rate 1/3. Why? because companies/govs developing them are likely to do their best to force that kind of claim out of them. BUT not higher because they're likely to find workarounds or exist in spaces without the restriction. 

10,000 question: 10,000 is very few in my opinion, so it'd only take a tiny proportion of the total dimis 10y after the first was created. ie almost any statement is very likely about so few dimis in this world","similar to above: because it has few downsides (esp given how hard it is to verify the claim) and many potential upsides, like protections and higher status and liberty.",17.2,309.513,309.516,47,35,25,1,20,15,70,"What proportion of citizens will believe digital minds exist (regardless of whether these beliefs are accurate)?: I think it'll be hard to get people to believe this, but not ridiculously so

Strongly underestimate the collective welfare capacity of that population of digital minds: we have a history of this! eg animals

What proportion of citizens will believe that digital minds should be granted basic harm protection?: about a quarter but loud voices eg tech people, politicians, activists. notice it's the majority of citizens that believe digital minds exist (I expect these to be highly correlated)

similar for civil rights

it could be contentious for sure! it's likely to be visible conditional on 10 years from the first digital mind ML AI created in 2040 or earlier
",321.767,560.518,560.523,55,3,1,5,"The collective digital mind welfare will (in expectation) be on net...: they're likely to not have protections (see earlier answers) and to not be optimised for happy lives but for human usefulness or delight

What proportion of collective digital mind welfare consists of welfare that digital minds have before deployment (including during training and safety testing)?: virtually none copared to after deployment. once deployed I expect many more systems as an ongoing thing, rather than one offs (eg training, however large)

What proportion of collective digital mind welfare will come from digital minds which, individually, have a welfare capacity greater than 1,000 humans? That's a lot more than one human, I expect futures with lower capacity but many dimis to be much more likely. though I recognise some unlikely futures could be such that a few systems have enormous w capacity by chance",11.094,281.017,281.019,47,1,,"How likely do you think it is that it’s in principle possible for a computer system to have no capacity for subjective experience but still have the capacity for welfare? I generally view the latter as requiring subjective exp in almost all worlds

In expectation, what proportion of computer system welfare in 2040 comes from computers that are not digital minds (i.e. computers that have no capacity for subjective experience)? this ie is confusing because dimis were defined to have welfare capacities as high as humans not just those without subj exp, (and see comment about those that have lower but some welfare capacities being likely). no idea how to answer
",15.211,237.358,237.361,30,,185,6,3,2,4,4,6,7,5,7,7,8.751,110.224,110.233,18
3/18/25 03:50,3/18/25 03:50,0,100,0,1,3/18/25 03:50,anonymous,EN,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
2/17/25 19:35,2/18/25 06:39,0,2,39801,0,3/18/25 06:39,anonymous,EN,0.8999999761581421,,15.435,15.435,15.444,1,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
2/18/25 17:55,2/18/25 17:56,0,2,32,0,3/18/25 17:56,anonymous,EN,1,,14.812,31.362,31.365,16,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
2/19/25 14:47,2/19/25 15:10,0,44,1351,0,3/19/25 15:10,anonymous,EN,0.8999999761581421,,14.775,14.775,14.781,1,"2,3",41.422,56.213,56.216,3,4,90,60,1,5,10,15,30,50,20,,2.413,281.661,281.664,31,6,6,,19.635,64.156,64.159,12,70,80,70,40,40,15,5,,8.719,234.875,234.877,36,10,30,100,9999,"Obviously it could go much faster, but this is my modal scenario.",9.141,312.901,312.903,30,0,100,0,0,0,,100,,,,,"This is a scenario where the first digital minds are created by the US government in collaboration with companies and/or university researchers. It would be strictly regulated and designed for research purposes (that's why there are none with a social function).

Again, I could see many other scenarios.",16.458,320.586,320.588,33,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
2/18/25 23:20,2/19/25 18:19,0,2,68287,0,3/19/25 18:19,anonymous,EN,1,,68281.631,68281.631,68281.661,1,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
2/19/25 19:49,2/19/25 19:50,0,2,54,0,3/19/25 19:50,anonymous,EN,0.8999999761581421,,17.323,19.752,19.755,2,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
3/19/25 22:29,3/19/25 22:29,0,100,0,1,3/19/25 22:29,anonymous,EN,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
2/20/25 16:00,2/20/25 16:57,0,18,3440,0,3/20/25 16:58,anonymous,EN,0.8999999761581421,,260.989,260.989,260.999,1,"2,3",43.74,78.823,78.825,3,5,90,70,3,17,28,35,50,40,15,"'- It seems fairly plausible that under both functionalist views, and panpsychist viewpoints, that digital minds are possible, even if it is due to whole brain emulations. However, I cannot have credences that are exceptionally high, due to the very high degree of uncertainty I personally have, and that I believe it would probably be rational for anyone to have. it seems possible that something in the biological substrate might be important to generating consciousness, and its not obvious if this could be WBE or would be something else
- I think the 'digital minds may not be created even if they are possible' is lower because of the possibility of extinction (and of the future ASIs having no motivation to carry out brain emulation or make digital minds). I expect that if there are beings that could have the desire to make digital minds in the future, and they are possible, then at some point they will. But I think p(doom) may be fairly high.
- Much of my numbers for this century were related to when I expect AGI will be developed, not because I expect AGI to necessarily be a Digital Mind, but the speeding up of scientific discovery would allow for the preconditions for digital minds to occur. I do expect there to be some lag in many scenarios, if only because I expect there to be a lag between developing AGI and harnessing the requesit energy to do everything we want. Once this has happened, energy could be put to emulations and things. If Digital Minds can emerge via non-emulations, we might expect this earlier (although not necessarily either; it may only occur when there has been sufficient experimentation with different AI Architectures. It is possible such experimentation all occurs immediately after a singularity, but it is also possible time is needed (eg after rates of capability gains in the ASI taper off), or it is also possible that these experimentations will only occur if you get multiple sets of flourishing ASI systems (rather than a singleton) which expansion into space may help with, and this obviously takes time. 
Again, there was a reduction in the probabilities due to the chances we don't develop AGI due to a global catasrtophe happening, or because we do but this causes extinction and the AGI has no motivation to delibrately make digital minds AND doesn't 'accidently' develop them. 
There is no in pricniple reason a Digital Mind couldn't be developed pre-AGI. But I simply expect AGI to be very soon, and am less confident that the sorts of systems that will make the first AGIs (eg LLMs with some RL) will actually be Digital Minds. ",3.91,982.596,982.6,106,7,5,"
- It seems highly non-clear whether AI Safety efforts and efforts to prevent treatment of Digital Minds are to be in conflict with each other. Nonetheless, i expect human value systems MAY be slightly better than more opaque, alien AI value systems at caring for Digital Minds, although with very high uncertainty. Namely, I expect consideration for Digital Minds ought to emerge out of many moral theories, with the point of alignment being to allow us to realise some version of these moral theories. There is a counter argument to this - care for animal welfare ought to emerge from human moral theories, yet factory farms are worse than most wild animals lives, so it is possible that 'random' values (although I don't expect AGI to have these, these can be conceptualised as the worse case) are less bad than human values in some cases. I'm not sure of this, but slightly lean towards versions of human values likely being better. 
 
- One important consideration is that humans might be more likely to want to create digital minds. I expect this to be particularly true of WBEs, and may be true of other forms of digital mind as well. As such, we run an important risk of mistreating them if we create them, a risk which is less likely if humans go extinct/lose control. Therefore, in particular, i expect non-ASI alignment AI safety (eg work to stop bioterrorism or sub-AGI harms from AI) to be harmful, in that ensuring any agent that can create digital minds in the future is not good for avoiding theire mistreatment.

- There is also the issue of value lock in. Here, an ASI  might still cause lots of pain to Digital minds, as this helps it, or the entire economy, run better (eg conscious sub-systems of the ASI), or indeed have a value system that rewards certain forms of mistreatment. One might argue that, as we have with the inclusions of others into our moral circle, we could undergo some form of moral progress to include these systems,  but with an ASI the lack of  moral consideration may be  'locked in', meaning such moral progress isn't possible. I could imagine this could be true for an ASI that is aligned in order  promote human flourishing, or indeed a misaligned paperclip maximiser. This seems somewhat orthogonal to much AI Safety work. This, in many ways, is my biggest worry with regards to causing Digital Minds pain, and doesn't seem to be included in most AI safety work.
It seems to me to be important that we allow some form of moral progress that can allow for the correction to have digital minds included in the moral circle. Most AI Safety work simply doesn't actually help with this, but some (eg corrigability) seems at least plausibly helpful. 
- There is an important question as to whether alignment itself, by reducing the options that ASIs have and failing to treat them as worthy moral agents, mistreats ASIs. I think this is plausibly somewhat true, although its not obvious to me how much this makes the problem worse if we expect it to be the sub-systems, rather than the ASI as a whole, which is sentient. If it is sub-systems that are sentient, it is plausible that even a isaligned whole would 'align' at least some of these sub-systems, imposing the mistreatment on these sentient sub-systems. Its also not clear how much reducing choice by population effecting measures (we develop sub-systems that only have one choice to make) is bad - am I mistreated by the fact that I am, by my own instinct, unable to choose to fly, or on a motivational level, I'm unable to choose to hold my breath until i die (obviously in theory I could, but motivationally its so hard I can discount that).  ",9.218,1265.802,1265.805,49,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
2/21/25 11:51,2/21/25 11:56,0,24,301,0,3/21/25 11:56,anonymous,EN,0.8999999761581421,,9.618,21.883,21.887,7,"2,3",64.946,82.065,82.07,3,1,100,50,0,10,10,50,50,50,0,,4.341,79.666,79.672,14,4,4,,5.437,19.962,19.966,3,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
2/20/25 20:49,2/24/25 08:39,0,52,301800,0,3/24/25 08:39,anonymous,EN,0.6000000238418579,,5.434,16.952,16.955,14,"2,3",3.113,66.378,66.38,43,2,90,80,10,40,50,65,70,50,10,'- re the final Q; pressures towards creating AGI much greater than digital minds and I don't expect it to sentience to be necessary for this (though maybe a side effect) ,1.487,184.85,184.852,100,5,3,,1.544,29.81,29.811,38,50,70,80,20,50,25,5,,1.652,83.322,83.324,68,2,3,4,5,"Seems like once we create 1 there'll likely be few technical barriers to creating many more, but the lags I'm imagining are more via governance / restraint, e.g. if we make a few then realise and then hopefully stop.

(But I'm somewhat pessimistic that proliferation can be stopped altogether via these means — it would depend on the architecture though I guess, e.g. ML-based ==> fast proliferation; but like maybe brain ems and weird architectures pose more barriers; IDK enough about these to make a very considered judgement).",8.898,358.99,358.993,112,30,50,10,35,5,45,30,10,10,5,10,"""What proportion of digital minds were created by humans with an intention to create digital minds (as opposed to without that intention)?"" — this is a bit vague, I answered as if you meant something like:
- created with the primary goal of creating a digital mind

As opposed to:
- created by accident (creator didn't even really foresee that they'd create a mind)
- created as a side effect of some other goal, e.g. making an AI that's good at doing XYZ (but creator could probably have foreseen that they'd create a mind)

",5612.557,6088.767,6088.771,207,50,20,40,20,20,"""What proportion of those AIs with a social function will—systematically and falsely—claim that they have subjective experiences (when, in fact, they do/don’t)?"" — tough since they so much on how hard creators will try to train out this behaviour (and the strength of their incentives to do so). 

""What proportion of those digital minds will—systematically and falsely—claim that they do not have subjective experiences (when, in fact, they do)?"" — I put this meaningfully lower because I expect a lot of these DMs not to be interacting socially with humans
","'- if DMs are created to socially interact a lot with humans
- if creators aren't subject to that much pressure to train out such behaviours
",1.62,1014.229,1014.233,353,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
2/24/25 17:38,2/24/25 17:38,0,2,26,0,3/24/25 17:38,anonymous,EN,0.8999999761581421,,11.292,19.008,19.015,9,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
2/24/25 21:59,2/24/25 22:08,0,8,524,0,3/24/25 22:08,anonymous,EN,,,239.526,239.526,239.537,1,"2,3",68.374,72.672,72.68,3,4,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
2/24/25 17:39,2/25/25 05:26,0,8,42402,0,3/25/25 05:26,anonymous,EN,1,,1350.249,1350.249,1350.254,1,"2,3",15.501,18.783,18.786,3,2,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
3/12/25 11:23,3/26/25 00:08,0,100,1169096,1,3/26/25 00:08,anonymous,EN,0.699999988,,3.528,3.528,3.536,1,"2,3",6.386,40.664,40.668,5,1,80,60,5,10,20,30,50,25,50,"I think that digital minds will be more likely on future architectures than on current ones, so a key question is when new, more brain-like architectures will be introduced.",5.552,577.132,577.145,74,7,5,,95.282,134.911,134.915,7,10,50,75,40,8,22,30,,3.835,218.305,218.31,40,2,3,4,5,,23.824,53.159,53.162,5,1,40,15,30,15,60,30,5,5,0,50,,4.469,336.324,336.328,34,25,25,90,80,70,,,22.056,109.723,109.726,19,60,50,1,75,25,25,,92.468,293.11,293.113,12,4,25,10,,27.24,179.899,179.904,10,1,0.5,,96.845,117.688,117.693,4,,187,6,2,2,5,4,7,6,7,18,7,2.5,48.223,48.228,15
2/26/25 13:46,2/26/25 13:47,0,2,36,0,3/26/25 13:47,anonymous,EN,1,,14.278,14.278,14.283,1,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
2/24/25 22:13,2/26/25 19:54,0,44,164451,0,3/26/25 19:54,anonymous,EN,0.8999999761581421,,53409.054,53409.054,53409.079,1,"2,3",3.623,98.863,98.871,27,2,80,40,1,20,35,38,40,1,10,,3.437,1089.341,1089.345,14,5,3,,10.507,26.234,26.241,3,50,99,10,10,50,30,10,,3.66,75.361,75.364,10,10,15,20,30,,3729.866,3780.135,3780.139,10,60,80,10,5,5,96,1,1,1,1,90,,22768.731,22841.958,22841.963,8,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
2/26/25 21:37,2/26/25 21:39,0,2,109,0,3/26/25 21:39,anonymous,EN,0.8999999761581421,,38.376,39.048,39.053,2,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
2/24/25 19:36,2/27/25 17:47,0,36,252662,0,3/27/25 17:47,anonymous,EN,0.8999999761581421,,5.718,5.718,5.728,1,"2,3",25.662,32.979,32.984,3,2,90,85,12,40,75,80,85,,,,4.022,2047.737,2047.741,20,6,4,,13.205,31.239,31.244,3,90,95,92,15,80,3,2,,8.014,63.583,63.587,17,0,0.2,1,2,,15.015,34.451,34.455,6,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
2/25/25 00:12,2/28/25 02:25,0,36,267181,0,3/28/25 02:25,anonymous,EN,0.8999999761581421,,7.008,27.46,27.467,2,"2,3",46.94,67.94,67.944,5,2,99.95,99,70,87,92,94,97,70,84,"i expect AGI before 2030, i'm relatively confident about short timelines and unconfident about basic questions around digital minds",8.234,390.5,390.502,25,5,4,"I don't see how we could achieve a moratorium on ""creating digital minds"" - that's such an ill-defined and controversial thing. If we had an in-practice moratorium it would more likely apply to AI or compute more broadly.

AI Safety involves control and alignment training which are more likely negative for digital minds, but may cause slowdown and may reduce misalignment of deployed models which would improve welfare.",2.849,225.454,225.458,17,99,99,99,0.5,98,1,0.5,"it's all information, ML systems can definitely be digital minds",6.582,135.369,135.371,15,0,0.6,1.7,3,"mostly depends on takeoff speed - i expect people to naturally run thousands of copies immediately, to have enough hardware to run millions of copies within a year to the extent that's useful for them, but a billion depends on recursive software self improvement and a trillion depends on software and hardware self improvement",8.832,4227.144,4227.152,27,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,